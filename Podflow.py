#!/usr/bin/env python
# coding: utf-8

import os
import re
import sys
import html
import json
import math
import time
import hashlib
import zipfile
import argparse
import binascii
import threading
import subprocess
import urllib.parse
from hashlib import md5
from functools import reduce
import xml.etree.ElementTree as ET
from http.cookiejar import LoadError
from http.cookiejar import MozillaCookieJar
from datetime import datetime, timedelta, timezone

# è·å–å‘½ä»¤è¡Œå‚æ•°å¹¶åˆ¤æ–­
shortcuts_url_original =[]
argument = ""
update_num = -1
def positive_int(value):
    ivalue = int(value)
    if ivalue <= 0:
        raise argparse.ArgumentTypeError(f"{value} is not a positive integer")
    return ivalue
# åˆ›å»º ArgumentParser å¯¹è±¡
parser = argparse.ArgumentParser(description="you can try: python Podflow.py -n 24 -d 3600")
# å‚æ•°
parser.add_argument("-n", "--times", nargs=1, type=positive_int, metavar="NUM", help="number of times")
parser.add_argument("-d", "--delay", type=positive_int, default=1500, metavar="NUM", help="delay in seconds(default: 1500)")
parser.add_argument("-c", "--config", type=str, default="config.json", metavar='FILE_PATH', help="path to the config.json file")
parser.add_argument("--shortcuts", nargs="*", type=str, metavar="URL", help="only shortcuts can be work")
parser.add_argument("--file", nargs='?', help=argparse.SUPPRESS)
parser.add_argument("--httpfs", action='store_true', help=argparse.SUPPRESS)
# è§£æå‚æ•°
args = parser.parse_args()
time_delay = args.delay
# æ£€æŸ¥å¹¶å¤„ç†å‚æ•°çš„çŠ¶æ€
if args.times is not None:
    update_num = int(args.times[0])
if args.shortcuts is not None:
    update_num = 1
    argument = "a-shell"
    shortcuts_url_original = args.shortcuts
if args.file is not None and ".json" in args.file:
    update_num = 1
    argument = ""
    shortcuts_url_original = []

# é»˜è®¤å‚æ•°
default_config = {
    "preparation_per_count": 100,  # è·å–åª’ä½“ä¿¡æ¯æ¯ç»„æ•°é‡
    "completion_count": 100,  # åª’ä½“ç¼ºå¤±æ—¶æœ€å¤§è¡¥å…¨æ•°é‡
    "retry_count": 5,  # åª’ä½“ä¸‹è½½é‡è¯•æ¬¡æ•°
    "url": "http://127.0.0.1",  # HTTPå…±äº«åœ°å€
    "port": 8000,  # HTTPå…±äº«ç«¯å£
    "port_in_url": True,  # HTTPå…±äº«åœ°å€æ˜¯å¦åŒ…å«ç«¯å£
    "httpfs": False, # HTTPå…±äº«æ—¥å¿—
    "title": "Podflow",  #åšå®¢çš„åç§°
    "filename": "Podflow",  # ä¸»XMLçš„æ–‡ä»¶åç§°
    "link": "https://github.com/gruel-zxz/podflow",  # åšå®¢ä¸»é¡µ
    "description": "åœ¨iOSå¹³å°ä¸Šå€ŸåŠ©workflowå’Œa-shellæ­å»ºä¸“å±çš„æ’­å®¢æœåŠ¡å™¨ã€‚",  # åšå®¢ä¿¡æ¯
    "icon": "https://raw.githubusercontent.com/gruel-zxz/podflow/main/Podflow.png",  # åšå®¢å›¾æ ‡
    "category": "TV &amp; Film",  # åšå®¢ç±»å‹
    "token": None,  # tokenè®¤è¯ï¼Œå¦‚ä¸ºnullæˆ–""å°†ä¸å¯ç”¨token
    "channelid_youtube": {
        "youtube": {
            "update_size": 15,  # æ¯æ¬¡è·å–é¢‘é“åª’ä½“æ•°é‡
            "id": "UCBR8-60-B28hp2BmDPdntcQ",  # é¢‘é“ID
            "title": "YouTube",  # é¢‘é“åç§°
            "quality": "480",  # åª’ä½“åˆ†è¾¨ç‡(ä»…åœ¨mediaä¸ºè§†é¢‘æ—¶æœ‰æ•ˆ)
            "last_size": 50,  # åª’ä½“ä¿ç•™æ•°é‡
            "media": "m4a",  # ä¸‹è½½åª’ä½“ç±»å‹
            "DisplayRSSaddress": False,  # æ˜¯å¦åœ¨Printä¸­æ˜¾ç¤ºå­åšå®¢åœ°å€
            "InmainRSS": True,  # æ˜¯å¦åœ¨ä¸»åšå®¢ä¸­
            "QRcode": False,  # æ˜¯å¦æ˜¾ç¤ºå­åšå®¢åœ°å€äºŒç»´ç (ä»…åœ¨DisplayRSSaddressä¸ºTrueæ—¶æœ‰æ•ˆ)
            "BackwardUpdate": False,  # æ˜¯å¦å‘åæ›´æ–°
            "BackwardUpdate_size": 3,  # å‘åæ›´æ–°æ•°é‡(ä»…åœ¨BackwardUpdateä¸ºTrueæ—¶æœ‰æ•ˆ)
            "want_retry_count": 8,  # åª’ä½“è·å–å¤±è´¥åå¤šå°‘æ¬¡åé‡è¯•(å°äºç­‰äºè¯¥æ•°é‡æ—¶å°†ä¸€ç›´é‡è¯•)
            "NoShorts": False,  # æ˜¯å¦ä¸ä¸‹è½½Shortsåª’ä½“
        },
    },
    "channelid_bilibili": {
        "å“”å“©å“”å“©å¼¹å¹•ç½‘": {
            "update_size": 25,
            "id": "8047632",
            "title": "å“”å“©å“”å“©å¼¹å¹•ç½‘",
            "quality": "480",
            "last_size": 100,
            "media": "m4a",
            "DisplayRSSaddress": False,
            "InmainRSS": True,
            "QRcode": False,
            "BackwardUpdate": False,
            "BackwardUpdate_size": 3,
            "want_retry_count": 8,
            "AllPartGet": False,  # æ˜¯å¦è·å–é¢‘é“å…¨éƒ¨åª’ä½“
        },
    },
}
# å¦‚æœInmainRSSä¸ºFalseæˆ–é¢‘é“æœ‰æ›´æ–°åˆ™æ— è§†DisplayRSSaddressçš„çŠ¶æ€, éƒ½ä¼šå˜ä¸ºTrueã€‚

print(f"{datetime.now().strftime('%H:%M:%S')}|Podflowå¼€å§‹è¿è¡Œ.....")

# å…¨å±€å˜é‡
config = {}  # é…ç½®æ–‡ä»¶å­—å…¸
channelid_youtube = {}  # YouTubeé¢‘é“å­—å…¸
channelid_bilibili = {}  # å“”å“©å“”å“©é¢‘é“å­—å…¸
channelid_youtube_ids = {}  # YouTubeé¢‘é“IDå­—å…¸
channelid_youtube_ids_original = {}  # åŸå§‹YouTubeé¢‘é“IDå­—å…¸
channelid_bilibili_ids = {}  # å“”å“©å“”å“©é¢‘é“IDå­—å…¸
channelid_bilibili_ids_original = {}  # åŸå§‹å“”å“©å“”å“©é¢‘é“IDå­—å…¸

server_process_print_flag = ["keep"]  # httpserverè¿›ç¨‹æ‰“å°æ ‡å¿—åˆ—è¡¨
update_generate_rss = True  # æ›´æ–°å¹¶ç”Ÿæˆrsså¸ƒæœ—å€¼
displayed_QRcode = []  # å·²æ˜¾ç¤ºäºŒç»´ç åˆ—è¡¨

bilibili_data = {}  # å“”å“©å“”å“©dataå­—å…¸
youtube_cookie = {}  # YouTube cookieå­—å…¸
channelid_youtube_ids_update = {}  # éœ€æ›´æ–°çš„YouTubeé¢‘é“å­—å…¸
youtube_content_ytid_update = {}  # éœ€ä¸‹è½½YouTubeè§†é¢‘å­—å…¸
youtube_content_ytid_backward_update = {}  # å‘åæ›´æ–°éœ€ä¸‹è½½YouTubeè§†é¢‘å­—å…¸
channelid_youtube_rss = {}  # YouTubeé¢‘é“æœ€æ–°Rss Responseå­—å…¸
channelid_bilibili_ids_update = {}  # éœ€æ›´æ–°çš„å“”å“©å“”å“©é¢‘é“å­—å…¸
bilibili_content_bvid_update = {}  # éœ€ä¸‹è½½å“”å“©å“”å“©è§†é¢‘å­—å…¸
channelid_bilibili_rss = {}  # å“”å“©å“”å“©é¢‘é“æœ€æ–°Rss Responseå­—å…¸
bilibili_content_bvid_backward_update = {}  # å‘åæ›´æ–°éœ€ä¸‹è½½å“”å“©å“”å“©è§†é¢‘å­—å…¸
video_id_failed = []  # YouTube&å“”å“©å“”å“©è§†é¢‘ä¸‹è½½å¤±è´¥åˆ—è¡¨
video_id_update_format = {}  # YouTubeå’Œå“”å“©å“”å“©è§†é¢‘ä¸‹è½½çš„è¯¦ç»†ä¿¡æ¯å­—å…¸
hash_rss_original = ""  # åŸå§‹rsså“ˆå¸Œå€¼æ–‡æœ¬
xmls_original = {}  # åŸå§‹xmlä¿¡æ¯å­—å…¸
xmls_original_fail = []  # æœªè·å–åŸå§‹xmlé¢‘é“åˆ—è¡¨
youtube_xml_get_tree = {}  # YouTubeé¢‘é“ç®€ä»‹å’Œå›¾æ ‡å­—å…¸
all_youtube_content_ytid = {}  # æ‰€æœ‰YouTubeè§†é¢‘idå­—å…¸
all_bilibili_content_bvid = {}  # æ‰€æœ‰å“”å“©å“”å“©è§†é¢‘idå­—å…¸
all_items = []  # æ›´æ–°åæ‰€æœ‰itemæ˜ç»†åˆ—è¡¨
overall_rss = ""  # æ›´æ–°åçš„rssæ–‡æœ¬
make_up_file_format = {}  # è¡¥å…¨ç¼ºå¤±åª’ä½“å­—å…¸
make_up_file_format_fail = {}  # è¡¥å…¨ç¼ºå¤±åª’ä½“å¤±è´¥å­—å…¸

shortcuts_url = {}  # è¾“å‡ºè‡³shortcutçš„urlå­—å…¸

# æ–‡ä»¶ä¿å­˜æ¨¡å—
def file_save(content, file_name, folder=None):
    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶å¤¹åˆ™å°†æ–‡ä»¶ä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­
    if folder:
        file_path = os.path.join(os.getcwd(), folder, file_name)
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶å¤¹åˆ™å°†æ–‡ä»¶ä¿å­˜åœ¨å½“å‰å·¥ä½œç›®å½•ä¸­
        file_path = os.path.join(os.getcwd(), file_name)
    # ä¿å­˜æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as file:
        if "." in file_name and file_name.split(".")[-1] == "json":
            json.dump(content, file, ensure_ascii=False, indent=4)
        else:
            file.write(content)

# æ—¥å¿—æ¨¡å—
def write_log(log, suffix=None, display=True, time_display=True, only_log=None, file_name="Podflow.log"):
    # è·å–å½“å‰çš„å…·ä½“æ—¶é—´
    current_time = datetime.now()
    # æ ¼å¼åŒ–è¾“å‡º, åªä¿ç•™å¹´æœˆæ—¥æ—¶åˆ†ç§’
    formatted_time = current_time.strftime("%Y-%m-%d %H:%M:%S")
    # æ‰“å¼€æ–‡ä»¶, å¹¶è¯»å–åŸæœ‰å†…å®¹
    try:
        with open(file_name, "r", encoding="utf-8") as file:
            contents = file.read()
    except FileNotFoundError:
        contents = ""
    # å°†æ–°çš„æ—¥å¿—å†…å®¹æ·»åŠ åœ¨åŸæœ‰å†…å®¹ä¹‹å‰
    log_in = re.sub(r"\033\[[0-9;]+m", "", log)
    log_in = re.sub(r"\n", "", log_in)
    new_contents = f"{formatted_time} {log_in}{only_log}\n{contents}" if only_log else f"{formatted_time} {log_in}\n{contents}"
    # å°†æ–°çš„æ—¥å¿—å†…å®¹å†™å…¥æ–‡ä»¶
    file_save(new_contents, file_name)
    if display:
        formatted_time_mini = current_time.strftime("%H:%M:%S")
        log_print = f"{formatted_time_mini}|{log}" if time_display else f"{log}"
        log_print = f"{log_print}|{suffix}" if suffix else f"{log_print}"
        print(log_print)

# æŸ¥çœ‹ffmpegã€requestsã€yt-dlpæ¨¡å—æ˜¯å¦å®‰è£…
exit_sys = False  # è®¾ç½®æš‚åœè¿è¡Œå˜é‡

ffmpeg_worry = """\033[0mFFmpegå®‰è£…æ–¹æ³•:
Ubuntu:
\033[32msudo apt update
sudo apt install ffmpeg\033[0m
CentOS:
\033[32msudo yum update
sudo yum install ffmpeg\033[0m
Debian:
\033[32msudo apt-get update
sudo apt-get install ffmpeg\033[0m
Arch Linuxã€Fedora:
\033[32msudo pacman -S ffmpeg
sudo dnf install ffmpeg\033[0m
æ£€æŸ¥FFmpegç‰ˆæœ¬:
\033[32mffmpeg -version\033[0m"""
try:
    # æ‰§è¡Œ ffmpeg å‘½ä»¤è·å–ç‰ˆæœ¬ä¿¡æ¯
    result = subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True)
    output = result.stdout.lower()
    # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦åŒ…å« ffmpeg ç‰ˆæœ¬ä¿¡æ¯
    if "ffmpeg version" not in output:
        write_log("FFmpeg æœªå®‰è£…, è¯·å®‰è£…åé‡è¯•")
        print(ffmpeg_worry)
        sys.exit(0)
except FileNotFoundError:
    write_log("FFmpeg æœªå®‰è£…, è¯·å®‰è£…åé‡è¯•")
    print(ffmpeg_worry)
    sys.exit(0)

try:
    import requests
    # å¦‚æœå¯¼å…¥æˆåŠŸä½ å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨requestsåº“
except ImportError:
    try:
        subprocess.run(
            ["pip", "install", "chardet", "-U"], capture_output=True, text=True
        )
        subprocess.run(
            ["pip", "install", "requests", "-U"], capture_output=True, text=True
        )
        write_log("\033[31mrequestså®‰è£…æˆåŠŸ, è¯·é‡æ–°è¿è¡Œ\033[0m")
        exit_sys = True
    except FileNotFoundError:
        write_log("\033[31mrequestså®‰è£…å¤±è´¥è¯·é‡è¯•\033[0m")
        exit_sys = True

try:
    import yt_dlp
    # å¦‚æœå¯¼å…¥æˆåŠŸä½ å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨requestsåº“
except ImportError:
    try:
        subprocess.run(
            ["pip", "install", "yt-dlp", "-U"], capture_output=True, text=True
        )
        write_log("\033[31myt-dlpå®‰è£…æˆåŠŸ, è¯·é‡æ–°è¿è¡Œ\033[0m")
        exit_sys = True
    except FileNotFoundError:
        write_log("\033[31myt-dlpå®‰è£…å¤±è´¥è¯·é‡è¯•\033[0m")
        exit_sys = True

if exit_sys:  #åˆ¤æ–­æ˜¯å¦æš‚åœè¿è¡Œ
    sys.exit(0)

# HTTP è¯·æ±‚é‡è¯•æ¨¡å—
def http_client(url, name="", max_retries=10, retry_delay=4, headers_possess=False, cookies=None, data=None, mode="get"):
    user_agent = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
    }
    if "bilibili" in url:
        user_agent["Referer"] = "https://www.bilibili.com/"
    elif "youtube" in url:
        user_agent["Referer"] = "https://www.youtube.com/"
    elif "douyin" in url:
        headers_douyin = {
            'authority': 'sso.douyin.com',
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'origin': 'https://www.douyin.com',
            'referer': 'https://www.douyin.com/',
            'sec-ch-ua': '"Google Chrome";v="117", "Not;A=Brand";v="8", "Chromium";v="117"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-site',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',
        }
        user_agent.update(headers_douyin)
    err = None  # åˆå§‹åŒ– err å˜é‡
    response = None  # åˆå§‹åŒ– response å˜é‡
    # åˆ›å»ºä¸€ä¸ªSessionå¯¹è±¡
    session = requests.Session()
    if headers_possess:
        session.headers.update(user_agent)
    if cookies:
        session.cookies.update(cookies)
    if data:
        session.params.update(data)
    for num in range(max_retries):
        try:
            if mode != "post":
                response = session.get(url, timeout=8)
            else:
                response = session.post(url, timeout=8)
            response.raise_for_status()
        except Exception as http_get_error:
            if response is not None and response.status_code in {404}:
                return response
            if name:
                print(
                    f"{datetime.now().strftime('%H:%M:%S')}|{name}|\033[31mè¿æ¥å¼‚å¸¸é‡è¯•ä¸­...\033[97m{num + 1}\033[0m"
                )
            if err:
                err = f":\n{str(http_get_error)}"
            else:
                err = ""
        else:
            return response
        time.sleep(retry_delay)
    if name:
        print(
            f"{datetime.now().strftime('%H:%M:%S')}|{name}|\033[31mè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°\033[97m{max_retries}\033[0m{err}"
        )
    return response

# æ‰¹é‡æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢åˆ é™¤æ¨¡å—
def vary_replace(varys, text):
    for vary in varys:
        text = re.sub(vary, "", text)
    return text

# è¯»å–ä¸‰æ–¹åº“å½“æ—¥æ—¥å¿—æ¨¡å—
def read_today_library_log():
    try:
        # æ‰“å¼€æ–‡ä»¶è¿›è¡Œè¯»å–
        with open("Podflow.log", "r", encoding="utf-8") as log_file:
            log_lines = log_file.readlines()  # è¯»å–æ‰€æœ‰è¡Œ
        today_log_lines = []
        for log_line in log_lines:
            if f"{(datetime.now()- timedelta(days=1)).strftime('%Y-%m-%d')}" not in log_line:
                if "æ›´æ–°æˆåŠŸ" in log_line or "å®‰è£…æˆåŠŸ" in log_line or "æ— éœ€æ›´æ–°" in log_line:
                    today_log_lines.append(log_line)
            else:
                break
        today_library_log = "".join(today_log_lines)
        # é‡Šæ”¾ lines å˜é‡å†…å­˜ç©ºé—´
        del log_lines
        return today_library_log
    except Exception:
        return ""

# å®‰è£…åº“æ¨¡å—
def library_install(library, library_install_dic=None):
    if version := re.search(
        r"(?<=Version\: ).+",
        subprocess.run(["pip", "show", library], capture_output=True, text=True).stdout,
    ):
        write_log(f"{library}å·²å®‰è£…")
        if library in library_install_dic:
            version_update = library_install_dic[library]
        else:
            # è·å–æœ€æ–°ç‰ˆæœ¬ç¼–å·
            version_update = http_client(
                f"https://pypi.org/project/{library}/", f"{library}", 2, 2
            )
            if version_update:
                version_update = re.search(
                    r"(?<=<h1 class=\"package-header__name\">).+?(?=</h1>)",
                    version_update.text,
                    flags=re.DOTALL,
                )
        # å¦‚æœåº“å·²å®‰è£…, åˆ¤æ–­æ˜¯å¦ä¸ºæœ€æ–°
        if version_update is None or version.group() not in version_update.group():
            # å¦‚æœåº“å·²å®‰è£…, åˆ™å°è¯•æ›´æ–°
            try:
                subprocess.run(
                    ["pip", "install", "--upgrade", library],
                    capture_output=True,
                    text=True,
                )
                write_log(f"{library}æ›´æ–°æˆåŠŸ")
            except FileNotFoundError:
                write_log(f"{library}æ›´æ–°å¤±è´¥")
        else:
            write_log(f"{library}æ— éœ€æ›´æ–°|ç‰ˆæœ¬ï¼š\033[32m{version.group()}\033[0m")
    else:
        write_log(f"{library}æœªå®‰è£…")
        # å¦‚æœåº“æœªå®‰è£…, åˆ™å°è¯•å®‰è£…
        try:
            subprocess.run(
                ["pip", "install", library, "-U"], capture_output=True, text=True
            )
            write_log(f"{library}å®‰è£…æˆåŠŸ")
        except FileNotFoundError:
            write_log(f"{library}å®‰è£…å¤±è´¥")
            sys.exit(0)

# å®‰è£…/æ›´æ–°å¹¶åŠ è½½ä¸‰æ–¹åº“
library_install_list = [
    "astral",
    "bottle",
    "qrcode",
    "yt-dlp",
    "chardet",
    "cherrypy",
    "requests",
    "pycryptodome",
    "ffmpeg-python",
    "BeautifulSoup4",
]

library_import = False
today_library_log = read_today_library_log()

while library_import is False:
    try:
        import ffmpeg
        import qrcode
        import yt_dlp
        import cherrypy
        from astral.sun import sun
        from bs4 import BeautifulSoup
        from astral import LocationInfo
        from Cryptodome.Hash import SHA256
        from Cryptodome.PublicKey import RSA
        from Cryptodome.Cipher import PKCS1_OAEP
        from bottle import Bottle, run, static_file, abort, redirect, request
        library_import = True
    except ImportError:
        today_library_log = ""
    for library in library_install_list:
        if library not in today_library_log:
            library_install_dic = {}
            def library_install_get(library):
                # è·å–æœ€æ–°ç‰ˆæœ¬ç¼–å·
                version_update = http_client(
                    f"https://pypi.org/project/{library}/", f"{library}", 2, 2
                )
                if version_update:
                    version_update = re.search(
                        r"(?<=<h1 class=\"package-header__name\">).+?(?=</h1>)",
                        version_update.text,
                        flags=re.DOTALL,
                    )
                if version_update:
                    library_install_dic[library] = version_update

            # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
            library_install_get_threads = []
            for library in library_install_list:
                thread = threading.Thread(target=library_install_get, args=(library,))
                library_install_get_threads.append(thread)
                thread.start()
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in library_install_get_threads:
                thread.join()
            # å®‰è£…æ›´æ–°ä¸‰æ–¹åº“
            for library in library_install_list:
                library_install(library, library_install_dic)
                today_library_log += f" {library}"
            break

# æ—¶é—´æˆ³æ¨¡å—
def time_stamp():
    time_stamps = []
    # è·å–æ—¶é—´æˆ³æ·˜å®
    def time_stamp_taobao():
        if response := http_client("http://api.m.taobao.com/rest/api3.do?api=mtop.common.getTimestamp", '', 1, 0):
            response_json = response.json()
            try:
                time_stamps.append(int(response_json["data"]["t"]))
            except KeyError:
                pass
    # è·å–æ—¶é—´æˆ³ç¾å›¢
    def time_stamp_meituan():
        if response := http_client("https://cube.meituan.com/ipromotion/cube/toc/component/base/getServerCurrentTime", '', 1, 0):
            response_json = response.json()
            try:
                time_stamps.append(int(response_json["data"]))
            except KeyError:
                pass
    # è·å–æ—¶é—´æˆ³è‹å®
    def time_stamp_suning():
        if response := http_client("https://f.m.suning.com/api/ct.do", '', 1, 0):
            response_json = response.json()
            try:
                time_stamps.append(int(response_json["currentTime"]))
            except KeyError:
                pass
    # åˆ›å»ºçº¿ç¨‹
    thread1 = threading.Thread(target=time_stamp_taobao)
    thread2 = threading.Thread(target=time_stamp_meituan)
    thread3 = threading.Thread(target=time_stamp_suning)
    # å¯åŠ¨çº¿ç¨‹
    thread1.start()
    thread2.start()
    thread3.start()
    # ç­‰å¾…çº¿ç¨‹ç»“æŸ
    thread1.join()
    thread2.join()
    thread3.join()
    if time_stamps:
        return int(sum(time_stamps) / len(time_stamps))
    else:
        print(
            f"{datetime.now().strftime('%H:%M:%S')}|\033[31mè·å–æ—¶é—´æˆ³apiå¤±è´¥\033[0m"
            )
        return round(time.time() * 1000)

# æ ¼å¼åŒ–æ—¶é—´æ¨¡å—
def time_format(duration):
    if duration is None:
        return "Unknown"
    duration = int(duration)
    hours, remaining_seconds = divmod(duration, 3600)
    minutes = remaining_seconds // 60
    remaining_seconds = remaining_seconds % 60
    if hours > 0:
        return "{:02}:{:02}:{:02}".format(hours, minutes, remaining_seconds)
    else:
        return "{:02}:{:02}".format(minutes, remaining_seconds)

# æ ¼å¼åŒ–å­—èŠ‚æ¨¡å—
def convert_bytes(byte_size, units=None, outweigh=1024):
    if units is None:
        units = [" B", "KB", "MB", "GB"]
    if byte_size is None:
        byte_size = 0
    # åˆå§‹å•ä½æ˜¯å­—èŠ‚
    unit_index = 0
    # å°†å­—èŠ‚å¤§å°é™¤ä»¥1024ç›´åˆ°å°äº1024ä¸ºæ­¢
    while byte_size > outweigh and unit_index < len(units) - 1:
        byte_size /= 1024.0
        unit_index += 1
    # æ ¼å¼åŒ–ç»“æœå¹¶è¿”å›
    return f"{byte_size:.2f}{units[unit_index]}"

# ç½‘å€äºŒç»´ç æ¨¡å—
def qr_code(data):
    # åˆ›å»ºä¸€ä¸ªQRCodeå¯¹è±¡
    qr = qrcode.QRCode(
        version=1,
        error_correction=qrcode.constants.ERROR_CORRECT_L,
        box_size=1,
        border=0,
    )
    # è®¾ç½®äºŒç»´ç çš„æ•°æ®
    qr.add_data(data)
    # è·å–QR CodeçŸ©é˜µ
    qr.make(fit=True)
    matrix = qr.make_image(fill_color="black", back_color="white").modules
    # è·å–å›¾åƒçš„å®½åº¦å’Œé«˜åº¦
    width, height = len(matrix), len(matrix)
    height_double = math.ceil(height / 2)
    # è½¬æ¢å›¾åƒä¸ºASCIIå­—ç¬¦
    fonts = ["â–€", "â–„", "â–ˆ", " "]
    ascii_art = ""
    for y in range(height_double):
        if (y + 1) * 2 - 1 >= height:
            for x in range(width):
                ascii_art += (
                    fonts[0] if matrix[(y + 1) * 2 - 2][x] is True else fonts[3]
                )
        else:
            for x in range(width):
                if (
                    matrix[(y + 1) * 2 - 2][x] is True
                    and matrix[(y + 1) * 2 - 1][x] is True
                ):
                    ascii_art += fonts[2]
                elif (
                    matrix[(y + 1) * 2 - 2][x] is True
                    and matrix[(y + 1) * 2 - 1][x] is False
                ):
                    ascii_art += fonts[0]
                elif (
                    matrix[(y + 1) * 2 - 2][x] is False
                    and matrix[(y + 1) * 2 - 1][x] is True
                ):
                    ascii_art += fonts[1]
                else:
                    ascii_art += fonts[3]
            ascii_art += "\n"
    print(ascii_art)
    return height_double

# ä¸‹è½½æ˜¾ç¤ºæ¨¡å—
def show_progress(stream):
    stream = dict(stream)
    if "downloaded_bytes" in stream:
        downloaded_bytes = convert_bytes(stream["downloaded_bytes"]).rjust(9)
    else:
        downloaded_bytes = " Unknow B"
    if "total_bytes" in stream:
        total_bytes = convert_bytes(stream["total_bytes"])
    else:
        total_bytes = "Unknow B"
    if stream["speed"] is None:
        speed = " Unknow B"
    else:
        speed = convert_bytes(stream["speed"], [" B", "KiB", "MiB", "GiB"], 1000).rjust(9)
    if stream["status"] in ["downloading", "error"]:
        if "total_bytes" in stream:
            bar = stream["downloaded_bytes"] / stream["total_bytes"] * 100
        else:
            bar = 0
        bar = f"{bar:.1f}" if bar == 100 else f"{bar:.2f}"
        bar = bar.rjust(5)
        eta = time_format(stream["eta"]).ljust(8)
        print(
            (
                f"\r\033[94m{bar}%\033[0m|{downloaded_bytes}/{total_bytes}|\033[32m{speed}/s\033[0m|\033[93m{eta}\033[0m"
            ),
            end="",
        )
    if stream["status"] == "finished":
        if "elapsed" in stream:
            elapsed = time_format(stream["elapsed"]).ljust(8)
        else:
            elapsed = "Unknown "
        print(f"\r100.0%|{downloaded_bytes}/{total_bytes}|\033[32m{speed}/s\033[0m|\033[97m{elapsed}\033[0m")

# è·å–åª’ä½“æ—¶é•¿å’ŒIDæ¨¡å—
def media_format(video_website, video_url, media="m4a", quality="480", cookies=None):
    fail_message = None
    class MyLogger:
        def debug(self, msg):
            pass
        def warning(self, msg):
            pass
        def info(self, msg):
            pass
        def error(self, msg):
            pass
    def duration_and_formats(video_website, video_url):
        fail_message, infos = None, []
        try:
            # åˆå§‹åŒ– yt_dlp å®ä¾‹, å¹¶å¿½ç•¥é”™è¯¯
            ydl_opts = {
                "no_warnings": True,
                "quiet": True,  # ç¦æ­¢éé”™è¯¯ä¿¡æ¯çš„è¾“å‡º
                "logger": MyLogger(),
            }
            if cookies:
                ydl_opts["http_headers"] = {
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
                    "Referer": "https://www.bilibili.com/",
                }
                ydl_opts["cookiefile"] = cookies  # cookies æ˜¯ä½ çš„ cookies æ–‡ä»¶å
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                # ä½¿ç”¨æä¾›çš„ URL æå–è§†é¢‘ä¿¡æ¯
                if info_dict := ydl.extract_info(
                    f"{video_website}", download=False
                ):
                    # è·å–è§†é¢‘æ—¶é•¿å¹¶è¿”å›
                    entries = info_dict.get("entries", None)
                    download_url = info_dict.get("original_url", None)
                    if entries:
                        for playlist_num, entry in enumerate(entries):
                            infos.append({
                                "title": entry.get("title"),
                                "duration": entry.get("duration"),
                                "formats": entry.get("formats"),
                                "timestamp": entry.get("timestamp"),
                                "id": entry.get("id"),
                                "description": entry.get("description"),
                                "url": entry.get("webpage_url"),
                                "image": entry.get("thumbnail"),
                                "download": {
                                    "url": download_url,
                                    "num": playlist_num + 1,
                                },
                                "format_note": entry.get("format_note"),
                            })
                    else:
                        infos.append({
                            "title": info_dict.get("title"),
                            "duration": info_dict.get("duration"),
                            "formats": info_dict.get("formats"),
                            "timestamp": info_dict.get("timestamp"),
                            "id": info_dict.get("id"),
                            "description": info_dict.get("description"),
                            "url": info_dict.get("webpage_url"),
                            "image": info_dict.get("thumbnail"),
                            "download": {
                                "url": download_url,
                                "num": None
                            },
                            "format_note": info_dict.get("format_note"),
                        })
        except Exception as message_error:
            fail_message = (
                (str(message_error))
                .replace("ERROR: ", "")
                .replace("\033[0;31mERROR:\033[0m ", "")
                .replace(f"{video_url}: ", "")
                .replace("[youtube] ", "")
                .replace("[BiliBili] ", "")
            )
            if video_url[:2] == "BV":
                fail_message = fail_message.replace(f"{video_url[2:]}: ", "")
        return fail_message, infos
    error_reason = {
        r"Premieres in ": ["\033[31mé¢„æ’­\033[0m|", "text"],
        r"This live event will begin in ": ["\033[31mç›´æ’­é¢„çº¦\033[0m|", "text"],
        r"Video unavailable. This video contains content from SME, who has blocked it in your country on copyright grounds": ["\033[31mç‰ˆæƒä¿æŠ¤\033[0m", "text"],
        r"Premiere will begin shortly": ["\033[31mé©¬ä¸Šå¼€å§‹é¦–æ˜ \033[0m", "text"],
        r"Private video. Sign in if you've been granted access to this video": ["\033[31mç§äº«è§†é¢‘\033[0m", "text"],
        r"This video is available to this channel's members on level: .*? Join this channel to get access to members-only content and other exclusive perks\.": ["\033[31mä¼šå‘˜ä¸“äº«\033[0m", "regexp"],
        r"Join this channel to get access to members-only content like this video, and other exclusive perks.": ["\033[31mä¼šå‘˜è§†é¢‘\033[0m", "text"],
        r"Video unavailable. This video has been removed by the uploader": ["\033[31mè§†é¢‘è¢«åˆ é™¤\033[0m", "text"],
        r"Video unavailable. This video is no longer available because the YouTube account associated with this video has been terminated.": ["\033[31må…³è”é¢‘é“è¢«ç»ˆæ­¢\033[0m", "text"],
        r"Video unavailable": ["\033[31mè§†é¢‘ä¸å¯ç”¨\033[0m", "text"],
        r"This video has been removed by the uploader": ["\033[31må‘å¸ƒè€…åˆ é™¤\033[0m", "text"],
        r"This video has been removed for violating YouTube's policy on harassment and bullying": ["\033[31mè¿è§„è§†é¢‘\033[0m", "text"],
        r"This video is private. If the owner of this video has granted you access, please sign in.": ["\033[31mç§äººè§†é¢‘\033[0m", "text"],
        r"This video is unavailable": ["\033[31mæ— æ³•è§‚çœ‹\033[0m", "text"],
        r"The following content is not available on this app.. Watch on the latest version of YouTube.": ["\033[31méœ€App\033[0m", "text"],
        r"This video may be deleted or geo-restricted. You might want to try a VPN or a proxy server (with --proxy)": ["\033[31måˆ é™¤æˆ–å—é™\033[0m", "text"],
        r"Sign in to confirm your age. This video may be inappropriate for some users. Use --cookies-from-browser or --cookies for the authentication. See  https://github.com/yt-dlp/yt-dlp/wiki/FAQ#how-do-i-pass-cookies-to-yt-dlp  for how to manually pass cookies. Also see  https://github.com/yt-dlp/yt-dlp/wiki/Extractors#exporting-youtube-cookies  for tips on effectively exporting YouTube cookies": ["\033[31må¹´é¾„é™åˆ¶\033[0m", "text"],
        r"Sign in to confirm your age. This video may be inappropriate for some users.": ["\033[31må¹´é¾„é™åˆ¶\033[0m", "text"],
    }
    def fail_message_initialize(fail_message, error_reason):
        for key, value in error_reason.items():
            if value[1] == "text":
                if key in fail_message:
                    return [key, value[0], value[1]]
            else:
                if re.search(key, fail_message):
                    return [key, value[0], value[1]]
    video_id_count, change_error, fail_message, infos = 0, None, "", []
    while (
        video_id_count < 3
        and change_error is None
        and (fail_message is not None or infos == [])
    ):
        video_id_count += 1
        fail_message, infos = duration_and_formats(video_website, video_url)
        if fail_message:
            change_error = fail_message_initialize(fail_message, error_reason)
    if change_error:
        if change_error[2] == "text":
            fail_message = fail_message.replace(f"{change_error[0]}", change_error[1])
        else:
            fail_message = re.sub(rf"{change_error[0]}", change_error[1], fail_message)
    if fail_message is None:
        lists = []
        for entry in infos:
            duration = entry["duration"]
            formats = entry["formats"]
            if duration == "" or duration is None:
                return "æ— æ³•è·å–æ—¶é•¿"
            if formats == "" or formats is None:
                return "æ— æ³•è·å–æ ¼å¼"
            duration_and_id = []
            duration_and_id.append(duration)
            # å®šä¹‰æ¡ä»¶åˆ¤æ–­å‡½æ•°
            def check_resolution(item):
                if "aspect_ratio" in item and (
                    isinstance(item["aspect_ratio"], (float, int))
                ):
                    if item["aspect_ratio"] >= 1:
                        return item["height"] <= int(quality)
                    else:
                        return item["width"] <= int(quality)
                else:
                    return False
            def check_ext(item, media):
                return item["ext"] == media if "ext" in item else False
            def check_vcodec(item):
                if "vcodec" in item:
                    return (
                        "vp" not in item["vcodec"].lower()
                        and "av01" not in item["vcodec"].lower()
                        and "hev1" not in item["vcodec"].lower()
                    )
                else:
                    return False
            # è·å–æœ€å¥½è´¨é‡åª’ä½“çš„id
            def best_format_id(formats):
                tbr_max = 0.0
                format_id_best = ""
                vcodec_best = ""
                for format in formats:
                    if (
                        "tbr" in format
                        and "drc" not in format["format_id"]
                        and format["protocol"] == "https"
                        and (isinstance(format["tbr"], (float, int)))
                        and format["tbr"] >= tbr_max
                    ):
                        tbr_max = format["tbr"]
                        format_id_best = format["format_id"]
                        vcodec_best = format["vcodec"]
                return format_id_best, vcodec_best
            # è¿›è¡Œç­›é€‰
            formats_m4a = list(
                filter(lambda item: check_ext(item, "m4a") and check_vcodec(item), formats)
            )
            (best_formats_m4a, vcodec_best) = best_format_id(formats_m4a)
            if best_formats_m4a == "" or best_formats_m4a is None:
                if entry["format_note"] == "è¯•çœ‹":
                    return "\033[31mè¯•çœ‹\033[0m"
                else:
                    return "æ— æ³•è·å–éŸ³é¢‘ID"
            duration_and_id.append(best_formats_m4a)
            if media == "mp4":
                formats_mp4 = list(
                    filter(
                        lambda item: check_resolution(item)
                        and check_ext(item, "mp4")
                        and check_vcodec(item),
                        formats,
                    )
                )
                (best_formats_mp4, vcodec_best) = best_format_id(formats_mp4)
                if best_formats_mp4 == "" or best_formats_mp4 is None:
                    if entry["format_note"] == "è¯•çœ‹":
                        return "\033[31mè¯•çœ‹\033[0m"
                    else:
                        return "æ— æ³•è·å–è§†é¢‘ID"
                duration_and_id.append(best_formats_mp4)
                duration_and_id.append(vcodec_best)
            lists.append({
                "duration_and_id": duration_and_id,
                "title": entry.get("title"),
                "timestamp": entry.get("timestamp"),
                "id": entry.get("id"),
                "description": entry.get("description"),
                "url": entry.get("url"),
                "image": entry.get("image"),
                "download": entry.get("download"),
            })
        return lists
    else:
        return fail_message

# è·å–å·²ä¸‹è½½è§†é¢‘æ—¶é•¿æ¨¡å—
def get_duration_ffmpeg(file_path):
    try:
        # è°ƒç”¨ffmpegè·å–è§†é¢‘æ–‡ä»¶çš„æ—¶é•¿ä¿¡æ¯
        probe = ffmpeg.probe(file_path)
        duration = float(probe['format']['duration'])
        return math.ceil(duration)
    except ffmpeg.Error as e:
        error_note = e.stderr.decode('utf-8').splitlines()[-1]
        write_log(f"\033[31mError:\033[0m {error_note}")

# ç­‰å¾…åŠ¨ç”»æ¨¡å—
def wait_animation(stop_flag, wait_animation_display_info):
    animation = "."
    i = 1
    prepare_youtube_print = datetime.now().strftime("%H:%M:%S")
    while True:
        if stop_flag[0] == "keep":
            print(
                f"\r{prepare_youtube_print}|{wait_animation_display_info}\033[34må‡†å¤‡ä¸­{animation.ljust(5)}\033[0m",
                end="",
            )
        elif stop_flag[0] == "error":
            print(
                f"\r{prepare_youtube_print}|{wait_animation_display_info}\033[34må‡†å¤‡ä¸­{animation} \033[31må¤±è´¥:\033[0m"
            )
            break
        elif stop_flag[0] == "end":
            print(
                f"\r{prepare_youtube_print}|{wait_animation_display_info}\033[34må‡†å¤‡ä¸­{animation} å·²å®Œæˆ\033[0m"
            )
            break
        if i % 5 == 0:
            animation = "."
        else:
            animation += "."
        i += 1
        time.sleep(0.5)

# ä¸‹è½½è§†é¢‘æ¨¡å—
def download_video(
    video_url,
    output_dir,
    output_format,
    format_id,
    video_website,
    video_write_log,
    sesuffix="",
    cookies=None,
    playlist_num=None,
):
    class MyLogger:
        def debug(self, msg):
            pass
        def warning(self, msg):
            pass
        def info(self, msg):
            pass
        def error(self, msg):
            msg = (
                msg
                .replace("ERROR: ", "")
                .replace("\033[0;31mERROR:\033[0m ", "")
                .replace(f"{video_url}: ", "")
                .replace("[youtube] ", "")
                .replace("[BiliBili] ", "")
                .replace("[download] ", "")
            )
            if video_url[:2] == "BV":
                msg = msg.replace(f"{video_url[2:]}: ", "")
            print(msg)
    ydl_opts = {
        "outtmpl": f"channel_audiovisual/{output_dir}/{video_url}{sesuffix}.{output_format}",  # è¾“å‡ºæ–‡ä»¶è·¯å¾„å’Œåç§°
        "format": f"{format_id}",  # æŒ‡å®šä¸‹è½½çš„æœ€ä½³éŸ³é¢‘å’Œè§†é¢‘æ ¼å¼
        "noprogress": True,
        "quiet": True,
        "progress_hooks": [show_progress],
        "logger": MyLogger(),
        "throttled_rate": "70K",  # è®¾ç½®æœ€å°ä¸‹è½½é€Ÿç‡ä¸º:å­—èŠ‚/ç§’
    }
    if cookies:
        ydl_opts["http_headers"] = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
            "Referer": "https://www.bilibili.com/",
        }
        ydl_opts["cookiefile"] = cookies  # cookies æ˜¯ä½ çš„ cookies æ–‡ä»¶å
    if playlist_num:  # æ’­æ”¾åˆ—è¡¨çš„ç¬¬nä¸ªè§†é¢‘
        ydl_opts["playliststart"] = playlist_num
        ydl_opts["playlistend"] = playlist_num
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([f"{video_website}"])  # ä¸‹è½½æŒ‡å®šè§†é¢‘é“¾æ¥çš„è§†é¢‘
    except Exception as download_video_error:
        write_log(
            f"{video_write_log} \033[31mä¸‹è½½å¤±è´¥\033[0m", None, True, True, (f"é”™è¯¯ä¿¡æ¯: {str(download_video_error)}")
            .replace("ERROR: ", "")
            .replace("\033[0;31mERROR:\033[0m ", "")
            .replace(f"{video_url}: ", "")
            .replace("[youtube] ", "")
            .replace("[download] ", "")
        )  # å†™å…¥ä¸‹è½½å¤±è´¥çš„æ—¥å¿—ä¿¡æ¯
        return video_url

# è§†é¢‘å®Œæ•´ä¸‹è½½æ¨¡å—
def dl_full_video(
    video_url,
    output_dir,
    output_format,
    format_id,
    id_duration,
    video_website,
    video_write_log,
    sesuffix="",
    cookies=None,
    playlist_num=None,
):
    if download_video(
        video_url,
        output_dir,
        output_format,
        format_id,
        video_website,
        video_write_log,
        sesuffix,
        cookies,
        playlist_num,
    ):
        return video_url
    duration_video = get_duration_ffmpeg(
        f"channel_audiovisual/{output_dir}/{video_url}{sesuffix}.{output_format}"
    )  # è·å–å·²ä¸‹è½½è§†é¢‘çš„å®é™…æ—¶é•¿
    if abs(id_duration - duration_video) <= 1:  # æ£€æŸ¥å®é™…æ—¶é•¿ä¸é¢„è®¡æ—¶é•¿æ˜¯å¦ä¸€è‡´
        return None
    if duration_video:
        write_log(
            f"{video_write_log} \033[31mä¸‹è½½å¤±è´¥\033[0m\né”™è¯¯ä¿¡æ¯: ä¸å®Œæ•´({id_duration}|{duration_video})"
        )
        os.remove(
            f"channel_audiovisual/{output_dir}/{video_url}{sesuffix}.{output_format}"
        )  # åˆ é™¤ä¸å®Œæ•´çš„è§†é¢‘
    return video_url

# è§†é¢‘é‡è¯•ä¸‹è½½æ¨¡å—
def dl_retry_video(
    video_url,
    output_dir,
    output_format,
    format_id,
    id_duration,
    retry_count,
    video_website,
    video_write_log,
    sesuffix="",
    cookies=None,
    playlist_num=None,
):
    video_id_failed = dl_full_video(
        video_url,
        output_dir,
        output_format,
        format_id,
        id_duration,
        video_website,
        video_write_log,
        sesuffix,
        cookies,
        playlist_num,
    )
    # ä¸‹è½½å¤±è´¥åé‡å¤å°è¯•ä¸‹è½½è§†é¢‘
    video_id_count = 0
    while video_id_count < retry_count and video_id_failed:
        video_id_count += 1
        write_log(f"{video_write_log}ç¬¬\033[34m{video_id_count}\033[0mæ¬¡é‡æ–°ä¸‹è½½")
        video_id_failed = dl_full_video(
            video_url,
            output_dir,
            output_format,
            format_id,
            id_duration,
            video_website,
            video_write_log,
            sesuffix,
            cookies,
            playlist_num,
        )
    return video_id_failed

# éŸ³è§†é¢‘æ€»ä¸‹è½½æ¨¡å—
def dl_aideo_video(
    video_url,
    output_dir,
    output_format,
    video_format,
    retry_count,
    video_website,
    output_dir_name="",
    cookies=None,
    playlist_num=None,
    display_color="\033[95m"
):
    if output_dir_name:
        video_write_log = f"{display_color}{output_dir_name}\033[0m|{video_url}"
    else:
        video_write_log = video_url
    id_duration = video_format[0]
    if cookies:
        print_message = "\033[34må¼€å§‹ä¸‹è½½\033[0m ğŸª"
    else:
        print_message = "\033[34må¼€å§‹ä¸‹è½½\033[0m"
    print(
        f"{datetime.now().strftime('%H:%M:%S')}|{video_write_log} {print_message}",
        end="",
    )
    if output_format == "m4a":
        if video_format[1] in ["140", "30280"]:
            print("")
        else:
            print(f" \033[97m{video_format[1]}\033[0m")
        video_id_failed = dl_retry_video(
            video_url,
            output_dir,
            "m4a",
            video_format[1],
            id_duration,
            retry_count,
            video_website,
            video_write_log,
            "",
            cookies,
            playlist_num,
        )
    else:
        print(
            f"\n{datetime.now().strftime('%H:%M:%S')}|\033[34mè§†é¢‘éƒ¨åˆ†å¼€å§‹ä¸‹è½½\033[0m \033[97m{video_format[2]}\033[0m"
        )
        video_id_failed = dl_retry_video(
            video_url,
            output_dir,
            "mp4",
            video_format[2],
            id_duration,
            retry_count,
            video_website,
            video_write_log,
            ".part",
            cookies,
            playlist_num,
        )
        if video_id_failed is None:
            print(
                f"{datetime.now().strftime('%H:%M:%S')}|\033[34méŸ³é¢‘éƒ¨åˆ†å¼€å§‹ä¸‹è½½\033[0m \033[97m{video_format[1]}\033[0m"
            )
            video_id_failed = dl_retry_video(
                video_url,
                output_dir,
                "m4a",
                video_format[1],
                id_duration,
                retry_count,
                video_website,
                video_write_log,
                ".part",
                cookies,
                playlist_num,
            )
        if video_id_failed is None:
            print(
                f"{datetime.now().strftime('%H:%M:%S')}|\033[34må¼€å§‹åˆæˆ...\033[0m", end=""
            )
            # æ„å»ºFFmpegå‘½ä»¤
            ffmpeg_cmd = [
                "ffmpeg",
                "-v",
                "error",
                "-i",
                f"channel_audiovisual/{output_dir}/{video_url}.part.mp4",
                "-i",
                f"channel_audiovisual/{output_dir}/{video_url}.part.m4a",
                "-c:v",
                "copy",
                "-c:a",
                "copy",
                f"channel_audiovisual/{output_dir}/{video_url}.mp4",
            ]
            # æ‰§è¡ŒFFmpegå‘½ä»¤
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True, text=True)
                print(" \033[32måˆæˆæˆåŠŸ\033[0m")
                os.remove(f"channel_audiovisual/{output_dir}/{video_url}.part.mp4")
                os.remove(f"channel_audiovisual/{output_dir}/{video_url}.part.m4a")
            except subprocess.CalledProcessError as dl_aideo_video_error:
                video_id_failed = video_url
                write_log(f"\n{video_write_log} \033[31mä¸‹è½½å¤±è´¥\033[0m\né”™è¯¯ä¿¡æ¯: åˆæˆå¤±è´¥:{dl_aideo_video_error}")
    if video_id_failed is None:
        if output_format == "m4a":
            only_log = f" {video_format[1]}"
        else:
            only_log = f" {video_format[1]}+{video_format[2]}"
        if cookies:
            only_log += " Cookies"
        write_log(f"{video_write_log} \033[32mä¸‹è½½æˆåŠŸ\033[0m", None, True, True, only_log)  # å†™å…¥ä¸‹è½½æˆåŠŸçš„æ—¥å¿—ä¿¡æ¯
    return video_id_failed

# æ„å»ºæ–‡ä»¶å¤¹æ¨¡å—
def folder_build(folder_name, parent_folder_name=None):
    if parent_folder_name:
        folder_path = os.path.join(os.getcwd(), parent_folder_name, folder_name)
    else:
        folder_path = os.path.join(os.getcwd(), folder_name)
    if not os.path.exists(folder_path):  # åˆ¤æ–­æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        os.makedirs(folder_path)  # åˆ›å»ºæ–‡ä»¶å¤¹
        write_log(f"æ–‡ä»¶å¤¹{folder_name}åˆ›å»ºæˆåŠŸ")

# å­—å…¸æ‹†åˆ†æ¨¡å—
def split_dict(data, chunk_size=100, firse_item_only=False):
    chunks = []
    if chunk_size == 0:
        return [{}]
    else:
        if firse_item_only:
            end_value = chunk_size
        else:
            end_value = len(data)
        for i in range(0, end_value, chunk_size):
            chunk = dict(list(data.items())[i:i+chunk_size])
            chunks.append(chunk)
        return chunks

# åˆå¹¶æ•´å½¢åˆ—è¡¨æ¨¡å—
def list_merge_tidy(list1, list2=[], length=None):
    final_list = []
    for item in list1 + list2:
        if item:
            item = item[:length]
        if item not in final_list:
            final_list.append(item)
    return final_list

# è·å–é…ç½®ä¿¡æ¯configæ¨¡å—
def get_config(file_name="config.json"):
    # æ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å­˜åœ¨configæ–‡ä»¶
    if file_name != "config.json" and not os.path.exists(file_name):
        if os.path.exists("config.json"):
            write_log(f"ä¸å­˜åœ¨é…ç½®æ–‡ä»¶{file_name}, å°†ä½¿ç”¨åŸå§‹é…ç½®æ–‡ä»¶")
            file_name = "config.json"
        else:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨, åˆ›å»ºå¹¶å†™å…¥é»˜è®¤å­—å…¸
            with open("config.json", "w") as file:
                json.dump(default_config, file, indent=4)
            write_log("ä¸å­˜åœ¨é…ç½®æ–‡ä»¶, å·²æ–°å»º, é»˜è®¤é¢‘é“")
            return default_config
    # å¦‚æœæ–‡ä»¶å­˜åœ¨, è¯»å–å­—å…¸å¹¶ä¿å­˜åˆ°configå˜é‡ä¸­
    try:
        with open(file_name, "r", encoding="utf-8") as file:
            config = json.load(file)
        print(f"{datetime.now().strftime('%H:%M:%S')}|å·²è¯»å–é…ç½®æ–‡ä»¶")
        return config
    # å¦‚æœconfigæ ¼å¼æœ‰é—®é¢˜, åœæ­¢è¿è¡Œå¹¶æŠ¥é”™
    except Exception as config_error:
        write_log(f"é…ç½®æ–‡ä»¶æœ‰è¯¯, è¯·æ£€æŸ¥{file_name}, {str(config_error)}")
        sys.exit(0)

# çº æ­£é…ç½®ä¿¡æ¯configæ¨¡å—
def correct_config():
    # å¯¹completion_countè¿›è¡Œçº æ­£
    if (
        "completion_count" not in config
        or not isinstance(config["completion_count"], int)
        or config["completion_count"] < 0
    ):
        config["completion_count"] = default_config["completion_count"]
    # å¯¹preparation_per_countè¿›è¡Œçº æ­£
    if (
        "preparation_per_count" not in config
        or not isinstance(config["preparation_per_count"], int)
        or config["preparation_per_count"] <= 0
    ):
        config["preparation_per_count"] = default_config["preparation_per_count"]
    # å¯¹retry_countè¿›è¡Œçº æ­£
    if (
        "retry_count" not in config
        or not isinstance(config["retry_count"], int)
        or config["retry_count"] <= 0
    ):
        config["retry_count"] = default_config["retry_count"]
    # å¯¹urlè¿›è¡Œçº æ­£
    match_url = re.search(
        r"^(https?|ftp)://([^/\s:]+)", config["url"]
    )
    if "url" in config and match_url:
        config["url"] = match_url.group()
    else:
        config["url"] = default_config["url"]
    # å¯¹portè¿›è¡Œçº æ­£
    if (
        "port" not in config
        or not isinstance(config["port"], int)
        or config["port"] < 0
        or config["port"] > 65535
    ):
        config["port"] = default_config["port"]
    # å¯¹port_in_urlè¿›è¡Œçº æ­£
    if "port_in_url" not in config or not isinstance(
        config["port_in_url"], bool
    ):
        config["port_in_url"] = default_config["port_in_url"]
    # åˆå¹¶åœ°å€å’Œç«¯å£
    if config["port_in_url"]:
        config["address"] = f"{config['url']}:{config['port']}"
    else:
        config["address"] = config['url']
    # å¯¹httpfsè¿›è¡Œçº æ­£
    if "httpfs" not in config or not isinstance(
        config["httpfs"], bool
    ):
        config["httpfs"] = default_config["httpfs"]
    # å¯¹titleè¿›è¡Œçº æ­£
    if "title" not in config:
        config["title"] = default_config["title"]
    # å¯¹filenameè¿›è¡Œçº æ­£
    if "filename" not in config:
        config["filename"] = default_config["filename"]
    # å¯¹linkè¿›è¡Œçº æ­£
    if "link" not in config or not re.search(
        r"^(https?|ftp)://[^\s/$.?#].[^\s]*$", config["link"]
    ):
        config["link"] = default_config["link"]
    # å¯¹descriptionè¿›è¡Œçº æ­£
    if "description" not in config:
        config["description"] = default_config["description"]
    # å¯¹iconè¿›è¡Œçº æ­£
    if "icon" not in config or not re.search(
        r"^(https?|ftp)://[^\s/$.?#].[^\s]*$", config["icon"]
    ):
        config["icon"] = default_config["icon"]
    # å¯¹categoryè¿›è¡Œçº æ­£
    if "category" not in config:
        config["category"] = default_config["category"]
    # å¯¹IOSä¸­shortcutsè‡ªåŠ¨å…³æ³¨è¿›è¡Œçº æ­£
    if f"{config['address']}/{config['filename']}.xml" not in shortcuts_url_original:
        shortcuts_url[f"{config['filename']}(Main RSS)"] = f"{config['address']}/{config['filename']}.xml"
    # å¯¹tokenè¿›è¡Œçº æ­£
    if "token" not in config:
        config["token"] = default_config["token"]
    if config["token"] in [None, ""]:
        config["token"] = ""
    else:
        config["token"] = str(config["token"])

# è·å–æ—¥å‡ºæ—¥è½å¹¶åˆ¤æ–­æ˜¼å¤œæ¨¡å—
def http_day_and_night(latitude, longitude):
    sun_url = "https://api.sunrise-sunset.org/json"
    sun_data = {
        "lat": latitude,
        "lng": longitude,
        "date": "today",
    }
    sunrise_sunset = http_client(sun_url, 'è·å–æ—¥å‡ºæ—¥è½', 3, 5, True, None, sun_data)
    if not sunrise_sunset:
        return None
    try:
        time_dict = sunrise_sunset.json()["results"]
        sunrise = time_dict['sunrise']
        sunset = time_dict['sunset']
    except KeyError:
        return None
    # è·å–å½“å‰æ—¶é—´ï¼Œå¹¶å»é™¤æ—¶åŒº
    now = datetime.now()
    # å°†æ—¥å‡ºå’Œæ—¥è½æ—¶é—´è½¬æ¢ä¸ºdatetimeå¯¹è±¡
    today = now.date()
    sunrise_time = datetime.strptime(sunrise, '%I:%M:%S %p')
    sunrise_time = sunrise_time.replace(year=today.year, month=today.month, day=today.day, tzinfo=timezone.utc)
    sunset_time = datetime.strptime(sunset, '%I:%M:%S %p')
    sunset_time = sunset_time.replace(year=today.year, month=today.month, day=today.day, tzinfo=timezone.utc)
    # è½¬æ¢æ—¥å‡ºå’Œæ—¥è½æ—¶é—´ä¸ºæ—¶é—´æˆ³
    sunrise_now = sunrise_time.timestamp()
    sunset_now = sunset_time.timestamp()
    today = now.timestamp()
    # è®¡ç®—æ˜¨å¤©åŠæ˜å¤©æ—¥å‡ºå’Œæ—¥è½æ—¶é—´æˆ³
    sunrise_yesterday = sunrise_now - 3600*24
    sunset_yesterday = sunset_now - 3600*24
    sunrise_tommorrow = sunrise_now + 3600*24
    sunset_tommorrow = sunset_now + 3600*24
    # åˆ¤æ–­ç°åœ¨æ˜¯ç™½å¤©è¿˜æ˜¯æ™šä¸Š
    if sunrise_now < sunset_now:
        if (
            sunrise_now < today < sunset_now
            or sunrise_yesterday < today < sunset_yesterday
            or sunrise_tommorrow < today < sunset_tommorrow
        ):
            return "light"
        else:
            return "dark"
    else:
        if (
            sunrise_now > today > sunset_now
            or sunrise_yesterday > today > sunset_yesterday
            or sunrise_tommorrow > today > sunset_tommorrow
        ):
            return "dark"
        else:
            return "light"

# æ ¹æ®ç»çº¬åº¦åˆ¤æ–­æ˜¼å¤œæ¨¡å—
def judging_day_and_night(latitude, longitude):
    # åˆ›å»ºä¸€ä¸ª LocationInfo å¯¹è±¡ï¼Œåªæä¾›ç»çº¬åº¦ä¿¡æ¯
    location = LocationInfo("", "", "", latitude=latitude, longitude=longitude)
    # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´ï¼Œå¹¶ä¸ºå…¶æ·»åŠ æ—¶åŒºä¿¡æ¯
    now = datetime.now(timezone.utc)
    yesterday = now - timedelta(days=1)
    tommorrow = now + timedelta(days=1)
    def sunrise_sunset(time):
        # åˆ›å»ºä¸€ä¸ª Sun å¯¹è±¡
        sun_time = sun(location.observer, date=time)
        # è®¡ç®—æ—¥å‡ºå’Œæ—¥è½æ—¶é—´ï¼Œä»¥åŠæ—¥è½å‰å’Œæ—¥å‡ºåçš„ä¸€å°æ—¶
        sunrise = sun_time["sunrise"]
        sunset = sun_time["sunset"]
        sunrise_minus_one_hour = sunrise  # - timedelta(hours=1)
        sunset_plus_one_hour = sunset  # + timedelta(hours=1)
        return sunrise_minus_one_hour, sunset_plus_one_hour
    sunrise_now, sunset_now = sunrise_sunset(now)
    sunrise_yesterday, sunset_yesterday = sunrise_sunset(yesterday)
    sunrise_tommorrow, sunset_tommorrow = sunrise_sunset(tommorrow)
    # åˆ¤æ–­ç°åœ¨æ˜¯ç™½å¤©è¿˜æ˜¯æ™šä¸Š
    if sunrise_now < sunset_now:
        if (
            sunrise_now < now < sunset_now
            or sunrise_yesterday < now < sunset_yesterday
            or sunrise_tommorrow < now < sunset_tommorrow
        ):
            return "light"
        else:
            return "dark"
    else:
        if (
            sunrise_now > now > sunset_now
            or sunrise_yesterday > now > sunset_yesterday
            or sunrise_tommorrow > now > sunset_tommorrow
        ):
            return "dark"
        else:
            return "light"

# æ ¹æ®æ—¥å‡ºæ—¥è½ä¿®æ”¹å°é¢(åªé€‚ç”¨åŸå°é¢)æ¨¡å—
def channge_icon():
    if config["icon"] == default_config["icon"]:
        def ipinfo():
            if response := http_client("https://ipinfo.io/json/", "", 1, 0):
                data = response.json()
                # æå–ç»åº¦å’Œçº¬åº¦
                coordinates = data ["loc"].split(",")
                return True, coordinates[0], coordinates[1]
            else: 
                return False, None, None
        def ipapi():
            if response := http_client("http://ip-api.com/json/", "", 1, 0):
                data = response.json()
                # æå–ç»åº¦å’Œçº¬åº¦
                return True, data["lat"], data["lon"]
            else: 
                return False, None, None
        def freegeoip():
            if response := http_client("https://freegeoip.app/json/", "", 1, 0):
                data = response.json()
                # æå–ç»åº¦å’Œçº¬åº¦
                return True, data["latitude"], data["longitude"]
            else: 
                return False, None, None
        label = False
        # å…¬ç½‘è·å–ç»çº¬åº¦
        label, latitude, longitude = ipinfo()
        if label is False:
            write_log("è·å–ç»çº¬åº¦ä¿¡æ¯é‡è¯•ä¸­...\033[97m1\033[0m")
            label, latitude, longitude = ipapi()
            if label is False:
                write_log("è·å–ç»çº¬åº¦ä¿¡æ¯é‡è¯•ä¸­...\033[97m2\033[0m")
                label, latitude, longitude = freegeoip()
                if label is False:
                    write_log("è·å–ç»çº¬åº¦ä¿¡æ¯å¤±è´¥")
        if label:
            picture_name = http_day_and_night(latitude, longitude)
            if not picture_name:
                write_log("è·å–æ—¥å‡ºæ—¥è½å¤±è´¥ï¼Œå°†è®¡ç®—æ˜¼å¤œ")
                picture_name = judging_day_and_night(latitude, longitude)
            config["icon"] = f"https://raw.githubusercontent.com/gruel-zxz/podflow/main/Podflow_{picture_name}.png"

# ä»é…ç½®æ–‡ä»¶ä¸­è·å–é¢‘é“æ¨¡å—
def get_channelid(name):
    output_name = ""
    if name == "youtube":
        output_name = "YouTube"
    elif name == "bilibili":
        output_name = "BiliBili"
    if f"channelid_{name}" in config:
        print(f"{datetime.now().strftime('%H:%M:%S')}|å·²è¯»å–{output_name}é¢‘é“ä¿¡æ¯")
        return config[f"channelid_{name}"]
    else:
        write_log(f"{output_name}é¢‘é“ä¿¡æ¯ä¸å­˜åœ¨")
        return {}

# channelidä¿®æ­£æ¨¡å—
def correct_channelid(channelid, website):
    channelid_name = ""
    output_name = ""
    if website == "youtube":
        channelid_name = "youtube"
        output_name = "YouTube"
    elif website == "bilibili":
        channelid_name = "å“”å“©å“”å“©å¼¹å¹•ç½‘"
        output_name = "BiliBili"
    # éŸ³è§†é¢‘æ ¼å¼åŠåˆ†è¾¨ç‡å¸¸é‡
    video_media = [
        "m4v",
        "mov",
        "qt",
        "avi",
        "flv",
        "wmv",
        "asf",
        "mpeg",
        "mpg",
        "vob",
        "mkv",
        "rm",
        "rmvb",
        "vob",
        "ts",
        "dat",
    ]
    dpi = [
        "144",
        "180",
        "216",
        "240",
        "360",
        "480",
        "720",
        "1080",
        "1440",
        "2160",
        "4320",
    ]
    media = ["m4a", "mp4"]
    # å¤åˆ¶å­—å…¸channelid, éå†å¤åˆ¶åçš„å­—å…¸è¿›è¡Œæ“ä½œä»¥é¿å…åœ¨å¾ªç¯ä¸­åˆ é™¤å…ƒç´ å¯¼è‡´çš„è¿­ä»£é”™è¯¯
    channelid_copy = channelid.copy()
    # å¯¹channelidçš„é”™è¯¯è¿›è¡Œæ›´æ­£
    for channelid_key, channeli_value in channelid_copy.items():
        # åˆ¤æ–­æ˜¯å¦ä¸ºå­—å…¸
        if not isinstance(channeli_value, dict):
            channeli_value = {"id": channeli_value}
            channelid[channelid_key] = channeli_value
        # åˆ¤æ–­idæ˜¯å¦æ­£ç¡®
        if "id" not in channeli_value or (
            website == "youtube" and not re.search(
                r"^UC.{22}", channeli_value["id"]
            )
        ) or (
            website == "bilibili" and not channeli_value["id"].isdigit()
        ):
            # åˆ é™¤é”™è¯¯çš„
            del channelid[channelid_key]
            write_log(f"{output_name}é¢‘é“ {channelid_key} IDä¸æ­£ç¡®")
        else:
            # å¯¹update_sizeè¿›è¡Œçº æ­£
            if (
                "update_size" not in channeli_value
                or not isinstance(channeli_value["update_size"], int)
                or channeli_value["update_size"] <= 0
            ):
                channelid[channelid_key]["update_size"] = default_config[
                    f"channelid_{website}"
                ][channelid_name]["update_size"]
            # å¯¹idè¿›è¡Œçº æ­£
            if website == "youtube":
                channelid[channelid_key]["id"] = re.search(
                    r"UC.{22}", channeli_value["id"]
                ).group()
            # å¯¹last_sizeè¿›è¡Œçº æ­£
            if (
                "last_size" not in channeli_value
                or not isinstance(channeli_value["last_size"], int)
                or channeli_value["last_size"] <= 0
            ):
                channelid[channelid_key]["last_size"] = default_config[
                    f"channelid_{website}"
                ][channelid_name]["last_size"]
            channelid[channelid_key]["last_size"] = max(
                channelid[channelid_key]["last_size"],
                channelid[channelid_key]["update_size"],
            )
            # å¯¹titleè¿›è¡Œçº æ­£
            if "title" not in channeli_value:
                channelid[channelid_key]["title"] = channelid_key
            # å¯¹qualityè¿›è¡Œçº æ­£
            if (
                (
                    "quality" not in channeli_value
                    or channeli_value["quality"] not in dpi
                )
                and "media" in channeli_value
                and channeli_value["media"] == "mp4"
            ):
                channelid[channelid_key]["quality"] = default_config[
                    f"channelid_{website}"
                ][channelid_name]["quality"]
            # å¯¹mediaè¿›è¡Œçº æ­£
            if (
                "media" in channeli_value
                and channeli_value["media"] not in media
                and channeli_value["media"] in video_media
            ):
                channelid[channelid_key]["media"] = "mp4"
            elif (
                "media" in channeli_value
                and channeli_value["media"] not in media
                or "media" not in channeli_value
            ):
                channelid[channelid_key]["media"] = "m4a"
            # å¯¹DisplayRSSaddressè¿›è¡Œçº æ­£
            if "DisplayRSSaddress" not in channeli_value or not isinstance(
                channeli_value["DisplayRSSaddress"], bool
            ):
                channelid[channelid_key]["DisplayRSSaddress"] = False
            # å¯¹InmainRSSè¿›è¡Œçº æ­£
            if "InmainRSS" in channeli_value and isinstance(
                channeli_value["InmainRSS"], bool
            ):
                if channeli_value["InmainRSS"] is False:
                    channelid[channelid_key]["DisplayRSSaddress"] = True
            else:
                channelid[channelid_key]["InmainRSS"] = True
            # å¯¹QRcodeè¿›è¡Œçº æ­£
            if "QRcode" not in channeli_value or not isinstance(
                channeli_value["QRcode"], bool
            ):
                channelid[channelid_key]["QRcode"] = False
            # å¯¹BackwardUpdateè¿›è¡Œçº æ­£
            if "BackwardUpdate" not in channeli_value or not isinstance(
                channeli_value["BackwardUpdate"], bool
            ):
                channelid[channelid_key]["BackwardUpdate"] = False
            # å¯¹BackwardUpdate_sizeè¿›è¡Œçº æ­£
            if channelid[channelid_key]["BackwardUpdate"] and (
                "BackwardUpdate_size" not in channeli_value
                or not isinstance(channeli_value["BackwardUpdate_size"], int)
                or channeli_value["BackwardUpdate_size"] <= 0
            ):
                channelid[channelid_key]["BackwardUpdate_size"] = default_config[
                    f"channelid_{website}"
                ][channelid_name]["BackwardUpdate_size"]
            # å¯¹want_retry_countè¿›è¡Œçº æ­£
            if (
                "want_retry_count" not in channeli_value
                or not isinstance(channeli_value["want_retry_count"], int)
                or channeli_value["want_retry_count"] <= 0
            ):
                channelid[channelid_key]["want_retry_count"] = default_config[
                    f"channelid_{website}"
                ][channelid_name]["want_retry_count"]
            if website == "bilibili":
                # å¯¹AllPartGetè¿›è¡Œçº æ­£
                if "AllPartGet" not in channeli_value or not isinstance(
                    channeli_value["AllPartGet"], bool
                ):
                    channelid[channelid_key]["AllPartGet"] = False
            if website == "youtube":
                # å¯¹NoShortsè¿›è¡Œçº æ­£
                if "NoShorts" not in channeli_value or not isinstance(
                    channeli_value["NoShorts"], bool
                ):
                    channelid[channelid_key]["NoShorts"] = False
        if channelid[channelid_key]["InmainRSS"] is False and f"{config['address']}/channel_rss/{channeli_value['id']}.xml" not in shortcuts_url_original:
            shortcuts_url[channelid_key] = f"{config['address']}/channel_rss/{channeli_value['id']}.xml"
    return channelid

# è¯»å–é¢‘é“IDæ¨¡å—
def get_channelid_id(channelid, idname):
    output_name = ""
    if idname == "youtube":
        output_name = "YouTube"
    elif idname == "bilibili":
        output_name = "BiliBili"
    if channelid:
        channelid_ids = dict(
            {channel["id"]: key for key, channel in channelid.items()}
        )
        print(f"{datetime.now().strftime('%H:%M:%S')}|è¯»å–{output_name}é¢‘é“çš„channelidæˆåŠŸ")
    else:
        channelid_ids = {}
    return channelid_ids

# è·å–æœ€æ–°çš„img_keyå’Œsub_keyæ¨¡å—
def getWbiKeys(bilibili_cookie=None):
    bilibili_url = "https://api.bilibili.com/x/web-interface/nav"
    if resp := http_client(bilibili_url, "è·å–æœ€æ–°çš„img_keyå’Œsub_key", 10, 4, True, bilibili_cookie):
        resp.raise_for_status()
        json_content = resp.json()
        img_url: str = json_content['data']['wbi_img']['img_url']
        sub_url: str = json_content['data']['wbi_img']['sub_url']
        img_key = img_url.rsplit('/', 1)[1].split('.')[0]
        sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]
        return img_key, sub_key
    else:
        return "", ""

# ç”ŸæˆNetscape_HTTP_Cookieæ¨¡å—
def bulid_Netscape_HTTP_Cookie(file_name, cookie={}):
    if "bilibili" in file_name:
        cookie_jar = f'''# Netscape HTTP Cookie File
# This file is generated by yt-dlp.  Do not edit.

.bilibili.com	TRUE	/	FALSE	0	SESSDATA	{cookie.get("SESSDATA", "")}
.bilibili.com	TRUE	/	FALSE	0	bili_jct	{cookie.get("bili_jct", "")}
.bilibili.com	TRUE	/	FALSE	0	DedeUserID	{cookie.get("DedeUserID", "")}
.bilibili.com	TRUE	/	FALSE	0	DedeUserID__ckMd5	{cookie.get("DedeUserID__ckMd5", "")}
.bilibili.com	TRUE	/	FALSE	0	sid	{cookie.get("sid", "")}
.bilibili.com	TRUE	/	FALSE	0	buvid3	{cookie.get("buvid3", "")}
.bilibili.com	TRUE	/	FALSE	0	b_nut	{cookie.get("b_nut", "")}'''
    elif "youtube" in file_name:
        cookie_jar = f'''# Netscape HTTP Cookie File
# This file is generated by yt-dlp.  Do not edit.

.youtube.com	TRUE	/	TRUE	0	__Secure-3PSID	{cookie.get("__Secure-3PSID", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-1PSIDTS	{cookie.get("__Secure-1PSIDTS", "")}
.youtube.com	TRUE	/	TRUE	0	SAPISID	{cookie.get("SAPISID", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-1PSIDCC	{cookie.get("__Secure-1PSIDCC", "")}
.youtube.com	TRUE	/	TRUE	0	SSID	{cookie.get("SSID", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-1PAPISID	{cookie.get("__Secure-1PAPISID", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-1PSID	{cookie.get("__Secure-1PSID", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-3PAPISID	{cookie.get("__Secure-3PAPISID", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-3PSIDCC	{cookie.get("__Secure-3PSIDCC", "")}
.youtube.com	TRUE	/	TRUE	0	__Secure-3PSIDTS	{cookie.get("__Secure-3PSIDTS", "")}
.youtube.com	TRUE	/	TRUE	0	LOGIN_INFO	{cookie.get("LOGIN_INFO", "")}
.youtube.com	TRUE	/	FALSE	0	PREF	{cookie.get("PREF", "")}
.youtube.com	TRUE	/	TRUE	0	YSC	{cookie.get("YSC", "")}
.youtube.com	TRUE	/	TRUE	0	VISITOR_INFO1_LIVE	{cookie.get("VISITOR_INFO1_LIVE", "")}
.youtube.com	TRUE	/	TRUE	0	VISITOR_PRIVACY_METADATA	{cookie.get("VISITOR_PRIVACY_METADATA", "")}'''
    else:
        cookie_jar = '''# Netscape HTTP Cookie File
# This file is generated by yt-dlp.  Do not edit.'''
    file_save(cookie_jar, f"{file_name}.txt", "channel_data")

# å°†Netscapeè½¬Dictæ¨¡å—
def get_cookie_dict(file):
    parts = file.split('/')
    try:
        # åŠ è½½Netscapeæ ¼å¼çš„cookieæ–‡ä»¶
        cookie_jar = MozillaCookieJar(file)
        cookie_jar.load(ignore_discard=True)
        # å°†cookieè½¬æ¢ä¸ºå­—å…¸
        cookie_dict = {}
        for cookie in cookie_jar:
            cookie_dict[cookie.name] = cookie.value
        return cookie_dict
    except FileNotFoundError :
        print(f"{datetime.now().strftime('%H:%M:%S')}|{parts[-1]}æ–‡ä»¶ä¸å­˜åœ¨")
        return None
    except LoadError:
        print(f"{datetime.now().strftime('%H:%M:%S')}|{parts[-1]}æ–‡ä»¶é”™è¯¯")
        return None

# è·å–YouTube cookieæ¨¡å—
def get_youtube_cookie():
    youtube_cookie = get_cookie_dict("channel_data/yt_dlp_youtube.txt")
    if youtube_cookie is None:
        write_log("YouTube \033[31mè·å–cookieå¤±è´¥\033[0m")
        return None
    if response := http_client("https://www.youtube.com", "YouTubeä¸»é¡µ", 10, 4, True, youtube_cookie):
        html_content = response.text
        if "\"LOGGED_IN\":true" in html_content:
            print(f"{datetime.now().strftime('%H:%M:%S')}|YouTube \033[32mè·å–cookieæˆåŠŸ\033[0m")
            return youtube_cookie
        elif "\"LOGGED_IN\":false" in html_content:
            print(f"{datetime.now().strftime('%H:%M:%S')}|ç™»é™†YouTubeå¤±è´¥")
            write_log("YouTube \033[31mè·å–cookieå¤±è´¥\033[0m")
            return None
        else:
            print(f"{datetime.now().strftime('%H:%M:%S')}|ç™»é™†YouTubeæ— æ³•åˆ¤æ–­")
            write_log("YouTube \033[31mè·å–cookieå¤±è´¥\033[0m")
            return None
    else:
        write_log("YouTube \033[31mè·å–cookieå¤±è´¥\033[0m")
        return None

# ç”³è¯·å“”å“©å“”å“©äºŒç»´ç å¹¶è·å–tokenå’ŒURLæ¨¡å—
def bilibili_request_qr_code():
    # å®é™…ç”³è¯·äºŒç»´ç çš„APIè¯·æ±‚
    response = http_client('https://passport.bilibili.com/x/passport-login/web/qrcode/generate', 'ç”³è¯·BiliBiliäºŒç»´ç ', 3, 5, True)
    data = response.json()
    return data['data']['qrcode_key'], data['data']['url']

# æ‰«ç ç™»å½•å“”å“©å“”å“©å¹¶è¿”å›çŠ¶æ€å’Œcookieæ¨¡å—
def bilibili_scan_login(token):
    # å‘é€GETè¯·æ±‚
    response = http_client('https://passport.bilibili.com/x/passport-login/web/qrcode/poll', '', 1, 1, True, None, {'qrcode_key': token})
    if response:
        data = response.json()
        cookies = response.cookies
        return data['data']['code'], cookies, data['data']['refresh_token']
    else:
        return None, None, None

# ç™»é™†å“”å“©å“”å“©æ¨¡å—
def bilibili_login():
    buvid3_and_bnut = http_client("https://www.bilibili.com", "å“”å“©å“”å“©ä¸»é¡µ", 10, 4, True).cookies.get_dict()
    token, url = bilibili_request_qr_code()
    print(f"{datetime.now().strftime('%H:%M:%S')}|è¯·ç”¨BiliBili Appæ‰«æç™»å½•:")
    upward = qr_code(url)
    login_status_change = ""
    time_print = f"{datetime.now().strftime('%H:%M:%S')}|BiliBili "
    while True:
        status, cookie, refresh_token = bilibili_scan_login(token)
        if status == 86101:
            login_status = '\033[0mæœªæ‰«æ\033[0m'
        elif status == 86038:
            login_status = '\033[31mäºŒç»´ç è¶…æ—¶, è¯·é‡è¯•\033[0m'
        elif status == 86090:
            login_status = '\033[32mæ‰«ææˆåŠŸ\033[0m'
        elif status == 0:
            login_status = '\033[32mç™»é™†æˆåŠŸ\033[0m'
        else:
            login_status = '\033[31mé”™è¯¯\033[0m'
        if login_status_change != login_status:
            if login_status == '':
                print(f"{time_print}{login_status}".ljust(42), end = "")
            else:
                print(f"\r{time_print}{login_status}".ljust(42), end = "")
        login_status_change = login_status
        if status == 86038:
            print("")
            return status, refresh_token, upward
        elif status == 0:
            print("")
            cookie["buvid3"] = buvid3_and_bnut.get("buvid3", "")
            cookie["b_nut"] = buvid3_and_bnut.get("b_nut", "")
            return cookie, refresh_token, upward
        time.sleep(1)

# ä¿å­˜å“”å“©å“”å“©ç™»é™†æˆåŠŸåçš„cookiesæ¨¡å—
def save_bilibili_cookies():
    bilibili_cookie, refresh_token, upward = bilibili_login()
    if bilibili_cookie == 86038:
        return {"cookie": None}, upward
    else:
        bilibili_cookie = requests.utils.dict_from_cookiejar(bilibili_cookie)
        bilibili_data = {
            "cookie": bilibili_cookie,
            "refresh_token": refresh_token
        }
        bulid_Netscape_HTTP_Cookie("yt_dlp_bilibili", bilibili_cookie)
        return bilibili_data, upward

# æ£€æŸ¥å“”å“©å“”å“©æ˜¯å¦éœ€è¦åˆ·æ–°æ¨¡å—
def judgment_bilibili_update(cookies):
    url = "https://passport.bilibili.com/x/passport-login/web/cookie/info"
    response = http_client(url, 'BiliBiliåˆ·æ–°åˆ¤æ–­', 3, 5, True, cookies)
    response = response.json()
    if response["code"] == 0:
        return response["code"], response["data"]["refresh"]
    else:
        return response["code"], None

# å“”å“©å“”å“©cookieåˆ·æ–°æ¨¡å—
def bilibili_cookie_update(bilibili_data):
    bilibili_cookie = bilibili_data["cookie"]
    # è·å–refresh_csrf
    key = RSA.importKey('''\
-----BEGIN PUBLIC KEY-----
MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDLgd2OAkcGVtoE3ThUREbio0Eg
Uc/prcajMKXvkCKFCWhJYJcLkcM2DKKcSeFpD/j6Boy538YXnR6VhcuUJOhH2x71
nzPjfdTcqMz7djHum0qSZA0AyCBDABUqCrfNgCiJ00Ra7GmRj+YCK1NJEuewlb40
JNrRuoEUXpabUzGB8QIDAQAB
-----END PUBLIC KEY-----''')
    # ç”ŸæˆCorrespondPathæ¨¡å—
    def getCorrespondPath(ts):
        cipher = PKCS1_OAEP.new(key, SHA256)
        encrypted = cipher.encrypt(f'refresh_{ts}'.encode())
        return binascii.b2a_hex(encrypted).decode()
    # è·å–å½“å‰æ—¶é—´æˆ³
    ts = time_stamp()
    # è·å–refresh_csrf
    refresh_csrf_response = http_client(f"https://www.bilibili.com/correspond/1/{getCorrespondPath(ts)}", 'è·å–refresh_csrf', 3, 5, True, bilibili_cookie)
    if refresh_csrf_match := re.search(r'<div id="1-name">(.+?)</div>', refresh_csrf_response.text):
        refresh_csrf_value = refresh_csrf_match[1]
    else:
        return {"cookie": None}
    # æ›´æ–°bilibili_cookie
    update_cookie_url = 'https://passport.bilibili.com/x/passport-login/web/cookie/refresh'
    update_cookie_data = {
        'csrf': bilibili_cookie["bili_jct"],
        'refresh_csrf': refresh_csrf_value,
        'source': 'main_web',
        'refresh_token': bilibili_data["refresh_token"]
    }
    update_cookie_response = http_client(update_cookie_url, 'æ›´æ–°BiliBili_cookie', 3, 5, True, bilibili_cookie, update_cookie_data, "post")
    if update_cookie_response.json()["code"] == 0:
        new_bilibili_cookie = requests.utils.dict_from_cookiejar(update_cookie_response.cookies)
        new_refresh_token = update_cookie_response.json()["data"]["refresh_token"]
    else:
        return {"cookie": None}
    # ç¡®è®¤æ›´æ–°bilibili_cookie
    confirm_cookie_url = 'https://passport.bilibili.com/x/passport-login/web/confirm/refresh'
    confirm_cookie_data = {
        'csrf': new_bilibili_cookie["bili_jct"],
        'refresh_token': bilibili_data["refresh_token"]
    }
    confirm_cookie_response = http_client(confirm_cookie_url, 'ç¡®è®¤æ›´æ–°BiliBili_cookie', 3, 5, True, new_bilibili_cookie, confirm_cookie_data, "post")
    if confirm_cookie_response.json()["code"] == 0:
        new_bilibili_cookie["buvid3"] = bilibili_cookie["buvid3"]
        new_bilibili_cookie["b_nut"] = bilibili_cookie["b_nut"]
        bilibili_data["cookie"] = new_bilibili_cookie
        bilibili_data["refresh_token"] = new_refresh_token
        bulid_Netscape_HTTP_Cookie("yt_dlp_bilibili", new_bilibili_cookie)
        return bilibili_data
    else:
        return {"cookie": None}

# ç™»é™†åˆ·æ–°å“”å“©å“”å“©å¹¶è·å–data
def get_bilibili_data(channelid_bilibili_ids):
    if channelid_bilibili_ids:
        try:
            with open('channel_data/bilibili_data.json', 'r') as file:
                bilibili_data = file.read()
            bilibili_data = json.loads(bilibili_data)
        except Exception:
            bilibili_data = {"cookie":None, "timestamp": 0.0}
        if time.time() - bilibili_data["timestamp"] - 60*60 > 0:
            bilibili_login_code, bilibili_login_refresh_token = judgment_bilibili_update(bilibili_data["cookie"])
            upward = 0
            try_num = 0
            while try_num < 2 and (bilibili_login_code != 0 or bilibili_login_refresh_token is not False):
                if bilibili_login_code != 0:
                    if try_num == 0:
                        print(f"{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[31mæœªç™»é™†\033[0m")
                    else:
                        print(f"\033[{upward + 3}F\033[{upward + 3}K{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[31mæœªç™»é™†, é‡è¯•ç¬¬\033[0m{try_num}\033[31mæ¬¡\033[0m")
                    bilibili_data, upward = save_bilibili_cookies()
                    try_num += 1
                else:
                    print(f"{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[33méœ€åˆ·æ–°\033[0m")
                    bilibili_data = bilibili_cookie_update(bilibili_data)
                    if bilibili_data["cookie"]:
                        print(f"{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[32måˆ·æ–°æˆåŠŸ\033[0m")
                    else:
                        print(f"{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[31måˆ·æ–°å¤±è´¥, é‡æ–°ç™»é™†\033[0m")
                bilibili_login_code, bilibili_login_refresh_token = judgment_bilibili_update(bilibili_data["cookie"])
            if bilibili_login_code == 0 and bilibili_login_refresh_token is False:
                print(f"{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[32mè·å–cookieæˆåŠŸ\033[0m")
                img_key, sub_key = getWbiKeys()
                bilibili_data["img_key"] = img_key
                bilibili_data["sub_key"] = sub_key
                bilibili_data["timestamp"] = time.time()
                file_save(bilibili_data, "bilibili_data.json", "channel_data")
                if not os.path.isfile("channel_data/yt_dlp_bilibili.txt"):
                    bulid_Netscape_HTTP_Cookie("yt_dlp_bilibili", bilibili_data["cookie"])
                return channelid_bilibili_ids, bilibili_data
            else:
                write_log("BiliBili \033[31mè·å–cookieå¤±è´¥\033[0m")
                return {}, {"cookie":None, "timestamp": 0.0}
        else:
            print(f"{datetime.now().strftime('%H:%M:%S')}|BiliBili \033[33mè·å–cookieæˆåŠŸ\033[0m")
            if not os.path.isfile("channel_data/yt_dlp_bilibili.txt"):
                bulid_Netscape_HTTP_Cookie("yt_dlp_bilibili", bilibili_data["cookie"])
            return channelid_bilibili_ids, bilibili_data
    else:
        return {}, {"cookie":None, "timestamp": 0.0}

# WBIç­¾åæ¨¡å—
def WBI_signature(params={}, img_key="", sub_key=""):
    mixinKeyEncTab = [
        46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,
        33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,
        61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,
        36, 20, 34, 44, 52
    ]
    def getMixinKey(orig: str):
        'å¯¹ imgKey å’Œ subKey è¿›è¡Œå­—ç¬¦é¡ºåºæ‰“ä¹±ç¼–ç '
        return reduce(lambda s, i: s + orig[i], mixinKeyEncTab, '')[:32]
    def encWbi(params: dict, img_key: str, sub_key: str):
        'ä¸ºè¯·æ±‚å‚æ•°è¿›è¡Œ wbi ç­¾å'
        mixin_key = getMixinKey(img_key + sub_key)
        curr_time = round(time.time())
        params['wts'] = curr_time                                   # æ·»åŠ  wts å­—æ®µ
        params = dict(sorted(params.items()))                       # æŒ‰ç…§ key é‡æ’å‚æ•°
        # è¿‡æ»¤ value ä¸­çš„ "!'()*" å­—ç¬¦
        params = {
            k: ''.join(filter(lambda chr: chr not in "!'()*", str(v)))
            for k, v 
            in params.items()
        }
        query = urllib.parse.urlencode(params)                      # åºåˆ—åŒ–å‚æ•°
        wbi_sign = md5((query + mixin_key).encode()).hexdigest()    # è®¡ç®— w_rid
        params['w_rid'] = wbi_sign
        return params
    return encWbi(
        params=params,
        img_key=img_key,
        sub_key=sub_key
    )

# é€šè¿‡bs4è·å–htmlä¸­å­—å…¸æ¨¡å—
def get_html_dict(url, name, script_label):
    if response := http_client(url, name):
        html_content = response.text
        # ä½¿ç”¨Beautiful Soupè§£æHTML
        soup = BeautifulSoup(html_content, 'html.parser')
        # æŸ¥æ‰¾å¯¹åº”çš„ script æ ‡ç­¾
        data_script = soup.find('script', string=lambda t: t and script_label in t)
        if data_script:
            try:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– JSON æ•°æ®
                pattern = re.compile(r'\{.*\}', re.DOTALL)
                match = pattern.search(data_script.text)
                if match:
                    data_str = match.group()
                    data = json.loads(data_str)
                    return data
            except json.JSONDecodeError:
                None

# è·å–æ–‡ä»¶åˆ—è¡¨å’Œåˆ†Påˆ—è¡¨
def get_file_list(video_key, video_media="m4a", length=12):
    media = (
        ("m4a", "mp4", "part")
        if video_media == "m4a"
        else ("mp4", "part")
    )
    try:
        content_id = [
            file  # è·å–æ–‡ä»¶åï¼ˆåŒ…æ‹¬æ‰©å±•åï¼‰
            for file in os.listdir(f"channel_audiovisual/{video_key}")  # éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            if file.endswith(media)  # ç­›é€‰å‡ºä»¥ media ç»“å°¾çš„æ–‡ä»¶
        ]
        content_id_items = []
        items_counts = {}
        for id in content_id:
            if len(id) > length + 4:
                content_id_items.append(id)
            if '.part' in id:
                items_counts[id[:length]] = 0
        content_id_items = [id[:length] for id in content_id_items]
        for id in content_id_items:
            if id not in items_counts:
                qtys = content_id_items.count(id)
                if video_media == "m4a":
                    pattern = re.compile(rf"{id}_[0-9]*\.(m4a|mp4)")
                else:
                    pattern = re.compile(rf"{id}_[0-9]*\.(mp4)")
                if len([item for item in content_id if pattern.search(item)]) == qtys:
                    items_counts[id] = qtys
                else:
                    fail = False
                    for qty in range(qtys):
                        if video_media == "m4a":
                            if f"{id}_p{qty+1}.m4a" not in content_id and f"{id}_p{qty+1}.mp4" not in content_id:
                                fail = True
                        else:
                            if f"{id}_p{qty+1}.mp4" not in content_id:
                                fail = True
                    if fail:
                        items_counts[id] = 0
                    else:
                        items_counts[id] = qtys
        content_id = list_merge_tidy(content_id, [] ,length)
        for id, num in items_counts.copy().items():
            if (num == 1 or num == 0) and id in content_id:
                content_id.remove(id)
                del items_counts[id]
    except Exception:
        content_id = []
        items_counts = {}
    return content_id, items_counts

# ä»YouTubeæ’­æ”¾åˆ—è¡¨è·å–æ›´æ–°æ¨¡å—
def get_youtube_html_playlists(youtube_key, youtube_value, guids=[""], direction_forward=True, update_size=20, youtube_content_ytid_original=[]):
    idlist = []
    item = {}
    threads = []
    fail = []
    try:
        if direction_forward:
            videoid_start = guids[0]
        else:
            videoid_start = guids[-1]
    except IndexError:
        videoid_start = ""
    # è·å–åª’ä½“ç›¸å…³ä¿¡æ¯æ¨¡å—
    def get_video_item(videoid, youtube_value):
        yt_Initial_Player_Response = get_html_dict(f"https://www.youtube.com/watch?v={videoid}", f"{youtube_value}|{videoid}", "ytInitialPlayerResponse")
        try:
            player_Microformat_Renderer = yt_Initial_Player_Response["microformat"]["playerMicroformatRenderer"]
        except (KeyError, TypeError, IndexError, ValueError):
            player_Microformat_Renderer = {}
            fail.append(videoid)
        if player_Microformat_Renderer:
            try:
                item[videoid]["description"] = player_Microformat_Renderer["description"]["simpleText"]
            except (KeyError, TypeError, IndexError, ValueError):
                item[videoid]["description"] = ""
            item[videoid]["pubDate"] = player_Microformat_Renderer["publishDate"]
            item[videoid]["image"] = player_Microformat_Renderer["thumbnail"]["thumbnails"][0]["url"]
            try:
                fail.remove(videoid)
            except (KeyError, TypeError, IndexError, ValueError):
                pass
    yt_initial_data = get_html_dict(f"https://www.youtube.com/watch?v={videoid_start}&list=UULF{youtube_key[-22:]}", f"{youtube_value} HTML", "ytInitialData")
    try:
        playlists = yt_initial_data['contents']['twoColumnWatchNextResults']['playlist']['playlist']['contents']
        main_title = yt_initial_data['contents']['twoColumnWatchNextResults']['playlist']['playlist']["ownerName"]["simpleText"]
    except (KeyError, TypeError, IndexError, ValueError):
        return None
    if direction_forward or videoid_start == "":
        for playlist in playlists:
            videoid = playlist['playlistPanelVideoRenderer']['videoId']
            if playlist['playlistPanelVideoRenderer']['navigationEndpoint']['watchEndpoint']['index'] == update_size:
                break
            if videoid not in guids:
                title = playlist['playlistPanelVideoRenderer']['title']['simpleText']
                idlist.append(videoid)
                item[videoid] = {
                    "title": title,
                    "yt-dlp": True,
                }
                if videoid in youtube_content_ytid_original:
                    item[videoid]["yt-dlp"] = False
                    item_thread = threading.Thread(target=get_video_item, args=(videoid, youtube_value,))
                    item_thread.start()
                    threads.append(item_thread)
    else:
        reversed_playlists = []
        for playlist in reversed(playlists):
            videoid = playlist['playlistPanelVideoRenderer']['videoId']
            if videoid not in guids:
                reversed_playlists.append(playlist)
            else:
                break
        for playlist in reversed(reversed_playlists[-update_size:]):
            videoid = playlist['playlistPanelVideoRenderer']['videoId']
            title = playlist['playlistPanelVideoRenderer']['title']['simpleText']
            idlist.append(videoid)
            item[videoid] = {
                "title": title,
                "yt-dlp": True,
            }
            if videoid in youtube_content_ytid_original:
                item[videoid]["yt-dlp"] = False
                item_thread = threading.Thread(target=get_video_item, args=(videoid, youtube_value,))
                item_thread.start()
                threads.append(item_thread)
    for thread in threads:
        thread.join()
    for videoid in fail:
        get_video_item(videoid, youtube_value)
    if fail:
        if direction_forward or videoid_start == "":
            for videoid in fail:
                print(f"{datetime.now().strftime('%H:%M:%S')}|{youtube_value}|{videoid} HTMLæ— æ³•æ›´æ–°, å°†ä¸è·å–")
                idlist.remove(videoid)
                del item[videoid]
        else:
            print(f"{datetime.now().strftime('%H:%M:%S')}|{youtube_value} HTMLæœ‰å¤±è´¥åªæ›´æ–°éƒ¨åˆ†")
            index = len(idlist)
            for videoid in fail:
                if videoid in idlist:
                    index = min(idlist.index(videoid), index)
            idlist_fail = idlist[index:]
            idlist = idlist[:index]
            for videoid in idlist_fail:
                idlist.remove(videoid)
    return {"list": idlist, "item": item, "title": main_title}

# è·å–YouTube Shortsè§†é¢‘åˆ—è¡¨
def get_youtube_shorts_id(youtube_key, youtube_value):
    videoIds = []
    url = f"https://www.youtube.com/channel/{youtube_key}/shorts"
    if data := get_html_dict(url, youtube_value, "ytInitialData"):
        try:
            items = data['contents']['twoColumnBrowseResultsRenderer']['tabs'][2]['tabRenderer']['content']['richGridRenderer']['contents']
            for item in items:
                videoId = item['richItemRenderer']['content']['shortsLockupViewModel']['onTap']['innertubeCommand']['reelWatchEndpoint']['videoId']
                videoIds.append(videoId)
        except (KeyError, TypeError, IndexError, ValueError):
            pass
    return videoIds

# æ›´æ–°Youtubeé¢‘é“xmlæ¨¡å—
def youtube_rss_update(youtube_key, youtube_value, pattern_youtube_varys, pattern_youtube404, pattern_youtube_error):
    # è·å–å·²ä¸‹è½½åª’ä½“åç§°
    youtube_media = (
        ("m4a", "mp4")  # æ ¹æ® channelid_youtube çš„åª’ä½“ç±»å‹é€‰æ‹©æ–‡ä»¶æ ¼å¼
        if channelid_youtube[youtube_value]["media"] == "m4a"
        else ("mp4",)  # å¦‚æœä¸æ˜¯ m4aï¼Œåˆ™åªé€‰æ‹© mp4
    )
    try:
        # éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œç­›é€‰å‡ºä»¥ youtube_media ç»“å°¾çš„æ–‡ä»¶
        youtube_content_ytid_original = [
            os.path.splitext(file)[0]  # è·å–æ–‡ä»¶åï¼ˆä¸åŒ…æ‹¬æ‰©å±•åï¼‰
            for file in os.listdir(
                f"channel_audiovisual/{youtube_key}"  # æŒ‡å®šçš„ç›®å½•
            )
            if file.endswith(youtube_media)  # ç­›é€‰æ–‡ä»¶
        ]
    except Exception:
        # å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œè®¾ç½®ä¸ºç©ºåˆ—è¡¨
        youtube_content_ytid_original = []
    try:
        # è·å–åŸå§‹XMLä¸­çš„å†…å®¹
        original_item = xmls_original[youtube_key]
        guids = re.findall(r"(?<=<guid>).+(?=</guid>)", original_item)  # æŸ¥æ‰¾æ‰€æœ‰guid
    except KeyError:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„keyï¼Œåˆ™guidsä¸ºç©º
        guids = []
    # æ„å»º URL
    youtube_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={youtube_key}"
    youtube_response = http_client(youtube_url, youtube_value)  # è¯·æ±‚YouTubeæ•°æ®
    youtube_html_playlists = None
    youtube_channel_response = None
    if youtube_response is not None and re.search(pattern_youtube404, youtube_response.text, re.DOTALL):
        youtube_url = f"https://www.youtube.com/channel/{youtube_key}"
        youtube_channel_response = http_client(youtube_url, f"{youtube_value} HTML")
        if youtube_channel_response is not None:
            pattern_youtube_error_mark = False  
            for pattern_youtube_error_key in pattern_youtube_error:
                if  pattern_youtube_error_key in youtube_channel_response.text:
                    pattern_youtube_error_mark = True
                    youtube_response = youtube_channel_response
                    break
            if not pattern_youtube_error_mark:
                # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆï¼Œæœ€å¤šé‡è¯•3æ¬¡
                for _ in range(3):
                    if youtube_html_playlists := get_youtube_html_playlists(
                        youtube_key,
                        youtube_value,
                        [elem for elem in guids if elem in youtube_content_ytid_original],  # ä»…é€‰æ‹©å·²ä¸‹è½½çš„guids
                        True,
                        channelid_youtube[youtube_value]["update_size"],
                        youtube_content_ytid_original
                    ):
                        break
        shorts_ytid = []
    elif youtube_response is not None and channelid_youtube[youtube_value]["NoShorts"]:
        shorts_ytid = get_youtube_shorts_id(youtube_key, youtube_value)
        global video_id_failed
        video_id_failed += shorts_ytid  # å°†Shortsè§†é¢‘æ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨ä¸­
    else:
        shorts_ytid = []
    # è¯»å–åŸYoutubeé¢‘é“xmlæ–‡ä»¶å¹¶åˆ¤æ–­æ˜¯å¦è¦æ›´æ–°
    try:
        with open(
            f"channel_id/{youtube_key}.txt", "r", encoding="utf-8"  # ä»¥utf-8ç¼–ç æ‰“å¼€æ–‡ä»¶
        ) as file:
            youtube_content_original = file.read()  # è¯»å–æ–‡ä»¶å†…å®¹
            youtube_content_original_clean = vary_replace(pattern_youtube_varys, youtube_content_original)  # æ¸…æ´—å†…å®¹
    except FileNotFoundError:  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
        youtube_content_original = None
        youtube_content_original_clean = None
    if youtube_html_playlists is not None:  # å¦‚æœæœ‰æ–°æ’­æ”¾åˆ—è¡¨
        channelid_youtube_rss[youtube_key] = {"content": youtube_html_playlists, "type": "dict"}
        if youtube_html_playlists["item"]:
            channelid_youtube_ids_update[youtube_key] = youtube_value  # æ›´æ–°æ ‡è¯†
        youtube_content_ytid = youtube_html_playlists["list"]  # è·å–è§†é¢‘IDåˆ—è¡¨
    else:
        if youtube_response is not None:
            # å¦‚æœæ²¡æœ‰æ–°çš„æ’­æ”¾åˆ—è¡¨ï¼Œä½†å“åº”æœ‰æ•ˆ
            channelid_youtube_rss[youtube_key] = {"content": youtube_response, "type": "html"}
            youtube_content = youtube_response.text  # è·å–å“åº”å†…å®¹
            if not youtube_channel_response:
                youtube_content_clean = vary_replace(pattern_youtube_varys, youtube_content)  # æ¸…æ´—å†…å®¹
                if youtube_content_clean != youtube_content_original_clean and youtube_response:  # åˆ¤æ–­æ˜¯å¦è¦æ›´æ–°
                    channelid_youtube_ids_update[youtube_key] = youtube_value  # æ›´æ–°æ ‡è¯†
        else:
            # å¦‚æœæ²¡æœ‰å“åº”ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            channelid_youtube_rss[youtube_key] = {"content": youtube_content_original, "type": "text"}
            youtube_content = youtube_content_original
        try:
            # ä»å†…å®¹ä¸­æå–è§†é¢‘ID
            youtube_content_ytid = re.findall(
                r"(?<=<id>yt:video:).{11}(?=</id>)", youtube_content
            )
        except TypeError:
            youtube_content_ytid = []  # å¤„ç†ç±»å‹é”™è¯¯
        youtube_content_ytid = youtube_content_ytid[
            : channelid_youtube[youtube_value]["update_size"]  # é™åˆ¶è§†é¢‘IDæ•°é‡
        ]
    youtube_content_new = list_merge_tidy(youtube_content_ytid, guids)  # åˆå¹¶å¹¶å»é‡
    if youtube_content_ytid := [
        exclude
        for exclude in youtube_content_ytid
        if exclude not in youtube_content_ytid_original and exclude not in shorts_ytid  # ä»…é€‰æ‹©æ–°è§†é¢‘ID(å¹¶ä¸”ä¸æ˜¯Shorts)
    ]:
        channelid_youtube_ids_update[youtube_key] = youtube_value  # æ›´æ–°æ ‡è¯†
        youtube_content_ytid_update[youtube_key] = youtube_content_ytid  # ä¿å­˜æ›´æ–°çš„è§†é¢‘ID
    # å‘åæ›´æ–°
    if channelid_youtube[youtube_value]["BackwardUpdate"] and guids:
        # è®¡ç®—å‘åæ›´æ–°çš„æ•°é‡
        backward_update_size = channelid_youtube[youtube_value]["last_size"] - len(youtube_content_new)
        if backward_update_size > 0:
            for _ in range(3):
                # è·å–å†å²æ’­æ”¾åˆ—è¡¨
                if youtube_html_backward_playlists := get_youtube_html_playlists(
                    youtube_key,
                    youtube_value,
                    guids,
                    False,
                    min(backward_update_size, channelid_youtube[youtube_value]["BackwardUpdate_size"]),
                    youtube_content_ytid_original
                ):
                    break
            backward_list = youtube_html_backward_playlists["list"]  # è·å–å‘åæ›´æ–°çš„åˆ—è¡¨
            for guid in backward_list.copy():
                if guid in youtube_content_new:
                    backward_list.remove(guid)  # ä»åˆ—è¡¨ä¸­ç§»é™¤å·²æ›´æ–°çš„GUID
            if youtube_html_backward_playlists and backward_list:
                channelid_youtube_ids_update[youtube_key] = youtube_value  # æ›´æ–°æ ‡è¯†
                channelid_youtube_rss[youtube_key].update({"backward": youtube_html_backward_playlists})  # æ·»åŠ å‘åæ›´æ–°å†…å®¹
                youtube_content_ytid_backward = []
                for guid in backward_list:
                    if guid not in youtube_content_ytid_original:
                        youtube_content_ytid_backward.append(guid)  # æ”¶é›†æœªä¸‹è½½çš„GUID
                if youtube_content_ytid_backward:
                    youtube_content_ytid_backward_update[youtube_key] = youtube_content_ytid_backward  # ä¿å­˜å‘åæ›´æ–°çš„ID

# è·å–bvæ‰€æœ‰çš„åˆ†Pä¿¡æ¯æ¨¡å—
def get_bilibili_all_part(bvid, bilibili_value):
    bvid_part = []
    if bvid_response := http_client(
        "https://api.bilibili.com/x/player/pagelist",
        f"{bilibili_value}|{bvid}",
        10,
        4,
        True,
        None,
        {"bvid": bvid}
    ):
        bvid_json = bvid_response.json()
        bvid_data = bvid_json["data"]
    else:
        bvid_data =[]
    if len(bvid_data) > 1:
        for part in bvid_data:
            bvid_part.append({
                "cid": part["cid"],
                "page": part["page"],
                "part": part["part"],
                "duration": part["duration"],
                "dimension": part["dimension"],
                "first_frame": part["first_frame"],
            })
    bvid_part.sort(key=lambda x: x["page"], reverse=True)
    return bvid_part

# è·å–bvæ‰€æœ‰çš„äº’åŠ¨è§†é¢‘ä¿¡æ¯æ¨¡å—
def get_bilibili_interactive(bvid, bilibili_value):
    bvid_part = []
    bvid_cid = []
    bvid_cid_choices = []
    if pagelist_response :=http_client(
        "https://api.bilibili.com/x/player/pagelist",
        f"{bilibili_value}|{bvid}",
        10,
        4,
        True,
        None,
        {"bvid": bvid, "jsonp": "jsonp"},
    ):
        pagelist = pagelist_response.json()
        pagelist_cid = pagelist["data"][0]["cid"]
        if playerwbi_response :=http_client(
            "https://api.bilibili.com/x/player/wbi/v2",
            f"{bilibili_value}|{bvid}",
            10,
            4,
            True,
            None,
            {"cid": pagelist_cid, "bvid": bvid},
        ):
            playerwbi = playerwbi_response.json()
            graph_version = playerwbi["data"]["interaction"]["graph_version"]
        else: 
            graph_version = ""
    else: 
        graph_version = ""
    def get_edge_info(bvid, bilibili_value, graph_version, edge_id):
        if edgeinfo_v2_response :=http_client(
            "https://api.bilibili.com/x/stein/edgeinfo_v2",
            f"{bilibili_value}|{bvid}",
            10,
            4,
            True,
            None,
            {"bvid": bvid, "graph_version": graph_version, "edge_id": edge_id}
        ):
            edgeinfo_v2 = edgeinfo_v2_response.json()
            return edgeinfo_v2["data"]
    def get_choices(data):
        options = []
        options_cid = []
        if "questions" in data["edges"]:
            for question in data["edges"]["questions"]:
                if "choices" in question:
                    for choice in question["choices"]:
                        if choice["cid"] not in bvid_cid and choice["cid"] not in bvid_cid_choices:
                            bvid_cid_choices.append({
                                "cid": choice["cid"],
                                "edge_id": choice["id"],
                                "option": choice["option"],
                            })
                            options.append(choice["option"])
                            options_cid.append(choice["cid"])
        return options, options_cid
    if graph_version:
        data_1 = get_edge_info(bvid, bilibili_value, graph_version, "1")
        for story_list in data_1["story_list"]:
            if story_list["edge_id"] == 1:
                story_list_1 = story_list
                break
        options, options_cid = get_choices(data_1)
        bvid_part.append({
            "cid": story_list_1["cid"],
            "title": data_1["title"],
            "edge_id": story_list_1["edge_id"],
            "first_frame": f"http://i0.hdslb.com/bfs/steins-gate/{story_list_1['cid']}_screenshot.jpg",
            "options": options,
            "options_cid": options_cid,
            "num": 1,
        })
        bvid_cid.append(story_list_1["cid"])
        while len(bvid_cid_choices) != 0:
            if bvid_cid_choices[0]["cid"] not in bvid_cid:
                data = get_edge_info(bvid, bilibili_value, graph_version, bvid_cid_choices[0]["edge_id"])
                options, options_cid = get_choices(data)
                bvid_part.append({
                    "cid": bvid_cid_choices[0]["cid"],
                    "title": data["title"],
                    "edge_id": bvid_cid_choices[0]["edge_id"],
                    "first_frame": f"http://i0.hdslb.com/bfs/steins-gate/{bvid_cid_choices[0]['cid']}_screenshot.jpg",
                    "options": options,
                    "options_cid": options_cid,
                    "num": len(bvid_part) + 1
                })
                bvid_cid.append(bvid_cid_choices[0]["cid"])
            del bvid_cid_choices[0]
    bvid_part.sort(key=lambda x: x["num"], reverse=True)
    return bvid_part

# æŸ¥è¯¢å“”å“©å“”å“©ç”¨æˆ·æŠ•ç¨¿è§†é¢‘æ˜ç»†æ¨¡å—
def get_bilibili_vlist(bilibili_key, bilibili_value, num=1, all_part_judgement=False):
    bilibili_list = []
    bilibili_entry = {}
    if bilibili_response := http_client(
        "https://api.bilibili.com/x/space/wbi/arc/search",
        bilibili_value,
        10,
        4,
        True,
        bilibili_data["cookie"],
        WBI_signature(
            {
                "mid": bilibili_key,
                "pn": str(num),
                "ps": "25",
            },
            bilibili_data["img_key"],
            bilibili_data["sub_key"],
        )
    ):
        bilibili_json = bilibili_response.json()
        bilibili_vlists = bilibili_json["data"]["list"]["vlist"]
        for vlist in bilibili_vlists:
            try:
                bilibili_entry[vlist["bvid"]] = {
                    "aid": vlist["aid"],
                    "author": vlist["author"],
                    "bvid": vlist["bvid"],
                    "copyright": vlist["copyright"],
                    "created": vlist["created"],
                    "description": vlist["description"],
                    "is_union_video": vlist["is_union_video"],
                    "length": vlist["length"],
                    "mid": vlist["mid"],
                    "pic": vlist["pic"],
                    "title": vlist["title"],
                    "typeid": vlist["typeid"],
                }
                bilibili_list.append(vlist["bvid"])
            except (KeyError, TypeError, IndexError, ValueError):
                pass
    if all_part_judgement and bilibili_list:
        def all_part(bvid):
            if bvid_part := get_bilibili_all_part(bvid, bilibili_value):
                bilibili_entry[bvid]["part"] = bvid_part
            elif bvid_edgeinfo := get_bilibili_interactive(bvid, bilibili_value):
                bilibili_entry[bvid]["edgeinfo"] = bvid_edgeinfo
        # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹åˆ—è¡¨
        threads = []
        for bvid in bilibili_list:
            thread = threading.Thread(target=all_part, args=(bvid,))
            threads.append(thread)
            thread.start()
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
    return bilibili_entry, bilibili_list

# æ›´æ–°å“”å“©å“”å“©é¢‘é“jsonæ¨¡å—
def bilibili_json_update(bilibili_key, bilibili_value):
    bilibili_space = {}
    bilibili_lists = []
    bilibili_entrys = {}
    # ç”¨æˆ·åç‰‡ä¿¡æ¯
    if bilibili_card_response := http_client(
        "https://api.bilibili.com/x/web-interface/card",
        bilibili_value,
        10,
        4,
        True,
        bilibili_data["cookie"],
        {
            "mid": bilibili_key,
            "photo": "true",
        },
    ):
        bilibili_card_json = bilibili_card_response.json()
        try:
            if bilibili_card_json["code"] == 0:
                bilibili_space = {
                    "mid": bilibili_card_json["data"]["card"]["mid"],
                    "name": bilibili_card_json["data"]["card"]["name"],
                    "sex": bilibili_card_json["data"]["card"]["sex"],
                    "face": bilibili_card_json["data"]["card"]["face"],
                    "spacesta": bilibili_card_json["data"]["card"]["spacesta"],
                    "sign": bilibili_card_json["data"]["card"]["sign"],
                    "Official": bilibili_card_json["data"]["card"]["Official"],
                    "official_verify": bilibili_card_json["data"]["card"]["official_verify"],
                }
            else:
                return bilibili_card_json["code"]
        except (KeyError, TypeError, IndexError, ValueError):
            pass
    else:
        return None
    # æŸ¥è¯¢å“”å“©å“”å“©ç”¨æˆ·æŠ•ç¨¿è§†é¢‘æ˜ç»†
    for num in range(math.ceil(channelid_bilibili[bilibili_value]["update_size"] / 25)):
        num += 1
        bilibili_entry, bilibili_list = get_bilibili_vlist(
            bilibili_key,
            f"{bilibili_value}ç¬¬{num}é¡µ",
            num,
            channelid_bilibili[bilibili_value]["AllPartGet"]
        )
        bilibili_entrys = bilibili_entrys | bilibili_entry
        bilibili_lists += bilibili_list
    bilibili_space["entry"] = bilibili_entrys
    bilibili_space["list"] = bilibili_lists
    return bilibili_space

# æ›´æ–°å“”å“©å“”å“©é¢‘é“xmlæ¨¡å—
def bilibili_rss_update(bilibili_key, bilibili_value):
    # è·å–å·²ä¸‹è½½æ–‡ä»¶åˆ—è¡¨
    bilibili_content_bvid_original = get_file_list(bilibili_key, channelid_bilibili[bilibili_value]["media"])[0]
    # è·å–åŸxmlä¸­æ–‡ä»¶åˆ—è¡¨
    try:
        original_item = xmls_original[bilibili_key]  # å°è¯•è·å–åŸå§‹çš„xmlå†…å®¹
        guids = list_merge_tidy(re.findall(r"(?<=<guid>).+(?=</guid>)", original_item), [], 12)  # ä»xmlä¸­æå–guid
    except KeyError:
        guids = []  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„keyï¼Œåˆ™åˆå§‹åŒ–guidsä¸ºç©ºåˆ—è¡¨
    bilibili_space = bilibili_json_update(bilibili_key, bilibili_value)  # æ›´æ–°bilibiliç›¸å…³çš„jsonå†…å®¹
    # è¯»å–åŸå“”å“©å“”å“©é¢‘é“xmlæ–‡ä»¶å¹¶åˆ¤æ–­æ˜¯å¦è¦æ›´æ–°
    try:
        with open(
            f"channel_id/{bilibili_key}.json", "r", encoding="utf-8"  # æ‰“å¼€æŒ‡å®šçš„jsonæ–‡ä»¶
        ) as file:
            bilibili_space_original = json.load(file)  # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è§£ææˆå­—å…¸
    except FileNotFoundError:  # æ•è·æ–‡ä»¶ä¸å­˜åœ¨å¼‚å¸¸
        bilibili_space_original = {}  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©ºå­—å…¸
    except json.decoder.JSONDecodeError:  # æ•è·jsonè§£ç é”™è¯¯
        bilibili_space_original = {}  # å¦‚æœjsonè¯»å–å¤±è´¥ï¼Œåˆå§‹åŒ–ä¸ºç©ºå­—å…¸
    # æ ¹æ®æ›´æ–°æ¡ä»¶æ›´æ–°é¢‘é“æ•°æ®
    if bilibili_space == -404:  # æ£€æŸ¥æ›´æ–°çŠ¶æ€
        channelid_bilibili_rss[bilibili_key] = {"content": bilibili_space, "type": "int"}  # è®¾ç½®ä¸ºæ•´å‹å†…å®¹
    elif bilibili_space is None:
        channelid_bilibili_rss[bilibili_key] = {"content": bilibili_space_original, "type": "json"}  # ä½¿ç”¨åŸå§‹jsonå†…å®¹
    else:
        channelid_bilibili_rss[bilibili_key] = {"content": bilibili_space, "type": "dict"}  # è®¾ç½®ä¸ºå­—å…¸ç±»å‹å†…å®¹
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°IDåˆ—è¡¨
        if bilibili_space != bilibili_space_original:
            channelid_bilibili_ids_update[bilibili_key] = bilibili_value  # æ›´æ–°ID
        # è·å–éœ€è¦æ›´æ–°çš„å†…å®¹åˆ—è¡¨
        bilibili_content_bvid = bilibili_space["list"][:channelid_bilibili[bilibili_value]["update_size"]]
        bilibili_space_new = list_merge_tidy(bilibili_content_bvid, guids)  # åˆå¹¶æ–°å†…å®¹å’ŒåŸå†…å®¹
        # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰å˜åŠ¨
        if bilibili_content_bvid := [
            exclude
            for exclude in bilibili_content_bvid
            if exclude not in bilibili_content_bvid_original  # ç­›é€‰æ–°å¢çš„å†…å®¹
        ]:
            channelid_bilibili_ids_update[bilibili_key] = bilibili_value  # éœ€è¦æ›´æ–°ID
            bilibili_content_bvid_update[bilibili_key] = bilibili_content_bvid  # æ›´æ–°æ–°å¢å†…å®¹
        # å‘åæ›´æ–°
        if channelid_bilibili[bilibili_value]["BackwardUpdate"] and guids:  # å¦‚æœè®¾ç½®äº†å‘åæ›´æ–°
            backward_update_size = channelid_bilibili[bilibili_value]["last_size"] - len(bilibili_space_new)  # è®¡ç®—éœ€è¦å‘åæ›´æ–°çš„æ•°é‡
            if backward_update_size > 0:
                backward_update_size = min(backward_update_size, channelid_bilibili[bilibili_value]["BackwardUpdate_size"])  # é™åˆ¶æ›´æ–°æ•°é‡
                backward_update_page_start = math.ceil(len(bilibili_space_new) / 25)  # ç¡®å®šå¼€å§‹é¡µé¢
                backward_update_page_end = math.ceil((len(bilibili_space_new) + backward_update_size) / 25)  # ç¡®å®šç»“æŸé¡µé¢
                backward_entry = {}  # åˆå§‹åŒ–å‘åæ›´æ–°çš„æ¡ç›®
                backward_list = []  # åˆå§‹åŒ–å‘åæ›´æ–°çš„åˆ—è¡¨
                # å¾ªç¯æ›´æ–°æ¯ä¸€é¡µçš„å†…å®¹
                for num in range(backward_update_page_start, backward_update_page_end + 1):
                    backward_entry_part, backward_list_part = get_bilibili_vlist(bilibili_key, bilibili_value, num)  # è·å–å…·ä½“å†…å®¹
                    backward_entry = backward_entry | backward_entry_part  # åˆå¹¶æ¡ç›®
                    backward_list += backward_list_part  # åˆå¹¶åˆ—è¡¨
                # æ£€æŸ¥æ¡ç›®å’Œåˆ—è¡¨æ˜¯å¦æœ‰æ•ˆ
                if backward_entry and backward_list and guids[-1] in backward_list:
                    try:
                        backward_list_start = backward_list.index(guids[-1]) + 1  # è·å–guidsçš„èµ·å§‹ç´¢å¼•
                        backward_list = backward_list[backward_list_start:][:backward_update_size]  # æ›´æ–°å‘ååˆ—è¡¨
                    except ValueError:
                        backward_list = []  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ¸…ç©ºåˆ—è¡¨
                    # æ ¹æ®æ¡ä»¶ç§»é™¤å·²ç»å­˜åœ¨çš„å…ƒç´ 
                    for guid in backward_list.copy():
                        if guid in bilibili_space_new:
                            backward_list.remove(guid)  # ç§»é™¤å·²å­˜åœ¨çš„æ¡ç›®
                    # å¦‚æœæœ‰å‘åæ¡ç›®éœ€è¦æ›´æ–°
                    if backward_list:
                        if channelid_bilibili[bilibili_value]["AllPartGet"]:  # å¦‚æœéœ€è¦è·å–æ‰€æœ‰éƒ¨åˆ†
                            def backward_all_part(guid):
                                if guid_part := get_bilibili_all_part(guid, bilibili_value):  # è·å–å½“å‰å†…å®¹çš„å…¨éƒ¨éƒ¨åˆ†
                                    backward_entry[guid]["part"] = guid_part  # æ›´æ–°æ¡ç›®
                                elif guid_edgeinfos := get_bilibili_interactive(guid, bilibili_value):  # è·å–äº¤äº’ä¿¡æ¯
                                    backward_entry[guid]["edgeinfo"] = guid_edgeinfos  # æ›´æ–°è¾¹ç¼˜ä¿¡æ¯
                            # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹åˆ—è¡¨
                            threads = []
                            for guid in backward_entry:
                                thread = threading.Thread(target=backward_all_part, args=(guid,))  # ä¸ºæ¯ä¸ªæ¡ç›®åˆ›å»ºçº¿ç¨‹
                                threads.append(thread)  # æ·»åŠ çº¿ç¨‹åˆ°åˆ—è¡¨
                                thread.start()  # å¯åŠ¨çº¿ç¨‹
                            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
                            for thread in threads:
                                thread.join()
                        # æ›´æ–°é¢‘é“ä¿¡æ¯
                        channelid_bilibili_rss[bilibili_key].update({"backward": {"list": backward_list, "entry": backward_entry}})
                        channelid_bilibili_ids_update[bilibili_key] = bilibili_value  # æ ‡è®°IDæ›´æ–°
                        bilibili_content_bvid_backward = []  # åˆå§‹åŒ–å‘åæ›´æ–°çš„å†…å®¹åˆ—è¡¨
                        for guid in backward_list:
                            if guid not in bilibili_content_bvid_original:  # æ£€æŸ¥æ–°å¢çš„å†…å®¹
                                bilibili_content_bvid_backward.append(guid)  # æ·»åŠ åˆ°å‘åæ›´æ–°åˆ—è¡¨
                        if bilibili_content_bvid_backward:
                            bilibili_content_bvid_backward_update[bilibili_key] = bilibili_content_bvid_backward  # æ›´æ–°æœ€ç»ˆçš„å‘åæ›´æ–°å†…å®¹

# æ›´æ–°Youtubeå’Œå“”å“©å“”å“©é¢‘é“xmlå¤šçº¿ç¨‹æ¨¡å—
def update_youtube_bilibili_rss():
    pattern_youtube404 = r"Error 404 \(Not Found\)"  # è®¾ç½®è¦åŒ¹é…çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    pattern_youtube_error = {
        "This channel was removed because it violated our Community Guidelines.": "è¿åç¤¾åŒºå‡†åˆ™",
        "This channel does not exist.": "ä¸å­˜åœ¨ï¼ˆIDé”™è¯¯ï¼‰",
    }
    pattern_youtube_varys = [
        r"[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-2][0-9]:[0-6][0-9]:[0-6][0-9]\+00:00",
        r'starRating count="[0-9]*"',
        r'statistics views="[0-9]*"',
        r"<id>yt:channel:(UC)?(.{22})?</id>",
        r"<yt:channelId>(UC)?(.{22})?</yt:channelId>",
    ]
    youtube_bilibili_rss_update_threads = []  # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
    # Youtubeå¤šçº¿ç¨‹
    for youtube_key, youtube_value in channelid_youtube_ids.items():
        thread = threading.Thread(
            target=youtube_rss_update, args=(youtube_key, youtube_value, pattern_youtube_varys, pattern_youtube404, pattern_youtube_error)
        )
        youtube_bilibili_rss_update_threads.append(thread)
        # å¼€å§‹å¤šçº¿ç¨‹
        thread.start()
    # å“”å“©å“”å“©å¤šçº¿ç¨‹
    for bilibili_key, bilibili_value in channelid_bilibili_ids.items():
        thread = threading.Thread(
            target=bilibili_rss_update, args=(bilibili_key, bilibili_value)
        )
        youtube_bilibili_rss_update_threads.append(thread)
        # å¼€å§‹å¤šçº¿ç¨‹
        thread.start()
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in youtube_bilibili_rss_update_threads:
        thread.join()
    # å¯»æ‰¾é”™è¯¯åŸå› 
    def youtube_error(youtube_content, pattern_youtube_error):
        for pattern_youtube_error_key, pattern_youtube_error_value in pattern_youtube_error.items():
            if pattern_youtube_error_key in youtube_content:
                return pattern_youtube_error_value
    # æ›´æ–°Youtubeé¢‘é“
    for youtube_key, youtube_value in channelid_youtube_ids.copy().items():
        youtube_response = channelid_youtube_rss[youtube_key]["content"]
        youtube_response_type = channelid_youtube_rss[youtube_key]["type"]
        # xmlåˆ†ç±»åŠå­˜å‚¨
        if youtube_response is not None:
            if youtube_response_type ==  "dict":
                # æ„å»ºé¢‘é“æ–‡ä»¶å¤¹
                folder_build(youtube_key, "channel_audiovisual")
            else:
                if youtube_response_type ==  "html":
                    youtube_content = youtube_response.text
                elif youtube_response_type ==  "text":
                    youtube_content = youtube_response
                    write_log(f"YouTubeé¢‘é“ {youtube_value} æ— æ³•æ›´æ–°")
                else:
                    youtube_content = ""
                # åˆ¤æ–­é¢‘é“idæ˜¯å¦æ­£ç¡®
                if re.search(pattern_youtube404, youtube_content, re.DOTALL):
                    del channelid_youtube_ids[youtube_key]  # åˆ é™¤é”™è¯¯ID
                    write_log(f"YouTubeé¢‘é“ {youtube_value} IDä¸æ­£ç¡®æ— æ³•è·å–")
                elif youtube_error_message := youtube_error(youtube_content, pattern_youtube_error):
                    del channelid_youtube_ids[youtube_key]  # åˆ é™¤é”™è¯¯ID
                    write_log(f"YouTubeé¢‘é“ {youtube_value} {youtube_error_message}")
                else:
                    # æ„å»ºæ–‡ä»¶
                    file_save(youtube_content, f"{youtube_key}.txt", "channel_id")
                    # æ„å»ºé¢‘é“æ–‡ä»¶å¤¹
                    folder_build(youtube_key, "channel_audiovisual")
        else:
            if youtube_response_type == "text":
                del channelid_youtube_ids[youtube_key]
            write_log(f"YouTubeé¢‘é“ {youtube_value} æ— æ³•æ›´æ–°")
    # æ›´æ–°å“”å“©å“”å“©é¢‘é“
    for bilibili_key, bilibili_value in channelid_bilibili_ids.copy().items():
        bilibili_space = channelid_bilibili_rss[bilibili_key]["content"]
        bilibili_space_type = channelid_bilibili_rss[bilibili_key]["type"]
        # xmlåˆ†ç±»åŠå­˜å‚¨
        if bilibili_space_type == "int":
            del channelid_bilibili_ids[bilibili_key]  # åˆ é™¤é”™è¯¯ID
            write_log(f"BiliBilié¢‘é“ {bilibili_value} IDä¸æ­£ç¡®æ— æ³•è·å–")
        elif bilibili_space_type == "json":
            write_log(f"BiliBilié¢‘é“ {youtube_value} æ— æ³•æ›´æ–°")
            if bilibili_space == {}:
                del channelid_bilibili_ids[bilibili_key]
        else:
            # æ„å»ºæ–‡ä»¶
            file_save(bilibili_space, f"{bilibili_key}.json", "channel_id")
            # æ„å»ºé¢‘é“æ–‡ä»¶å¤¹
            folder_build(bilibili_key, "channel_audiovisual")

# åˆ¤æ–­æ˜¯å¦é‡è¯•æ¨¡å—
def want_retry(video_url, num=1):
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    pattern = rf'\|{video_url}\|(è¯•çœ‹|è·³è¿‡æ›´æ–°|åˆ é™¤æˆ–å—é™|ç›´æ’­é¢„çº¦\|a few moments\.)'
    # è¯»å– Podflow.log æ–‡ä»¶
    try:
        with open('Podflow.log', 'r', encoding='utf-8') as file:
            content = file.read()  # è¯»å–æ–‡ä»¶å†…å®¹
        # ä½¿ç”¨ re.findall() æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
        matches = re.findall(pattern, content)
        # è®¡ç®—åŒ¹é…çš„ä¸ªæ•°
        count = len(matches)
    except (FileNotFoundError, Exception):
        count = 0
    if count < num or count % num == 0:
        return True
    else:
        return False

# è¾“å‡ºéœ€è¦æ›´æ–°çš„ä¿¡æ¯æ¨¡å—
def update_information_display(channelid_ids_update, content_id_update, content_id_backward_update, name):
    if channelid_ids_update:
        print_channelid_ids_update = f"éœ€æ›´æ–°çš„{name}é¢‘é“:\n"
        # è·å–å‘½ä»¤è¡Œå­—èŠ‚å®½åº¦
        try:
            terminal_width = os.get_terminal_size().columns
        except OSError:
            terminal_width = 47
        # å°è¯•æ‹†åˆ†è¾“å‡º
        try:
            for channelid_key, channelid_value in channelid_ids_update.items():
                if len(print_channelid_ids_update) != len(name) + 8:
                    if (
                        len(
                            re.sub(
                                r"\033\[[0-9;]+m",
                                "",
                                print_channelid_ids_update.split("\n")[-1],
                            ).encode("GBK")
                        )
                        + len(f" | {channelid_value}".encode("utf-8"))
                        <= terminal_width
                    ):
                        print_channelid_ids_update += " | "
                    else:
                        print_channelid_ids_update += "\n"
                if channelid_key in content_id_update and channelid_key in content_id_backward_update:
                    print_channelid_ids_update += (
                        f"\033[34m{channelid_value}\033[0m"
                    )
                elif channelid_key in content_id_update:
                    print_channelid_ids_update += (
                        f"\033[32m{channelid_value}\033[0m"
                    )
                elif channelid_key in content_id_backward_update:
                    print_channelid_ids_update += (
                        f"\033[36m{channelid_value}\033[0m"
                    )
                else:
                    print_channelid_ids_update += (
                        f"\033[33m{channelid_value}\033[0m"
                    )
        # å¦‚æœå«æœ‰ç‰¹æ®Šå­—ç¬¦å°†ä½¿ç”¨æ­¤è¾“å‡º
        except Exception:
            len_channelid_ids_update = len(channelid_ids_update)
            count_channelid_ids_update = 1
            for channelid_key, channelid_value in channelid_ids_update.items():
                if channelid_key in content_id_update and channelid_key in content_id_backward_update:
                    print_channelid_ids_update += (
                        f"\033[34m{channelid_value}\033[0m"
                    )
                elif channelid_key in content_id_update:
                    print_channelid_ids_update += (
                        f"\033[32m{channelid_value}\033[0m"
                    )
                elif channelid_key in content_id_backward_update:
                    print_channelid_ids_update += (
                        f"\033[36m{channelid_value}\033[0m"
                    )
                else:
                    print_channelid_ids_update += (
                        f"\033[33m{channelid_value}\033[0m"
                    )
                if count_channelid_ids_update != len_channelid_ids_update:
                    if count_channelid_ids_update % 2 != 0:
                        print_channelid_ids_update += " | "
                    else:
                        print_channelid_ids_update += "\n"
                    count_channelid_ids_update += 1
        write_log(print_channelid_ids_update)

# YouTube&å“”å“©å“”å“©è§†é¢‘ä¿¡æ¯æ¨¡å—
def get_youtube_and_bilibili_video_format(id, stop_flag, video_format_lock, prepare_animation):
    id_update_format = media_format(
        video_id_update_format[id]["url"],
        id,
        video_id_update_format[id]["media"],
        video_id_update_format[id]["quality"],
        video_id_update_format[id]["cookie"],
    )
    if "å¹´é¾„é™åˆ¶" in id_update_format:
        if youtube_cookie:
            video_id_update_format[id]["cookie"] = "channel_data/yt_dlp_youtube.txt"
            id_update_format = media_format(
                video_id_update_format[id]["url"],
                id,
                video_id_update_format[id]["media"],
                video_id_update_format[id]["quality"],
                video_id_update_format[id]["cookie"],
            )
            if "å¹´é¾„é™åˆ¶" in id_update_format:
                id_update_format = "\x1b[31må¹´é¾„é™åˆ¶\x1b[0m(Cookiesé”™è¯¯)"
        else:
            id_update_format = "\x1b[31må¹´é¾„é™åˆ¶\x1b[0m(éœ€è¦Cookies)"
    if isinstance(id_update_format, list):
        if len(id_update_format) == 1:
            entry_id_update_format = id_update_format[0]
            video_id_update_format[id]["url"] = entry_id_update_format["url"]
            video_id_update_format[id]["format"] = entry_id_update_format["duration_and_id"]
            video_id_update_format[id]["title"] = entry_id_update_format["title"]
            video_id_update_format[id]["timestamp"] = entry_id_update_format["timestamp"]
            video_id_update_format[id]["description"] = entry_id_update_format["description"]
            video_id_update_format[id]["main"] = id
            video_id_update_format[id]["image"] = entry_id_update_format["image"]
            video_id_update_format[id]["download"] = entry_id_update_format["download"]
        else:
            entrys_id = []
            for entry_id_update_format in id_update_format:
                entry_id = entry_id_update_format["id"]
                entrys_id.append(entry_id)
                video_id_update_format[entry_id] = {
                    "id": video_id_update_format[id]["id"],
                    "media": video_id_update_format[id]["media"],
                    "quality": video_id_update_format[id]["quality"],
                    "url": entry_id_update_format["url"],
                    "name": video_id_update_format[id]["name"],
                    "cookie": video_id_update_format[id]["cookie"],
                    "format": entry_id_update_format["duration_and_id"],
                    "title": entry_id_update_format["title"],
                    "timestamp": entry_id_update_format["timestamp"],
                    "description": entry_id_update_format["description"],
                    "main": id,
                    "image": entry_id_update_format["image"],
                    "download": entry_id_update_format["download"],
                    "backward_update": video_id_update_format[id]["backward_update"],
                }
            video_id_update_format[id] = entrys_id
    else:
        with video_format_lock:
            stop_flag[0] = "error"
            prepare_animation.join()
            video_id_failed.append(id)
            write_log(
                f"{video_id_update_format[id]['name']}|{id}|{id_update_format}",
                None,
                True,
                False,
            )
            del video_id_update_format[id]

# YouTube&å“”å“©å“”å“©è·å–è§†é¢‘ä¿¡æ¯å¤šçº¿ç¨‹æ¨¡å—
def get_youtube_and_bilibili_video_format_multithreading(video_id_update_format_item, wait_animation_display_info):
    # åˆ›å»ºå…±äº«çš„æ ‡å¿—å˜é‡
    stop_flag = ["keep"]  # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨æ ‡å¿—å˜é‡
    # åˆ›å»ºä¸¤ä¸ªçº¿ç¨‹åˆ†åˆ«è¿è¡Œç­‰å¾…åŠ¨ç”»å’Œå…¶ä»–ä»£ç ï¼Œå¹¶ä¼ é€’å…±äº«çš„æ ‡å¿—å˜é‡
    prepare_animation = threading.Thread(target=wait_animation, args=(stop_flag, wait_animation_display_info,))
    # å¯åŠ¨åŠ¨ç”»çº¿ç¨‹
    prepare_animation.start()
    # åˆ›å»ºçº¿ç¨‹é”
    video_format_lock = threading.Lock()
    # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
    video_id_update_threads = []
    for video_id in video_id_update_format_item.keys():
        thread = threading.Thread(target=get_youtube_and_bilibili_video_format, args=(video_id, stop_flag, video_format_lock, prepare_animation))
        video_id_update_threads.append(thread)
        thread.start()
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in video_id_update_threads:
        thread.join()
    stop_flag[0] = "end"
    prepare_animation.join()

# è·å–YouTube&å“”å“©å“”å“©è§†é¢‘æ ¼å¼ä¿¡æ¯æ¨¡å—
def get_video_format():
    def get_youtube_format_front(ytid_content_update, backward_update):
        for ytid_key, ytid_value in ytid_content_update.items():
            # è·å–å¯¹åº”æ–‡ä»¶ç±»å‹
            yt_id_file = channelid_youtube[channelid_youtube_ids_update[ytid_key]]["media"]
            yt_id_failed_count = channelid_youtube[channelid_youtube_ids_update[ytid_key]]["want_retry_count"]
            # å¦‚æœä¸ºè§†é¢‘æ ¼å¼è·å–åˆ†è¾¨ç‡
            if yt_id_file == "mp4":
                yt_id_quality = channelid_youtube[channelid_youtube_ids_update[ytid_key]][
                    "quality"
                ]
            else:
                yt_id_quality = None
            for yt_id in ytid_value:
                if want_retry(yt_id, yt_id_failed_count):
                    yt_id_format = {
                        "id": ytid_key,
                        "media": yt_id_file,
                        "quality": yt_id_quality,
                        "url": f"https://www.youtube.com/watch?v={yt_id}",
                        "name": channelid_youtube_ids[ytid_key],
                        "cookie": None,  # ç‰¹å®šè§†é¢‘éœ€è¦
                        "backward_update": backward_update,
                    }
                    video_id_update_format[yt_id] = yt_id_format
                else:
                    video_id_failed.append(yt_id)
                    write_log(
                        f"{channelid_youtube_ids[ytid_key]}|{yt_id}|è·³è¿‡æ›´æ–°",
                        None,
                        False,
                    )
    def get_bilibili_format_front(bvid_content_update, backward_update):
        for bvid_key, bvid_value in bvid_content_update.items():
            # è·å–å¯¹åº”æ–‡ä»¶ç±»å‹
            bv_id_file = channelid_bilibili[channelid_bilibili_ids_update[bvid_key]]["media"]
            bv_id_failed_count = channelid_bilibili[channelid_bilibili_ids_update[bvid_key]]["want_retry_count"]
            # å¦‚æœä¸ºè§†é¢‘æ ¼å¼è·å–åˆ†è¾¨ç‡
            if bv_id_file == "mp4":
                bv_id_quality = channelid_bilibili[channelid_bilibili_ids_update[bvid_key]][
                    "quality"
                ]
            else:
                bv_id_quality = None
            for bv_id in bvid_value:
                if want_retry(bv_id, bv_id_failed_count):
                    bv_id_format = {
                        "id": bvid_key,
                        "media": bv_id_file,
                        "quality": bv_id_quality,
                        "url": f"https://www.bilibili.com/video/{bv_id}",
                        "name": channelid_bilibili_ids[bvid_key],
                        "cookie": "channel_data/yt_dlp_bilibili.txt",
                        "backward_update": backward_update,
                    }
                    video_id_update_format[bv_id] = bv_id_format
                else:
                    video_id_failed.append(bv_id)
                    write_log(
                        f"{channelid_bilibili_ids[bvid_key]}|{bv_id}|è·³è¿‡æ›´æ–°",
                        None,
                        False,
                    )
    get_youtube_format_front(youtube_content_ytid_update, False)
    get_bilibili_format_front(bilibili_content_bvid_update, False)
    get_youtube_format_front(youtube_content_ytid_backward_update, True)
    get_bilibili_format_front(bilibili_content_bvid_backward_update, True)
    # æŒ‰å‚æ•°æ‹†åˆ†è·å–é‡
    if len(video_id_update_format) != 0:
        video_id_update_format_list = split_dict(video_id_update_format, config["preparation_per_count"])
        wait_animation_num = 1
        for video_id_update_format_item in video_id_update_format_list:
            if len(video_id_update_format_list) == 1:
                wait_animation_display_info = "åª’ä½“è§†é¢‘ "
            else:
                wait_animation_display_info = f"åª’ä½“è§†é¢‘|No.{str(wait_animation_num).zfill(2)} "
            wait_animation_num += 1
            # è·å–è§†é¢‘ä¿¡æ¯å¤šçº¿ç¨‹æ¨¡å—
            get_youtube_and_bilibili_video_format_multithreading(video_id_update_format_item, wait_animation_display_info)

# ä¸‹è½½YouTubeå’Œå“”å“©å“”å“©è§†é¢‘
def youtube_and_bilibili_download():
    for video_id in video_id_update_format.keys():
        if isinstance(video_id_update_format[video_id], dict) and video_id_update_format[video_id]["main"] not in video_id_failed:
            output_dir_name = video_id_update_format[video_id]["name"]
            if video_id_update_format[video_id]["backward_update"]:
                display_color = "\033[35m"
            else:
                display_color = "\033[95m"
            if dl_aideo_video(
                video_id,
                video_id_update_format[video_id]["id"],
                video_id_update_format[video_id]["media"],
                video_id_update_format[video_id]["format"],
                config["retry_count"],
                video_id_update_format[video_id]["download"]["url"],
                output_dir_name,
                video_id_update_format[video_id]["cookie"],
                video_id_update_format[video_id]["download"]["num"],
                display_color
            ):
                video_id_failed.append(video_id_update_format[video_id]["main"])
                write_log(
                    f"{display_color}{output_dir_name}\033[0m|{video_id} \033[31mæ— æ³•ä¸‹è½½\033[0m"
                )

# ç”ŸæˆXMLæ¨¡å—
def xml_rss(title, link, description, category, icon, items):
    # è·å–å½“å‰æ—¶é—´
    current_time_now = time.time()  # è·å–å½“å‰æ—¶é—´çš„ç§’æ•°
    # è·å–å½“å‰æ—¶åŒºå’Œå¤ä»¤æ—¶ä¿¡æ¯
    time_info_now = time.localtime(current_time_now)
    # æ„é€ æ—¶é—´å­—ç¬¦ä¸²
    formatted_time_now = time.strftime("%a, %d %b %Y %H:%M:%S %z", time_info_now)
    itunes_summary = description#.replace("\n", "&#xA;")
    if title == "Podflow":
        author = "gruel-zxz"
        subtitle = "gruel-zxz-podflow"
    else:
        author = title
        subtitle = title
    # åˆ›å»ºä¸»XMLä¿¡æ¯
    return f"""<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd">
    <channel>
        <title>{title}</title>
        <link>{link}</link>
        <description>{description}</description>
        <category>{category}</category>
        <generator>Podflow (support us at https://github.com/gruel-zxz/podflow)</generator>
        <language>en-us</language>
        <lastBuildDate>{formatted_time_now}</lastBuildDate>
        <pubDate>Sun, 24 Apr 2005 11:20:54 +0800</pubDate>
        <image>
            <url>{icon}</url>
            <title>{title}</title>
            <link>{link}</link>
        </image>
        <itunes:author>{author}</itunes:author>
        <itunes:subtitle>{subtitle}</itunes:subtitle>
        <itunes:summary><![CDATA[{itunes_summary}]]></itunes:summary>
        <itunes:image href="{icon}"></itunes:image>
        <itunes:explicit>no</itunes:explicit>
        <itunes:category text="{category}"></itunes:category>
{items}
    </channel>
</rss>"""

# ç”Ÿæˆitemæ¨¡å—
def xml_item(
    video_url,
    output_dir,
    video_website,
    channelid_title,
    title,
    description,
    pubDate,
    image,
):
    channelid_title = html.escape(channelid_title)
    # æŸ¥çœ‹æ ‡é¢˜ä¸­æ˜¯å¦æœ‰é¢‘é“åç§°å¦‚æ— æ·»åŠ åˆ°æè¿°ä¸­å¹¶å»é™¤ç©ºå­—ç¬¦
    title = title.replace('\x00', '')
    if channelid_title not in title:
        if description == "":
            description = f"ã€{channelid_title}ã€{description}"
        else:
            description = f"ã€{channelid_title}ã€\n{description}".replace('\x00', '')
    # æ›´æ¢æè¿°æ¢è¡Œç¬¦
    replacement_description = description.replace("\n", "&#xA;").replace("\t", "&#x9;")
    # è·å–æ–‡ä»¶åç¼€å’Œæ–‡ä»¶å­—èŠ‚å¤§å°
    if os.path.exists(f"channel_audiovisual/{output_dir}/{video_url}.mp4"):
        video_length_bytes = os.path.getsize(
            f"channel_audiovisual/{output_dir}/{video_url}.mp4"
        )
        output_format = "mp4"
        video_type = "video/mp4"
    else:
        if os.path.exists(f"channel_audiovisual/{output_dir}/{video_url}.m4a"):
            video_length_bytes = os.path.getsize(
                f"channel_audiovisual/{output_dir}/{video_url}.m4a"
            )
        else:
            video_length_bytes = 0
        output_format = "m4a"
        video_type = "audio/x-m4a"
    # è·å–æ–‡ä»¶æ—¶é•¿
    duration = time_format(
        get_duration_ffmpeg(
            f"channel_audiovisual/{output_dir}/{video_url}.{output_format}"
        )
    )
    # ç”Ÿæˆurl
    if config["token"]:
        input_string = f"{config['token']}/channel_audiovisual/{output_dir}/{video_url}.{output_format}"
    else:
        input_string = f"channel_audiovisual/{output_dir}/{video_url}.{output_format}"
    sha256_hash = hashlib.sha256(input_string.encode()).hexdigest()
    url = f"{config['address']}/channel_audiovisual/{output_dir}/{video_url}.{output_format}?token={sha256_hash}"
    # å›æ˜¾å¯¹åº”çš„item
    return f"""
        <item>
            <guid>{video_url}</guid>
            <title>{title}</title>
            <link>{video_website}</link>
            <description>{replacement_description}</description>
            <pubDate>{pubDate}</pubDate>
            <enclosure url="{url}" length="{video_length_bytes}" type="{video_type}"></enclosure>
            <itunes:author>{title}</itunes:author>
            <itunes:subtitle>{title}</itunes:subtitle>
            <itunes:summary><![CDATA[{description}]]></itunes:summary>
            <itunes:image href="{image}"></itunes:image>
            <itunes:duration>{duration}</itunes:duration>
            <itunes:explicit>no</itunes:explicit>
            <itunes:order>1</itunes:order>
        </item>
"""

# æ ¼å¼åŒ–æ—¶é—´åŠæ—¶åŒºæ¨¡å—
def format_time(time_str):
    original_tz = timezone.utc  # åŸå§‹æ—¶åŒºä¸ºUTC
    # è§£ææ—¶é—´å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºdatetimeå¯¹è±¡
    dt = datetime.fromisoformat(time_str[:-6] if time_str[-3] == ":" else time_str[:-5]).replace(tzinfo=original_tz)
    # è½¬æ¢ä¸ºç›®æ ‡æ—¶åŒº
    if time_str[-3] == ":":
        tz = timedelta(hours=int(time_str[-6:-3]),minutes=int(f"{time_str[-6]}{time_str[-2:]}"))
    else:
        tz = timedelta(hours=int(time_str[-5:-2]),minutes=int(f"{time_str[-5]}{time_str[-2:]}"))
    dt -= tz
    target_tz = timezone(timedelta(seconds=-(time.timezone + time.daylight)))
    dt_target = dt.astimezone(target_tz)
    # æ ¼å¼åŒ–ä¸ºç›®æ ‡æ—¶é—´å­—ç¬¦ä¸²
    target_format = "%a, %d %b %Y %H:%M:%S %z"
    pubDate = dt_target.strftime(target_format)
    return pubDate

# ç”ŸæˆYouTubeçš„itemæ¨¡å—
def youtube_xml_item(entry):
    # è¾“å…¥æ—¶é—´å­—ç¬¦ä¸²å’ŒåŸå§‹æ—¶åŒº
    time_str = re.search(r"(?<=<published>).+(?=</published>)", entry).group()
    pubDate = format_time(time_str)
    output_dir = re.search(r"(?<=<yt:channelId>).+(?=</yt:channelId>)", entry).group()
    description = re.search(
        r"(?<=<media:description>).+(?=</media:description>)",
        re.sub(r"\n+", "\n", entry),
        flags=re.DOTALL,
    )
    description = description.group() if description else ""
    id = re.search(r"(?<=<yt:videoId>).+(?=</yt:videoId>)", entry).group()
    return xml_item(
        id,
        output_dir,
        f"https://youtube.com/watch?v={id}",
        channelid_youtube[channelid_youtube_ids[output_dir]]["title"],
        re.search(r"(?<=<title>).+(?=</title>)", entry).group(),
        description,
        pubDate,
        re.search(r"(?<=<media:thumbnail url=\").+(?=\" width=\")", entry).group(),
    )

# ç”ŸæˆåŸæœ‰çš„itemæ¨¡å—
def xml_original_item(original_item):
    guid = re.search(r"(?<=<guid>).+(?=</guid>)", original_item).group()
    title = re.search(r"(?<=<title>).+(?=</title>)", original_item).group()
    link = re.search(r"(?<=<link>).+(?=</link>)", original_item).group()
    description = re.search(r"(?<=<description>).+(?=</description>)", original_item)
    description = description.group() if description else ""
    pubDate = re.search(r"(?<=<pubDate>).+(?=</pubDate>)", original_item).group()
    url = re.search(r"(?<=<enclosure url\=\").+?(?=\")", original_item).group()
    url = re.search(r"(?<=/channel_audiovisual/).+/.+\.(m4a|mp4)", url).group()
    if config["token"]:
        input_string = f"{config['token']}/channel_audiovisual/{url}"
    else:
        input_string = f"channel_audiovisual/{url}"
    sha256_hash = hashlib.sha256(input_string.encode()).hexdigest()
    url = f"{config['address']}/channel_audiovisual/{url}?token={sha256_hash}"
    length = re.search(r"(?<=length\=\")[0-9]+(?=\")", original_item).group()
    type_video = re.search(
        r"(?<=type\=\")(video/mp4|audio/x-m4a|audio/mpeg)(?=\")", original_item
    ).group()
    if type_video == "audio/mpeg":
        type_video = "audio/x-m4a"
    itunes_author = re.search(
        r"(?<=<itunes:author>).+(?=</itunes:author>)", original_item
    ).group()
    itunes_subtitle = re.search(
        r"(?<=<itunes:subtitle>).+(?=</itunes:subtitle>)", original_item
    ).group()
    itunes_summary = re.search(
        r"(?<=<itunes:summary><\!\[CDATA\[).+(?=\]\]></itunes:summary>)",
        original_item,
        flags=re.DOTALL,
    )
    itunes_summary = itunes_summary.group() if itunes_summary else ""
    itunes_image = re.search(
        r"(?<=<itunes:image href\=\").+(?=\"></itunes:image>)", original_item
    )
    itunes_image = itunes_image.group() if itunes_image else ""
    itunes_duration = re.search(
        r"(?<=<itunes:duration>).+(?=</itunes:duration>)", original_item
    ).group()
    itunes_explicit = re.search(
        r"(?<=<itunes:explicit>).+(?=</itunes:explicit>)", original_item
    ).group()
    itunes_order = re.search(
        r"(?<=<itunes:order>).+(?=</itunes:order>)", original_item
    ).group()
    return f"""
        <item>
            <guid>{guid}</guid>
            <title>{title}</title>
            <link>{link}</link>
            <description>{description}</description>
            <pubDate>{pubDate}</pubDate>
            <enclosure url="{url}" length="{length}" type="{type_video}"></enclosure>
            <itunes:author>{itunes_author}</itunes:author>
            <itunes:subtitle>{itunes_subtitle}</itunes:subtitle>
            <itunes:summary><![CDATA[{itunes_summary}]]></itunes:summary>
            <itunes:image href="{itunes_image}"></itunes:image>
            <itunes:duration>{itunes_duration}</itunes:duration>
            <itunes:explicit>{itunes_explicit}</itunes:explicit>
            <itunes:order>{itunes_order}</itunes:order>
        </item>
"""

# ç”Ÿæˆå“ˆå¸Œå€¼æ¨¡å—
def create_hash(data):
    data_str = str(data)
    hash_object = hashlib.sha256()
    hash_object.update(data_str.encode())
    hash_value = hash_object.hexdigest()
    return hash_value

# rssç”Ÿæˆå“ˆå¸Œå€¼æ¨¡å—
def rss_create_hash(data):
    patterns = [
        r"<lastBuildDate>(\w+), (\d{2}) (\w+) (\d{4}) (\d{2}):(\d{2}):(\d{2}) \+\d{4}</lastBuildDate>",
        r"Podflow_light\.png",
        r"Podflow_dark\.png"
    ]
    replacement = ""
    for pattern in patterns:
        data = re.sub(pattern, replacement, data)
    hash_value = create_hash(data)
    return hash_value

# è·å–åŸå§‹xmlæ¨¡å—
def get_original_rss():
    xmls_original_fail =[]
    # è·å–åŸå§‹æ€»xmlæ–‡ä»¶
    try:
        with open(f"{config['filename']}.xml", "r", encoding="utf-8") as file:  # æ‰“å¼€æ–‡ä»¶è¿›è¡Œè¯»å–
            rss_original = file.read()  # è¯»å–æ–‡ä»¶å†…å®¹
            get_xmls_original = {
                rss_original_channel: rss_original.split(
                    f"<!-- {{{rss_original_channel}}} -->\n"
                )[1]
                for rss_original_channel in list(
                    set(re.findall(r"(?<=<!-- \{).+?(?=\} -->)", rss_original))
                )
            }
    except FileNotFoundError:  # æ–‡ä»¶ä¸å­˜åœ¨ç›´æ¥æ›´æ–°
        get_xmls_original = {}
        rss_original = ""
    # å¦‚åŸå§‹xmlæ— å¯¹åº”çš„åŸé¢‘é“items, å°†å°è¯•ä»å¯¹åº”é¢‘é“çš„xmlä¸­è·å–
    for channelid_key in (channelid_youtube_ids | channelid_bilibili_ids).keys():
        if channelid_key not in get_xmls_original.keys():
            try:
                with open(
                    f"channel_rss/{channelid_key}.xml", "r", encoding="utf-8"
                ) as file:  # æ‰“å¼€æ–‡ä»¶è¿›è¡Œè¯»å–
                    youtube_rss_original = file.read()  # è¯»å–æ–‡ä»¶å†…å®¹
                    get_xmls_original[channelid_key] = youtube_rss_original.split(
                        f"<!-- {{{channelid_key}}} -->\n"
                    )[1]
            except FileNotFoundError:  # æ–‡ä»¶ä¸å­˜åœ¨ç›´æ¥æ›´æ–°
                xmls_original_fail.append(channelid_key)
    # ç”ŸæˆåŸå§‹rssçš„å“ˆå¸Œå€¼
    hash_rss_original = rss_create_hash(rss_original)
    return get_xmls_original, hash_rss_original, xmls_original_fail

# æ‰“å°æ— æ³•ä¿ç•™åŸèŠ‚ç›®ä¿¡æ¯æ¨¡å—
def original_rss_fail_print(xmls_original_fail):
    for item in xmls_original_fail:
        if item in (channelid_youtube_ids | channelid_bilibili_ids).keys():
            write_log(f"RSSæ–‡ä»¶ä¸­ä¸å­˜åœ¨ {(channelid_youtube_ids | channelid_bilibili_ids)[item]} æ— æ³•ä¿ç•™åŸèŠ‚ç›®")

# è·å–YouTubeé¢‘é“ç®€ä»‹æ¨¡å—
def get_youtube_introduction():
    # åˆ›å»ºçº¿ç¨‹é”
    youtube_xml_get_lock = threading.Lock()
    # ä½¿ç”¨httpè·å–youtubeé¢‘é“ç®€ä»‹å’Œå›¾æ ‡æ¨¡å—
    def youtube_xml_get(output_dir):
        if channel_about := http_client(
            f"https://www.youtube.com/channel/{output_dir}/about",
            f"{channelid_youtube_ids[output_dir]} ç®€ä»‹",
            2,
            5,
        ):
            channel_about = channel_about.text
            xml_tree = {
                "icon": re.sub(
                    r"=s(0|[1-9]\d{0,3}|1[0-9]{1,3}|20[0-3][0-9]|204[0-8])-c-k",
                    "=s2048-c-k",
                    re.search(
                        r"https?://yt3.googleusercontent.com/[^\s]*(?=\">)",
                        channel_about,
                    ).group(),
                )
            }
            xml_tree["description"] = re.search(
                r"(?<=\<meta itemprop\=\"description\" content\=\").*?(?=\")",
                channel_about,
                flags=re.DOTALL,
            ).group()
            with youtube_xml_get_lock:
                youtube_xml_get_tree[output_dir] = xml_tree
    # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
    youtube_xml_get_threads = []
    for output_dir in channelid_youtube_ids_update.keys():
        thread = threading.Thread(target=youtube_xml_get, args=(output_dir,))
        youtube_xml_get_threads.append(thread)
        thread.start()
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in youtube_xml_get_threads:
        thread.join()

# ç”ŸæˆYouTubeå¯¹åº”channelçš„éœ€æ›´æ–°çš„itemsæ¨¡å—
def youtube_xml_items(output_dir):
    items_list = [f"<!-- {output_dir} -->"]
    entry_num = 0
    def get_xml_item(guid, item):
        video_website = f"https://youtube.com/watch?v={guid}"
        channelid_title = channelid_youtube[channelid_youtube_ids[output_dir]]["title"]
        if item["yt-dlp"]:
            title = html.escape(video_id_update_format[guid]["title"])
            description = html.escape(re.sub(r"\n+", "\n", video_id_update_format[guid]["description"]))
            timestamp = video_id_update_format[guid]["timestamp"]
            published = datetime.fromtimestamp(timestamp, timezone.utc).strftime("%Y-%m-%dT%H:%M:%S%z")
            pubDate = format_time(published)
            image = re.sub(r'\?.*$', "",video_id_update_format[guid]["image"])
        else:
            title = html.escape(item["title"])
            description = html.escape(re.sub(r"\n+", "\n", item["description"]))
            pubDate = format_time(item["pubDate"])
            image = re.sub(r'\?.*$', "",item["image"])
        return xml_item(
            guid,
            output_dir,
            video_website,
            channelid_title,
            title,
            description,
            pubDate,
            image,
        )
    # æœ€æ–°æ›´æ–°
    if channelid_youtube_rss[output_dir]["type"] == "dict":
        for guid in channelid_youtube_rss[output_dir]["content"]["list"]:
            if guid not in video_id_failed:
                item = channelid_youtube_rss[output_dir]["content"]["item"][guid]
                xml_item_text = get_xml_item(guid, item)
                items_list.append(f"{xml_item_text}<!-- {output_dir} -->")
                entry_num += 1
    else:
        if channelid_youtube_rss[output_dir]["type"] == "html":  # è·å–æœ€æ–°çš„rssä¿¡æ¯
            file_xml = channelid_youtube_rss[output_dir]["content"].text
        else:
            file_xml = channelid_youtube_rss[output_dir]["content"]
        entrys = re.findall(r"<entry>.+?</entry>", file_xml, re.DOTALL)
        for entry in entrys:
            if (
                re.search(r"(?<=<yt:videoId>).+(?=</yt:videoId>)", entry).group()
                not in video_id_failed
            ):
                items_list.append(f"{youtube_xml_item(entry)}<!-- {output_dir} -->")
                entry_num += 1
            if (
                entry_num
                >= channelid_youtube[channelid_youtube_ids[output_dir]]["update_size"]
            ):
                break
    items_guid = re.findall(r"(?<=<guid>).+?(?=</guid>)", "".join(items_list))
    # å­˜é‡æ¥å…¥
    entry_count = channelid_youtube[channelid_youtube_ids[output_dir]][
        "last_size"
    ] - len(items_guid)
    if xmls_original and output_dir in xmls_original and entry_count > 0:
        xml_num = 0
        for xml in xmls_original[output_dir].split(f"<!-- {output_dir} -->"):
            xml_guid = re.search(r"(?<=<guid>).+(?=</guid>)", xml)
            if xml_guid and xml_guid.group() not in items_guid and xml_guid.group() not in video_id_failed:
                items_list.append(f"{xml_original_item(xml)}<!-- {output_dir} -->")
                xml_num += 1
            if xml_num >= entry_count:
                break
    # å‘åæ›´æ–°
    try:
        backward = channelid_youtube_rss[output_dir]["backward"]
        for backward_guid in backward["list"]:
            if backward_guid not in video_id_failed:
                backward_item = backward["item"][backward_guid]
                backward_xml_item_text = get_xml_item(backward_guid, backward_item)
                items_list.append(f"{backward_xml_item_text}<!-- {output_dir} -->")
    except KeyError:
        pass
    # ç”Ÿæˆå¯¹åº”xml
    try:
        with open(
            f"channel_rss/{output_dir}.xml", "r", encoding="utf-8"
        ) as file:  # æ‰“å¼€æ–‡ä»¶è¿›è¡Œè¯»å–
            root = ET.parse(file).getroot()
            description = (root.findall(".//description")[0]).text
            description = "" if description is None else html.escape(description)
            icon = (root.findall(".//url")[0]).text
    except Exception:  # å‚æ•°ä¸å­˜åœ¨ç›´æ¥æ›´æ–°
        description = config["description"]
        icon = config["icon"]
    if (
        output_dir in channelid_youtube_ids_update
        and output_dir in youtube_xml_get_tree
    ):
        description = youtube_xml_get_tree[output_dir]["description"]
        icon = youtube_xml_get_tree[output_dir]["icon"]
    category = config["category"]
    if channelid_youtube_rss[output_dir]["type"] == "dict":
        title = channelid_youtube_rss[output_dir]["content"]["title"]
    else:
        title = re.search(r"(?<=<title>).+(?=</title>)", file_xml).group()
    link = f"https://www.youtube.com/channel/{output_dir}"
    items = "".join(items_list)
    items = f"""<!-- {{{output_dir}}} -->
{items}
<!-- {{{output_dir}}} -->"""
    file_save(
        xml_rss(title, link, description, category, icon, items),
        f"{output_dir}.xml",
        "channel_rss",
    )
    return items

# ç”Ÿæˆå“”å“©å“”å“©å¯¹åº”channelçš„éœ€æ›´æ–°çš„itemsæ¨¡å—
def bilibili_xml_items(output_dir):
    content_id, items_counts = get_file_list(output_dir, channelid_bilibili[channelid_bilibili_ids[output_dir]]["media"])
    items_list = [f"<!-- {output_dir} -->"]
    entry_num = 0
    def get_items_list(guid, item):
        pubDate = datetime.fromtimestamp(item["created"], timezone.utc).strftime("%Y-%m-%dT%H:%M:%S%z")
        if guid in items_counts:
            guid_parts = []
            guid_edgeinfos = []
            if "part" in item:
                guid_parts = item["part"]
            elif "edgeinfo" in item:
                guid_edgeinfos = item["edgeinfo"]
            else:
                guid_parts = get_bilibili_all_part(guid, channelid_bilibili_ids[output_dir])
                if not guid_parts:
                    guid_edgeinfos = get_bilibili_interactive(guid, channelid_bilibili_ids[output_dir])
            if guid_parts and items_counts[guid] == len(guid_parts):
                for guid_part in guid_parts:
                    guid_part_text = f"{item['title']} Part{guid_part['page']:0{len(str(len(guid_parts)))}}"
                    if item["title"] != guid_part["part"]:
                        guid_part_text += f" {guid_part['part']}"
                    xml_item_text = xml_item(
                        f"{item['bvid']}_p{guid_part['page']}",
                        output_dir,
                        f"https://www.bilibili.com/video/{guid}?p={guid_part['page']}",
                        channelid_bilibili[channelid_bilibili_ids[output_dir]]["title"],
                        html.escape(guid_part_text),
                        html.escape(re.sub(r"\n+", "\n", item["description"])),
                        format_time(pubDate),
                        guid_part["first_frame"],
                    )
                    items_list.append(f"{xml_item_text}<!-- {output_dir} -->")
            elif guid_edgeinfos and items_counts[guid] == len(guid_edgeinfos):
                cid_edgeinfos = {guid_edgeinfo['cid']: guid_edgeinfo['title'] for guid_edgeinfo in guid_edgeinfos}
                for guid_edgeinfo in guid_edgeinfos:
                    if guid_edgeinfo["options"]:
                        description = (
                            "ã€–äº’åŠ¨è§†é¢‘ã€—\n"
                            + "\n".join(f"{option}\tâœª{cid_edgeinfos[option_cid]}" for option, option_cid in zip(guid_edgeinfo["options"], guid_edgeinfo["options_cid"]))
                            + "\n------------------------------------------------\n"
                            + item["description"]
                        )
                    else:
                        description = (
                            "ã€–äº’åŠ¨è§†é¢‘ã€—\nTHE END."
                            + "\n------------------------------------------------\n"
                            + item["description"]
                        )
                    guid_edgeinfo_text = f"{item['title']} Part{guid_edgeinfo['num']:0{len(str(len(guid_edgeinfos)))}} {guid_edgeinfo['title']}"
                    xml_item_text = xml_item(
                        f"{item['bvid']}_{guid_edgeinfo['cid']}",
                        output_dir,
                        f"https://www.bilibili.com/video/{guid}",
                        channelid_bilibili[channelid_bilibili_ids[output_dir]]["title"],
                        html.escape(guid_edgeinfo_text),
                        html.escape(re.sub(r"\n+", "\n", description)),
                        format_time(pubDate),
                        guid_edgeinfo["first_frame"],
                    )
                    items_list.append(f"{xml_item_text}<!-- {output_dir} -->")
        else:
            xml_item_text = xml_item(
                item["bvid"],
                output_dir,
                f"https://www.bilibili.com/video/{guid}",
                channelid_bilibili[channelid_bilibili_ids[output_dir]]["title"],
                html.escape(item["title"]),
                html.escape(re.sub(r"\n+", "\n", item["description"])),
                format_time(pubDate),
                item["pic"],
            )
            items_list.append(f"{xml_item_text}<!-- {output_dir} -->")
    # æœ€æ–°æ›´æ–°
    for guid in channelid_bilibili_rss[output_dir]["content"]["list"]:
        if guid not in video_id_failed and guid in content_id:
            item = channelid_bilibili_rss[output_dir]["content"]["entry"][guid]
            get_items_list(guid, item)
            entry_num += 1
            if (
                entry_num
                >= channelid_bilibili[channelid_bilibili_ids[output_dir]]["update_size"]
            ):
                break
    items_guid = re.findall(r"(?<=<guid>).+?(?=</guid>)", "".join(items_list))
    # å­˜é‡æ¥å…¥
    entry_count = channelid_bilibili[channelid_bilibili_ids[output_dir]][
        "last_size"
    ] - len(items_guid)
    if xmls_original and output_dir in xmls_original and entry_count > 0:
        xml_num = 0
        for xml in xmls_original[output_dir].split(f"<!-- {output_dir} -->"):
            xml_guid = re.search(r"(?<=<guid>).+(?=</guid>)", xml)
            if xml_guid and xml_guid.group() not in items_guid and xml_guid.group() not in video_id_failed:
                items_list.append(f"{xml_original_item(xml)}<!-- {output_dir} -->")
                xml_num += 1
            if xml_num >= entry_count:
                break
    # å‘åæ›´æ–°
    try:
        backward = channelid_bilibili_rss[output_dir]["backward"]
        for backward_guid in backward["list"]:
            if backward_guid not in video_id_failed and backward_guid in content_id:
                backward_item = backward["entry"][backward_guid]
                get_items_list(backward_guid, backward_item)
    except KeyError:
        pass
    # ç”Ÿæˆå¯¹åº”xml
    description = html.escape(channelid_bilibili_rss[output_dir]["content"]["sign"])
    icon = channelid_bilibili_rss[output_dir]["content"]["face"]
    category = config["category"]
    title = html.escape(channelid_bilibili_rss[output_dir]["content"]["name"])
    link = f"https://space.bilibili.com/{output_dir}"
    items = "".join(items_list)
    items = f"""<!-- {{{output_dir}}} -->
{items}
<!-- {{{output_dir}}} -->"""
    file_save(
        xml_rss(title, link, description, category, icon, items),
        f"{output_dir}.xml",
        "channel_rss",
    )
    return items

# æ˜¾ç¤ºç½‘å€åŠäºŒç»´ç æ¨¡å—
def display_qrcode_and_url(output_dir, channelid_video, channelid_video_name, channelid_video_ids_update):
    if output_dir in channelid_video_ids_update:
        update_text = "å·²æ›´æ–°"
    else:
        update_text = "æ— æ›´æ–°"
    if config["token"]:
        xml_url = f"{config['address']}/channel_rss/{output_dir}.xml?token={config['token']}"
    else:
        xml_url = f"{config['address']}/channel_rss/{output_dir}.xml"
        
    if (
        channelid_video["DisplayRSSaddress"] 
        or output_dir in channelid_video_ids_update
    ):
        print(f"{datetime.now().strftime('%H:%M:%S')}|{channelid_video_name} æ’­å®¢{update_text}|åœ°å€:\n\033[34m{xml_url}\033[0m")
    if (
        (
            channelid_video["DisplayRSSaddress"] 
            or output_dir in channelid_video_ids_update
        )
        and channelid_video["QRcode"]
        and output_dir not in displayed_QRcode
    ):
        qr_code(xml_url)
        displayed_QRcode.append(output_dir)

# ç”Ÿæˆä¸»rssæ¨¡å—
def create_main_rss():
    for output_dir in channelid_youtube_ids:
        items = youtube_xml_items(output_dir)
        display_qrcode_and_url(
            output_dir,
            channelid_youtube[channelid_youtube_ids[output_dir]],
            channelid_youtube_ids[output_dir],
            channelid_youtube_ids_update
        )
        if channelid_youtube[channelid_youtube_ids[output_dir]]["InmainRSS"]:
            all_items.append(items)
        all_youtube_content_ytid[output_dir] = re.findall(
            r"(?:/UC.{22}/)(.{11}\.m4a|.{11}\.mp4)(?=\"|\?)", items
        )
    for output_dir in channelid_bilibili_ids:
        items = bilibili_xml_items(output_dir)
        display_qrcode_and_url(
            output_dir,
            channelid_bilibili[channelid_bilibili_ids[output_dir]],
            channelid_bilibili_ids[output_dir],
            channelid_bilibili_ids_update
        )
        if channelid_bilibili[channelid_bilibili_ids[output_dir]]["InmainRSS"]:
            all_items.append(items)
        all_bilibili_content_bvid[output_dir] = re.findall(
            r"(?:/[0-9]+/)(BV.{10}\.m4a|BV.{10}\.mp4|BV.{10}_p[0-9]+\.m4a|BV.{10}_p[0-9]+\.mp4|BV.{10}_[0-9]{9}\.m4a|BV.{10}_[0-9]{9}\.mp4)(?=\"|\?)", items
        )

# xmlå¤‡ä»½ä¿å­˜æ¨¡å—
def backup_zip_save(file_content):
    def get_file_name():
        # è·å–å½“å‰çš„å…·ä½“æ—¶é—´
        current_time = datetime.now()
        # æ ¼å¼åŒ–è¾“å‡º, åªä¿ç•™å¹´æœˆæ—¥æ—¶åˆ†ç§’
        formatted_time = current_time.strftime("%Y%m%d%H%M%S")
        return f"{formatted_time}.xml"
    # å®šä¹‰è¦æ·»åŠ åˆ°å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶åå’Œå†…å®¹
    compress_file_name = "Podflow_backup.zip"
    # ç”Ÿæˆæ–°rssçš„å“ˆå¸Œå€¼
    hash_overall_rss = rss_create_hash(overall_rss)
    # ä½¿ç”¨å“ˆå¸Œå€¼åˆ¤æ–­æ–°è€rssæ˜¯å¦ä¸€è‡´
    if hash_overall_rss == hash_rss_original:
        judging_save = True
        write_log("é¢‘é“æ— æ›´æ–°å†…å®¹å°†ä¸è¿›è¡Œå¤‡ä»½")
    else:
        judging_save = False
    while not judging_save:
        # è·å–è¦å†™å…¥å‹ç¼©åŒ…çš„æ–‡ä»¶å
        file_name_str = get_file_name()
        # æ‰“å¼€å‹ç¼©æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        with zipfile.ZipFile(compress_file_name, 'a') as zipf:
            # è®¾ç½®å‹ç¼©çº§åˆ«ä¸ºæœ€å¤§
            zipf.compression = zipfile.ZIP_LZMA
            zipf.compresslevel = 9
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äºå‹ç¼©åŒ…ä¸­
            if file_name_str not in zipf.namelist():
                # å°†æ–‡ä»¶å†…å®¹å†™å…¥å‹ç¼©åŒ…
                zipf.writestr(file_name_str, file_content)
                judging_save = True
            else:
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¾“å‡ºæç¤ºä¿¡æ¯
                print(f"{file_name_str}å·²å­˜åœ¨äºå‹ç¼©åŒ…ä¸­ï¼Œé‡è¯•ä¸­...")

# åˆ é™¤å¤šä½™åª’ä½“æ–‡ä»¶æ¨¡å—
def remove_file():
    for output_dir in channelid_youtube_ids:
        for file_name in os.listdir(f"channel_audiovisual/{output_dir}"):
            if file_name not in all_youtube_content_ytid[output_dir]:
                os.remove(f"channel_audiovisual/{output_dir}/{file_name}")
                write_log(f"{channelid_youtube_ids[output_dir]}|{file_name}å·²åˆ é™¤")
    for output_dir in channelid_bilibili_ids:
        for file_name in os.listdir(f"channel_audiovisual/{output_dir}"):
            if file_name not in all_bilibili_content_bvid[output_dir]:
                os.remove(f"channel_audiovisual/{output_dir}/{file_name}")
                write_log(f"{channelid_bilibili_ids[output_dir]}|{file_name}å·²åˆ é™¤")

# åˆ é™¤å·²æŠ›å¼ƒçš„åª’ä½“æ–‡ä»¶å¤¹æ¨¡å—
def remove_dir():
    folder_names = [
        folder
        for folder in os.listdir("channel_audiovisual")
        if os.path.isdir(f"channel_audiovisual/{folder}")
    ]
    folder_names_youtube = [name for name in folder_names if re.match(r"UC.{22}", name)]
    for name in folder_names_youtube:
        if name not in channelid_youtube_ids_original:
            os.system(f"rm -r channel_audiovisual/{name}")
            write_log(f"{name}æ–‡ä»¶å¤¹å·²åˆ é™¤")
    folder_names_bilibili = [name for name in folder_names if re.match(r"[0-9]+", name)]
    for name in folder_names_bilibili:
        if name not in channelid_bilibili_ids_original:
            os.system(f"rm -r channel_audiovisual/{name}")
            write_log(f"{name}æ–‡ä»¶å¤¹å·²åˆ é™¤")

# è¡¥å…¨ç¼ºå¤±åª’ä½“æ–‡ä»¶åˆ°å­—å…¸æ¨¡å—
def make_up_file():
    for output_dir in channelid_youtube_ids:
        for file_name in all_youtube_content_ytid[output_dir]:
            if file_name not in os.listdir(f"channel_audiovisual/{output_dir}"):
                main = file_name.split(".")[0]
                media = file_name.split(".")[1]
                video_id_format = {
                    "id": output_dir,
                    "media": media,
                    "url": f"https://www.youtube.com/watch?v={main}",
                    "name": channelid_youtube_ids[output_dir],
                    "cookie": None,
                    "main": main,
                }
                if media == "mp4":
                    video_quality = channelid_youtube[channelid_youtube_ids[output_dir]][
                        "quality"
                    ]
                else:
                    video_quality = 480
                video_id_format["quality"] = video_quality
                make_up_file_format[main] = video_id_format
    for output_dir in channelid_bilibili_ids:
        for file_name in all_bilibili_content_bvid[output_dir]:
            if file_name not in os.listdir(f"channel_audiovisual/{output_dir}"):
                main = file_name.split(".")[0][:12]
                if main not in make_up_file_format:
                    media = file_name.split(".")[1]
                    video_id_format = {
                        "id": output_dir,
                        "media": media,
                        "url": f"https://www.bilibili.com/video/{main}",
                        "name": channelid_bilibili_ids[output_dir],
                        "cookie": "channel_data/yt_dlp_bilibili.txt",
                        "main": main
                    }
                    if media == "mp4":
                        video_quality = channelid_bilibili[channelid_bilibili_ids[output_dir]][
                            "quality"
                        ]
                    else:
                        video_quality = 480
                    video_id_format["quality"] = video_quality
                    make_up_file_format[main] = video_id_format

# è¡¥å…¨åœ¨rssä¸­ç¼ºå¤±çš„åª’ä½“æ ¼å¼ä¿¡æ¯æ¨¡å—
def make_up_file_format_mod():
    # åˆ¤æ–­æ˜¯å¦è¡¥å…¨
    if len(make_up_file_format) != 0:
        print(f"{datetime.now().strftime('%H:%M:%S')}|è¡¥å…¨ç¼ºå¤±åª’ä½“ \033[34mä¸‹è½½å‡†å¤‡ä¸­...\033[0m")
    # åˆ›å»ºçº¿ç¨‹é”
    makeup_yt_format_lock = threading.Lock()
    def makeup_yt_format(video_id):
        makeup_id_format = media_format(
            make_up_file_format[video_id]["url"],
            make_up_file_format[video_id]["main"],
            make_up_file_format[video_id]["media"],
            make_up_file_format[video_id]["quality"],
            make_up_file_format[video_id]["cookie"],
        )
        if  "å¹´é¾„é™åˆ¶" in makeup_id_format:
            if youtube_cookie:
                make_up_file_format[video_id]["cookie"] = "channel_data/yt_dlp_youtube.txt"
                makeup_id_format = media_format(
                    make_up_file_format[video_id]["url"],
                    make_up_file_format[video_id]["main"],
                    make_up_file_format[video_id]["media"],
                    make_up_file_format[video_id]["quality"],
                    make_up_file_format[video_id]["cookie"],
                )
                if  "å¹´é¾„é™åˆ¶" in makeup_id_format:
                    makeup_id_format = "\x1b[31må¹´é¾„é™åˆ¶\x1b[0m(Cookiesé”™è¯¯)"
            else:
                makeup_id_format = "\x1b[31må¹´é¾„é™åˆ¶\x1b[0m(éœ€è¦Cookies)"
        if isinstance(makeup_id_format, list):
            if len(makeup_id_format) == 1:
                entry_id_makeup_format = makeup_id_format[0]
                make_up_file_format[video_id]["format"] = entry_id_makeup_format["duration_and_id"]
                make_up_file_format[video_id]["download"] = entry_id_makeup_format["download"]
            else:
                entrys_id = []
                for entry_id_makeup_format in makeup_id_format:
                    entry_id = entry_id_makeup_format["id"]
                    entrys_id.append(entry_id)
                    make_up_file_format[entry_id] = {
                        "id": make_up_file_format[video_id]["id"],
                        "name": make_up_file_format[video_id]["name"],
                        "media": make_up_file_format[video_id]["media"],
                        "quality": make_up_file_format[video_id]["quality"],
                        "url": entry_id_makeup_format["url"],
                        "cookie": make_up_file_format[video_id]["cookie"],
                        "format": entry_id_makeup_format["duration_and_id"],
                        "main": make_up_file_format[video_id]["main"],
                        "download": entry_id_makeup_format["download"],
                    }
                del make_up_file_format[video_id]
        else:
            with makeup_yt_format_lock:
                write_log(
                    f"{make_up_file_format[video_id]['name']}|{video_id}|{makeup_id_format}"
                )
                make_up_file_format_fail[video_id] = make_up_file_format[video_id]['id']  # å°†æ— æ³•è¡¥å…¨çš„åª’ä½“æ·»åŠ åˆ°å¤±è´¥å­—å…¸ä¸­
                del make_up_file_format[video_id]
    # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
    makeup_yt_format_threads = []
    for video_id in make_up_file_format.keys():
        thread = threading.Thread(target=makeup_yt_format, args=(video_id,))
        makeup_yt_format_threads.append(thread)
        thread.start()
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in makeup_yt_format_threads:
        thread.join()

# ä¸‹è½½è¡¥å…¨Youtubeå’Œå“”å“©å“”å“©è§†é¢‘æ¨¡å—
def make_up_file_mod():
    for video_id in make_up_file_format.keys():
        media = make_up_file_format[video_id]["media"]
        id = make_up_file_format[video_id]["id"]
        name = make_up_file_format[video_id]['name']
        if f"{video_id}.{media}" not in os.listdir(f"channel_audiovisual/{id}"):
            write_log(
                f"{name}|{video_id} ç¼ºå¤±å¹¶é‡æ–°ä¸‹è½½"
            )
            if dl_aideo_video(
                video_id,
                id,
                media,
                make_up_file_format[video_id]["format"],
                config["retry_count"],
                make_up_file_format[video_id]["download"]["url"],
                name,
                make_up_file_format[video_id]["cookie"],
                make_up_file_format[video_id]["download"]["num"]
            ):
                video_id_failed.append(video_id)
                write_log(
                    f"{make_up_file_format[video_id]['name']}|{video_id} \033[31mæ— æ³•ä¸‹è½½\033[0m"
                )

# åˆ é™¤æ— æ³•è¡¥å…¨çš„åª’ä½“æ¨¡å—
def del_makeup_yt_format_fail(overall_rss):
    for video_id in make_up_file_format_fail.keys():
        pattern_video_fail_item = rf'<!-- {make_up_file_format_fail[video_id]} -->(?:(?!<!-- {make_up_file_format_fail[video_id]} -->).)+?<guid>{video_id}</guid>.+?<!-- {make_up_file_format_fail[video_id]} -->'
        replacement_video_fail_item = f'<!-- {make_up_file_format_fail[video_id]} -->'
        overall_rss = re.sub(pattern_video_fail_item, replacement_video_fail_item, overall_rss, flags=re.DOTALL)
    return overall_rss

# è·å–é…ç½®æ–‡ä»¶config
config = get_config(args.config)
# çº æ­£é…ç½®ä¿¡æ¯config
correct_config()
# ä»é…ç½®æ–‡ä»¶ä¸­è·å–YouTubeçš„é¢‘é“
channelid_youtube = get_channelid("youtube")
# ä»é…ç½®æ–‡ä»¶ä¸­è·å–å“”å“©å“”å“©çš„é¢‘é“
channelid_bilibili = get_channelid("bilibili")
# æ„å»ºæ–‡ä»¶å¤¹channel_id
folder_build("channel_id")
# æ„å»ºæ–‡ä»¶å¤¹channel_audiovisual
folder_build("channel_audiovisual")
# æ„å»ºæ–‡ä»¶å¤¹channel_rss
folder_build("channel_rss")
# æ„å»ºæ–‡ä»¶å¤¹channel_data
folder_build("channel_data")
# ä¿®æ­£channelid_youtube
channelid_youtube = correct_channelid(channelid_youtube, "youtube")
# ä¿®æ­£channelid_bilibili
channelid_bilibili = correct_channelid(channelid_bilibili, "bilibili")
# è¯»å–youtubeé¢‘é“çš„id
channelid_youtube_ids = get_channelid_id(channelid_youtube, "youtube")
# å¤åˆ¶youtubeé¢‘é“idç”¨äºåˆ é™¤å·²æŠ›å¼ƒçš„åª’ä½“æ–‡ä»¶å¤¹
channelid_youtube_ids_original = channelid_youtube_ids.copy()
# è¯»å–bilibilié¢‘é“çš„id
channelid_bilibili_ids = get_channelid_id(channelid_bilibili, "bilibili")
# å¤åˆ¶bilibilié¢‘é“idç”¨äºåˆ é™¤å·²æŠ›å¼ƒçš„åª’ä½“æ–‡ä»¶å¤¹
channelid_bilibili_ids_original = channelid_bilibili_ids.copy()

# Bottleå’ŒCherrypyåˆå§‹åŒ–æ¨¡å—
Shutdown_VALID_TOKEN = "shutdown"
Shutdown_VALID_TOKEN += datetime.now().strftime("%Y%m%d%H%M%S")
Shutdown_VALID_TOKEN += os.urandom(32).hex()
Shutdown_VALID_TOKEN = hashlib.sha256(Shutdown_VALID_TOKEN.encode()).hexdigest()  # ç”¨äºæœåŠ¡å™¨å…³é—­çš„éªŒè¯ Token
VALID_TOKEN = config["token"]  # ä»é…ç½®ä¸­è¯»å–ä¸»éªŒè¯ Token
bottle_filename = config["filename"]  # ä»é…ç½®ä¸­è¯»å–æ–‡ä»¶å
server_process_print_flag = ["keep"]  # æ§åˆ¶æ˜¯å¦æŒç»­è¾“å‡ºæ—¥å¿—

app = Bottle()  # åˆ›å»º Bottle åº”ç”¨

# å®šä¹‰è¦å…±äº«çš„æ–‡ä»¶è·¯å¾„
shared_files = {
    bottle_filename.lower(): f'{bottle_filename}.xml',  # æ–‡ä»¶è·¯å¾„æ˜ å°„ï¼Œæ”¯æŒå¤§å°å†™ä¸æ•æ„Ÿçš„æ–‡ä»¶å
    f'{bottle_filename.lower()}.xml': f'{bottle_filename}.xml',  # åŒä¸Šï¼Œæ”¯æŒå¸¦ .xml åç¼€
}
bottle_channelid = channelid_youtube_ids_original | channelid_bilibili_ids_original | {"channel_audiovisual/": "", "channel_rss/": ""}  # åˆå¹¶å¤šä¸ªé¢‘é“ ID
bottle_print = []  # å­˜å‚¨æ‰“å°æ—¥å¿—

# åˆ¤æ–­tokenæ˜¯å¦æ­£ç¡®çš„éªŒè¯æ¨¡å—
def token_judgment(token, VALID_TOKEN="", filename="", foldername=""):
    # å¦‚æœè¯·æ±‚çš„æ˜¯ 'channel_audiovisual/' æ–‡ä»¶å¤¹ï¼Œé‡‡ç”¨ç‰¹æ®Šçš„ Token éªŒè¯é€»è¾‘
    if foldername == 'channel_audiovisual/':
        if VALID_TOKEN == "" and token == hashlib.sha256(f"{filename}".encode()).hexdigest():  # å¦‚æœæ²¡æœ‰é…ç½® Tokenï¼Œåˆ™ä½¿ç”¨æ–‡ä»¶åçš„å“ˆå¸Œå€¼
            return True
        elif token == hashlib.sha256(f"{VALID_TOKEN}/{filename}".encode()).hexdigest():  # ä½¿ç”¨éªŒè¯ Token å’Œæ–‡ä»¶åçš„å“ˆå¸Œå€¼
            return True
        else:
            return False
    else:
        # å¯¹äºå…¶ä»–æ–‡ä»¶å¤¹ï¼Œé‡‡ç”¨å¸¸è§„çš„ Token éªŒè¯
        if VALID_TOKEN == "":
            return True  # å¦‚æœæ²¡æœ‰é…ç½®éªŒè¯ Tokenï¼Œåˆ™é»˜è®¤é€šè¿‡
        elif token == VALID_TOKEN:
            return True  # å¦‚æœ Token åŒ¹é…ï¼Œåˆ™é€šè¿‡
        else:
            return False  # å¦åˆ™éªŒè¯å¤±è´¥

# æ·»åŠ è‡³bottle_printæ¨¡å—
def add_bottle_print(client_ip, filename, status):
    # åç¼€
    suffixs = [".mp4", ".m4a", ".xml", ".ico"]
    # è®¾ç½®çŠ¶æ€ç å¯¹åº”çš„é¢œè‰²
    status_colors = {
        200: '\033[32m',  # ç»¿è‰² (æˆåŠŸ)
        401: '\033[31m',  # çº¢è‰² (æœªç»æˆæƒ)
        404: '\033[35m',  # ç´«è‰² (æœªæ‰¾åˆ°)
        303: '\033[33m',  # é»„è‰² (é‡å®šå‘)
        206: '\033[36m',  # é’è‰² (éƒ¨åˆ†å†…å®¹)
    }
    # é»˜è®¤é¢œè‰²
    color = status_colors.get(status, '\033[0m')
    status = f"{color}{status}\033[0m"
    now_time = datetime.now().strftime('%H:%M:%S')
    client_ip = f"\033[34m{client_ip}\033[0m"
    if config["httpfs"]:
        write_log(f"{client_ip} {filename} {status}", None, False, True, None, "httpfs.log")
    for suffix in suffixs:
        filename = filename.replace(suffix, "")
    bottle_print.append(f"{now_time}|{client_ip} {filename} {status}")

# CherryPy æœåŠ¡å™¨æ‰“å°æ¨¡å—
def cherry_print(flag_judgment=True):
    if flag_judgment:
        server_process_print_flag[0] = "keep"
    if server_process_print_flag[0] == "keep" and bottle_print:  # å¦‚æœè®¾ç½®ä¸ºä¿æŒè¾“å‡ºï¼Œåˆ™æ‰“å°æ—¥å¿—
        print('\n'.join(bottle_print))
        bottle_print.clear()

# ä¸»è·¯ç”±ï¼Œå¤„ç†æ ¹è·¯å¾„è¯·æ±‚
@app.route('/')
def home():
    # è¾“å‡ºè¯·æ±‚æ—¥å¿—çš„å‡½æ•°
    def print_out(status):
        client_ip = request.remote_addr  # è·å–å®¢æˆ·ç«¯ IP åœ°å€
        client_port = request.environ.get('REMOTE_PORT')  # è·å–å®¢æˆ·ç«¯ç«¯å£
        if client_port:
            client_ip = f"{client_ip}:{client_port}"  # å¦‚æœæœ‰ç«¯å£ä¿¡æ¯ï¼Œåˆ™åŒ…æ‹¬ç«¯å£
        add_bottle_print(client_ip, "/", status)  # æ·»åŠ æ—¥å¿—ä¿¡æ¯
        cherry_print(False)

    token = request.query.get('token')  # è·å–è¯·æ±‚ä¸­çš„ Token
    if token_judgment(token, VALID_TOKEN):  # éªŒè¯ Token
        print_out(303)  # å¦‚æœéªŒè¯æˆåŠŸï¼Œè¾“å‡º 200 çŠ¶æ€
        return redirect('https://github.com/gruel-zxz/podflow')  # è¿”å›æ­£å¸¸å“åº”
    else:
        print_out(401)  # å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¾“å‡º 401 çŠ¶æ€
        abort(401, "Unauthorized: Invalid Token")  # è¿”å›æœªç»æˆæƒé”™è¯¯

# è·¯ç”±ï¼Œå¤„ç†å…³é—­æœåŠ¡å™¨çš„è¯·æ±‚
@app.route('/shutdown')
def shutdown():
    # è¾“å‡ºå…³é—­è¯·æ±‚æ—¥å¿—çš„å‡½æ•°
    def print_out(status):
        client_ip = request.remote_addr
        client_port = request.environ.get('REMOTE_PORT')
        if client_port:
            client_ip = f"{client_ip}:{client_port}"
        add_bottle_print(client_ip, "shutdown", status)
        cherry_print(False)
    
    token = request.query.get('token')  # è·å–è¯·æ±‚ä¸­çš„ Token
    if token_judgment(token, Shutdown_VALID_TOKEN):  # éªŒè¯ Token æ˜¯å¦ä¸ºå…³é—­ç”¨çš„ Token
        print_out(200)  # å¦‚æœéªŒè¯æˆåŠŸï¼Œè¾“å‡º 200 çŠ¶æ€
        cherrypy.engine.exit()  # ä½¿ç”¨ CherryPy æä¾›çš„åœæ­¢åŠŸèƒ½æ¥å…³é—­æœåŠ¡å™¨
        return "Shutting down..."  # è¿”å›å…³æœºå“åº”
    else:
        print_out(401)  # å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¾“å‡º 401 çŠ¶æ€
        abort(401, "Unauthorized: Invalid Token")  # è¿”å›æœªç»æˆæƒé”™è¯¯

# è·¯ç”±ï¼Œå¤„ç† favicon è¯·æ±‚
@app.route('/favicon.ico')
def favicon():
    client_ip = request.remote_addr
    client_port = request.environ.get('REMOTE_PORT')
    if client_port:
        client_ip = f"{client_ip}:{client_port}"
    add_bottle_print(client_ip, "favicon.ico", 303)  # è¾“å‡ºè®¿é—® favicon çš„æ—¥å¿—
    cherry_print(False)
    return redirect('https://raw.githubusercontent.com/gruel-zxz/podflow/main/Podflow.png')  # é‡å®šå‘åˆ°å›¾æ ‡ URL

# è·¯ç”±ï¼Œå¤„ç†é™æ€æ–‡ä»¶è¯·æ±‚
@app.route('/<filename:path>')
def serve_static(filename):
    token = request.query.get('token')  # è·å–è¯·æ±‚ä¸­çš„ Token

    # è¾“å‡ºæ–‡ä»¶è¯·æ±‚æ—¥å¿—çš„å‡½æ•°
    def print_out(filename, status):
        client_ip = request.remote_addr
        client_port = request.environ.get('REMOTE_PORT')
        if client_port:
            client_ip = f"{client_ip}:{client_port}"
        for bottle_channelid_key, bottle_channelid_value in bottle_channelid.items():
            filename = filename.replace(bottle_channelid_key, bottle_channelid_value)  # æ›¿æ¢é¢‘é“è·¯å¾„
            if status == 200 and request.headers.get('Range'):  # å¦‚æœæ˜¯éƒ¨åˆ†è¯·æ±‚ï¼Œåˆ™è¿”å› 206 çŠ¶æ€
                status = 206
        add_bottle_print(client_ip, filename, status)  # è¾“å‡ºæ—¥å¿—
        cherry_print(False)
    
    # æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ£€æŸ¥çš„å‡½æ•°
    def file_exist(token, VALID_TOKEN, filename, foldername=""):
        if token_judgment(token, VALID_TOKEN, filename, foldername):  # éªŒè¯ Token
            if os.path.exists(filename):  # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿”å›æ–‡ä»¶
                print_out(filename, 200)
                return static_file(filename, root=".")
            else:  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å› 404 é”™è¯¯
                print_out(filename, 404)
                abort(404, "File not found")
        else:  # å¦‚æœ Token éªŒè¯å¤±è´¥ï¼Œè¿”å› 401 é”™è¯¯
            print_out(filename, 401)
            abort(401, "Unauthorized: Invalid Token")
    
    # å¤„ç†ä¸åŒçš„æ–‡ä»¶è·¯å¾„
    if filename in ['channel_audiovisual/', 'channel_rss/']:
        print_out(filename, 404)
        abort(404, "File not found")
    elif filename.startswith('channel_audiovisual/'):
        return file_exist(token, VALID_TOKEN, filename, "channel_audiovisual/")
    elif filename.startswith('channel_rss/') and filename.endswith(".xml"):
        return file_exist(token, VALID_TOKEN, filename)
    elif filename.startswith('channel_rss/'):
        return file_exist(token, VALID_TOKEN, f"{filename}.xml")
    elif filename.lower() in shared_files:
        return file_exist(token, VALID_TOKEN, shared_files[filename.lower()])
    else:
        print_out(filename, 404)  # å¦‚æœæ–‡ä»¶è·¯å¾„æœªåŒ¹é…ï¼Œè¿”å› 404 é”™è¯¯
        abort(404, "File not found")

# å¯åŠ¨ CherryPy æœåŠ¡å™¨
cherrypy.tree.graft(app)  # å°† Bottle åº”ç”¨åµŒå…¥åˆ° CherryPy ä¸­
cherrypy.config.update({
    'global': {
        'tools.sessions.on': True,  # å¯ç”¨ä¼šè¯æ”¯æŒ
        'server.socket_host': '0.0.0.0',  # ç›‘å¬æ‰€æœ‰ IP åœ°å€
        'server.socket_port': config["port"],  # è®¾ç½®ç›‘å¬ç«¯å£
        'log.screen': False,  # ç¦ç”¨å±å¹•æ—¥å¿—è¾“å‡º
        'log.access_file': '',  # å…³é—­è®¿é—®æ—¥å¿—
        'log.error_file': ''  # å…³é—­é”™è¯¯æ—¥å¿—
    }
})

# ä¸»æµç¨‹
cherrypy.engine.start()  # å¯åŠ¨ CherryPy æœåŠ¡å™¨
print(f"{datetime.now().strftime('%H:%M:%S')}|HTTPæœåŠ¡å™¨å¯åŠ¨ï¼Œç«¯å£ï¼š{config['port']}")
if args.httpfs:  # HttpFSå‚æ•°åˆ¤æ–­ï¼Œæ˜¯å¦ç»§ç»­è¿è¡Œ
    cherrypy.engine.block()  # é˜»æ­¢ç¨‹åºé€€å‡ºï¼Œä¿æŒHTTPæœåŠ¡è¿è¡Œ
while update_num > 0 or update_num == -1:  # å¾ªç¯ä¸»æ›´æ–°
    # æš‚åœè¿›ç¨‹æ‰“å°
    server_process_print_flag[0] = "pause"
    # è·å–YouTube cookie
    youtube_cookie = get_youtube_cookie()
    # æ›´æ–°å“”å“©å“”å“©data
    channelid_bilibili_ids, bilibili_data = get_bilibili_data(channelid_bilibili_ids_original)
    # æ¢å¤è¿›ç¨‹æ‰“å°
    cherry_print()
    # è·å–åŸå§‹xmlå­—å…¸å’Œrssæ–‡æœ¬
    xmls_original, hash_rss_original, xmls_original_fail = get_original_rss()
    # æ›´æ–°Youtubeå’Œå“”å“©å“”å“©é¢‘é“xml
    update_youtube_bilibili_rss()
    # åˆ¤æ–­æ˜¯å¦æœ‰æ›´æ–°å†…å®¹
    if channelid_youtube_ids_update != {} or channelid_bilibili_ids_update != {}:
        update_generate_rss = True
    if update_generate_rss:
        # æ ¹æ®æ—¥å‡ºæ—¥è½ä¿®æ”¹å°é¢(åªé€‚ç”¨åŸå°é¢)
        channge_icon()
        # è¾“å‡ºéœ€è¦æ›´æ–°çš„ä¿¡æ¯
        update_information_display(channelid_youtube_ids_update, youtube_content_ytid_update, youtube_content_ytid_backward_update, "YouTube")
        update_information_display(channelid_bilibili_ids_update, bilibili_content_bvid_update, bilibili_content_bvid_backward_update, "BiliBili")
        # æš‚åœè¿›ç¨‹æ‰“å°
        server_process_print_flag[0] = "pause"
        # è·å–è§†é¢‘æ ¼å¼ä¿¡æ¯
        get_video_format()
        # æ¢å¤è¿›ç¨‹æ‰“å°
        cherry_print()
        # æš‚åœè¿›ç¨‹æ‰“å°
        server_process_print_flag[0] = "pause"
        # ä¸‹è½½YouTubeå’Œå“”å“©å“”å“©è§†é¢‘
        youtube_and_bilibili_download()
        # æ¢å¤è¿›ç¨‹æ‰“å°
        cherry_print()
        # æ‰“å°æ— æ³•ä¿ç•™åŸèŠ‚ç›®ä¿¡æ¯
        original_rss_fail_print(xmls_original_fail)
        # è·å–YouTubeé¢‘é“ç®€ä»‹
        get_youtube_introduction()
        # æš‚åœè¿›ç¨‹æ‰“å°
        server_process_print_flag[0] = "pause"
        # ç”Ÿæˆåˆ†å’Œä¸»rss
        create_main_rss()
        # æ¢å¤è¿›ç¨‹æ‰“å°
        cherry_print()
        # åˆ é™¤ä¸åœ¨rssä¸­çš„åª’ä½“æ–‡ä»¶
        remove_file()
        # åˆ é™¤å·²æŠ›å¼ƒçš„åª’ä½“æ–‡ä»¶å¤¹
        remove_dir()
        # è¡¥å…¨ç¼ºå¤±åª’ä½“æ–‡ä»¶åˆ°å­—å…¸
        make_up_file()
        # æŒ‰å‚æ•°è·å–éœ€è¦è¡¥å…¨çš„æœ€å¤§ä¸ªæ•°
        make_up_file_format = split_dict(make_up_file_format, config["completion_count"], True)[0]
        # æš‚åœè¿›ç¨‹æ‰“å°
        server_process_print_flag[0] = "pause"
        # è¡¥å…¨åœ¨rssä¸­ç¼ºå¤±çš„åª’ä½“æ ¼å¼ä¿¡æ¯
        make_up_file_format_mod()
        # æ¢å¤è¿›ç¨‹æ‰“å°
        cherry_print()
        # ç”Ÿæˆä¸»rss
        overall_rss = xml_rss(
            config["title"],
            config["link"],
            config["description"],
            config["category"],
            config["icon"],
            "\n".join(all_items),
            )
        # åˆ é™¤æ— æ³•è¡¥å…¨çš„åª’ä½“
        overall_rss = del_makeup_yt_format_fail(overall_rss)
        # ä¿å­˜ä¸»rss
        file_save(overall_rss, f"{config['filename']}.xml")
        # æš‚åœè¿›ç¨‹æ‰“å°
        server_process_print_flag[0] = "pause"
        # ç”Ÿæˆä¸»urlåŠäºŒç»´ç 
        if config["token"]:
            overall_url = f"{config['address']}/{config['filename']}.xml?token={config['token']}"
        else:
            overall_url = f"{config['address']}/{config['filename']}.xml"
            
        write_log("æ€»æ’­å®¢å·²æ›´æ–°", f"åœ°å€:\n\033[34m{overall_url}\033[0m")
        if "main" not in displayed_QRcode:
            qr_code(overall_url)
            displayed_QRcode.append("main")
        # æ¢å¤è¿›ç¨‹æ‰“å°
        cherry_print()
        # å¤‡ä»½ä¸»rss
        backup_zip_save(overall_rss)
        # æš‚åœè¿›ç¨‹æ‰“å°
        server_process_print_flag[0] = "pause"
        # ä¸‹è½½è¡¥å…¨Youtubeå’Œå“”å“©å“”å“©è§†é¢‘æ¨¡å—
        make_up_file_mod()
        # æ¢å¤è¿›ç¨‹æ‰“å°
        cherry_print()
    else:
        print(f"{datetime.now().strftime('%H:%M:%S')}|é¢‘é“æ— æ›´æ–°å†…å®¹")
    # æ¸…ç©ºå˜é‡å†…æ•°æ®
    channelid_youtube_ids_update.clear()  # éœ€æ›´æ–°çš„YouTubeé¢‘é“å­—å…¸
    youtube_content_ytid_update.clear()  # éœ€ä¸‹è½½YouTubeè§†é¢‘å­—å…¸
    youtube_content_ytid_backward_update.clear()  # å‘åæ›´æ–°éœ€ä¸‹è½½YouTubeè§†é¢‘å­—å…¸
    channelid_youtube_rss.clear()  # YouTubeé¢‘é“æœ€æ–°Rss Responseå­—å…¸
    channelid_bilibili_ids_update.clear()  # éœ€æ›´æ–°çš„å“”å“©å“”å“©é¢‘é“å­—å…¸
    bilibili_content_bvid_update.clear()  # éœ€ä¸‹è½½å“”å“©å“”å“©è§†é¢‘å­—å…¸
    channelid_bilibili_rss.clear()  # å“”å“©å“”å“©é¢‘é“æœ€æ–°Rss Responseå­—å…¸
    bilibili_content_bvid_backward_update.clear()  # å‘åæ›´æ–°éœ€ä¸‹è½½å“”å“©å“”å“©è§†é¢‘å­—å…¸
    video_id_failed.clear()  # YouTube&å“”å“©å“”å“©è§†é¢‘ä¸‹è½½å¤±è´¥åˆ—è¡¨
    video_id_update_format.clear()  # YouTube&å“”å“©å“”å“©è§†é¢‘ä¸‹è½½çš„è¯¦ç»†ä¿¡æ¯å­—å…¸
    hash_rss_original = ""  # åŸå§‹rsså“ˆå¸Œå€¼æ–‡æœ¬
    xmls_original.clear()  # åŸå§‹xmlä¿¡æ¯å­—å…¸
    xmls_original_fail.clear()  # æœªè·å–åŸå§‹xmlé¢‘é“åˆ—è¡¨
    youtube_xml_get_tree.clear()  # YouTubeé¢‘é“ç®€ä»‹å’Œå›¾æ ‡å­—å…¸
    all_youtube_content_ytid.clear()  # æ‰€æœ‰YouTubeè§†é¢‘idå­—å…¸
    all_bilibili_content_bvid.clear()  # æ‰€æœ‰å“”å“©å“”å“©è§†é¢‘idå­—å…¸
    all_items.clear()  # æ›´æ–°åæ‰€æœ‰itemæ˜ç»†åˆ—è¡¨
    overall_rss = ""  # æ›´æ–°åçš„rssæ–‡æœ¬
    make_up_file_format.clear()  # è¡¥å…¨ç¼ºå¤±åª’ä½“å­—å…¸
    make_up_file_format_fail.clear()  # è¡¥å…¨ç¼ºå¤±åª’ä½“å¤±è´¥å­—å…¸
    # å°†éœ€è¦æ›´æ–°è½¬ä¸ºå¦
    update_generate_rss = False
    if update_num != -1:
        update_num -= 1
    if argument == "a-shell":
        openserver_process = subprocess.Popen(
            ["open", f"shortcuts://run-shortcut?name=Podflow&input=text&text={urllib.parse.quote(json.dumps(shortcuts_url))}"]
        )
        # å»¶æ—¶
        time.sleep(60+len(shortcuts_url)*5)
        openserver_process.terminate()
        break
    elif update_num == 0:
        break
    else:
        # å»¶æ—¶
        time.sleep(time_delay)

# å…³é—­CherryPyæœåŠ¡å™¨
cherrypy.engine.exit()
print(f"{datetime.now().strftime('%H:%M:%S')}|Podflowè¿è¡Œç»“æŸ")